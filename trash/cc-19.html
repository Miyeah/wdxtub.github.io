<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="一个逗比的碎碎念"><title>云计算 第 18 课 用 MapReduce 进行批处理 | 小土刀</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">云计算 第 18 课 用 MapReduce 进行批处理</h1><a id="logo" href="/.">小土刀</a><p class="description">Agony is my triumph</p></div><div id="nav-menu"><a href="/."><i class="icon-home"> 首页</i></a><a href="/about/"><i class="icon-power-cord"> 技术</i></a><a href="/life/"><i class="icon-pacman"> 生活</i></a><a href="/portfolio/"><i class="icon-infinite"> 作品</i></a><a href="/archives/"><i class="icon-floppy-disk"> 归档</i></a><a href="/atom.xml"><i class="icon-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">云计算 第 18 课 用 MapReduce 进行批处理</h1><div class="post-content"><p>从这节课开始我们进入了一个新的阶段，开始具体来应用 MapReduce 编程模型，这次主要是计算文本的 N-Gram 及语言模型并连接到 web 服务中。</p>
<a id="more"></a>
<hr>
<h2 id="u5B66_u4E60_u76EE_u6807"><a href="#u5B66_u4E60_u76EE_u6807" class="headerlink" title="学习目标"></a>学习目标</h2><ol>
<li>列举不同的并行和分布式编程模型</li>
<li>解释 MapReduce 编程的执行流程</li>
<li>使用 MapReduce 处理文本数据集</li>
<li>使用 MapReduce 计算 n-gram 已经构造语言模型</li>
<li>直接把 MapReduce 的结果载入到后端存储</li>
<li>搭建前端用来连接后端并显示结果</li>
</ol>
<p>一般来说我们根据运行时的延迟以及执行的频率会把分布式编程模型分为以下三种：</p>
<ul>
<li>批量数据处理系统 Batch Data Processing Systems    <ul>
<li>用于批量处理历史数据</li>
<li>MapReduce</li>
</ul>
</li>
<li>内存中迭代批量数据处理系统 In-Memory Iterative Batch Data Processing Systems<ul>
<li>MapReduce 需要在每次迭代后保存当前计算结果</li>
<li>对于需要多次迭代直到收敛的问题，不够高效</li>
<li>这种处理方式会把数据保存在内存中来解决这个问题</li>
<li>Spark</li>
</ul>
</li>
<li>流/实时处理系统 Streaming or Real-time processing systems<ul>
<li>前两种都是处理历史数据的</li>
<li>这种处理方式则能够实时处理数据</li>
<li>Spark Streaming, Apache Storm, Apache Samza</li>
</ul>
</li>
</ul>
<h2 id="u80CC_u666F_u77E5_u8BC6"><a href="#u80CC_u666F_u77E5_u8BC6" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="MapReduce__u4ECB_u7ECD"><a href="#MapReduce__u4ECB_u7ECD" class="headerlink" title="MapReduce 介绍"></a>MapReduce 介绍</h3><p>更加详细的介绍可以看我的<a href="/./2016/03/20/hadoop-guide/">Hadoop 指南</a></p>
<p><a href="http://hadoop.apache.org/" target="_blank" rel="external">Hadoop</a> 是 Google MapReduce 的开源实现。在 MapReduce 程序中，数据以键值对形式存储，然后通过 Mapper 和 Reducer 进行处理：</p>
<p><img src="/images/14597702456209.jpg" alt="MapReduce Overview"></p>
<p><a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/InputFormat.html" target="_blank" rel="external">InputFormat</a> 定义了 Mapper 如何从文件读入数据，并写入为 <a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/Writable.html" target="_blank" rel="external">Writable</a> 类型</p>
<p>Mapper: <code>Map(k1, v1) --&gt; list(k2, v2)</code></p>
<p>然后会进行 Shuffle 和 Sort（按照 key 的值），接着就到 Reducer，会针对同一个 key 的所有 value 进行处理</p>
<p>Reducer: <code>Reduce(k2, list (v2)) --&gt; list(v3)</code></p>
<p>具体的单词统计的例子可以参考以下资料，这里不赘述</p>
<ul>
<li>代码：<a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0" target="_blank" rel="external">简单的例子</a>；<a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v2.0" target="_blank" rel="external">复杂的例子</a></li>
<li>视频：<a href="https://www.youtube.com/watch?v=3O5e6zGb1dw" target="_blank" rel="external">代码讲解</a>；<a href="https://www.youtube.com/watch?v=iWGqAhViyfY" target="_blank" rel="external">EMR 使用指南</a></li>
</ul>
<p>在这个例子中，打包代码的时候不建议使用 maven 或者 eclipse，因为可能会弄错 Hadoop 包的版本，使用下面的命令（代码文件名为 <code>WordCount.java</code>）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mkdir wordcount_classes</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">javac -classpath hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar <span class="operator">-d</span> wordcount_classes WordCount.java</span><br><span class="line">jar -cvf wordcount.jar -C wordcount_classes/ .</span><br></pre></td></tr></table></figure>
<ul>
<li>然后需要把输入数据放入到 HDFS 中，如 <code>hadoop fs -put /input</code></li>
<li>然后执行 <code>hadoop jar wordcount.jar WordCount /input /output</code> 来进行 MapReduce 工作</li>
<li>查看结果 <code>hadoop fs -ls /output</code></li>
</ul>
<p>上面的部分是用命令行来进行执行，实际我们可以直接在 web 界面操作</p>
<ul>
<li>创建 EMR 的时候选择 Advanced Opitons</li>
<li>在 Steps 中选择 Custom JAR，然后 Configure and Add 具体的 JAR 包以及参数（比如 JAR 在 S3 中的位置）</li>
<li>然后执行即可</li>
</ul>
<h3 id="N-Grams__u4ECB_u7ECD"><a href="#N-Grams__u4ECB_u7ECD" class="headerlink" title="N-Grams 介绍"></a>N-Grams 介绍</h3><p>N-Grams 的定义在<a href="https://en.wikipedia.org/wiki/N-gram" target="_blank" rel="external">这里</a>，不过直接看下图也就很清晰了</p>
<p><img src="/images/14597807939944.jpg" alt=""></p>
<p>我们这次的任务只需要计算从 1-gram 到 5-gram（虽然图中也写了 6-gram）</p>
<h2 id="u4EFB_u52A1_u76EE_u6807"><a href="#u4EFB_u52A1_u76EE_u6807" class="headerlink" title="任务目标"></a>任务目标</h2><p>这里我们需要构建一个输入文本预测器，通过 n-gram 及对应的语言模型，预测用户之后可能会输入的内容，具体步骤如下：</p>
<ol>
<li>[40%] 在 Wiki 数据上计算 ngram</li>
<li>[40%] 构造语言模型</li>
<li>[20%] 代码质量</li>
<li>[10%(bonus)] 词语自动完成</li>
</ol>
<p>环境要求</p>
<ul>
<li>打上标签：<code>Project:4.1</code></li>
<li>AWS Elastic MapReducer(EMR)，用 <code>m3</code> 开头的机器</li>
<li>预算 <code>$20</code></li>
<li>MapReduce 的 java 程序需要用 JRE 1.7 编译（因为 Amazon EMR 只支持这个）</li>
</ul>
<h2 id="u4EFB_u52A1_1__u6784_u9020_n-gram__u6A21_u578B"><a href="#u4EFB_u52A1_1__u6784_u9020_n-gram__u6A21_u578B" class="headerlink" title="任务 1 构造 n-gram 模型"></a>任务 1 构造 n-gram 模型</h2><ul>
<li>数据集 <code>s3://cmucc-datasets/enwiki-20160204-pages</code></li>
<li>n-gram 格式 <code>&lt;phrase&gt;&lt;\t&gt;&lt;count&gt;</code></li>
</ul>
<p>格式的一个例子</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>        <span class="number">1000</span></span><br><span class="line"><span class="keyword">this</span> is     <span class="number">500</span></span><br><span class="line"><span class="keyword">this</span> is a   <span class="number">250</span></span><br></pre></td></tr></table></figure>
<p>算出所有的 n-gram 之后，需要选出出现次数最多的 100 个 n-gram，如果次数一样就按照字母序排列，完成之后保存到文件中，之后会用来评分。这里最好使用 Hive 的 SQL 语法来选择，不过其他任何方法都行。</p>
<p>具体步骤：</p>
<ol>
<li>因为原始数据（总大小 6.2 GB）是 XML 格式，所以需要进行数据清洗<ul>
<li>移除 <code>&lt;ref&gt;</code> 和 <code>&lt;/ref&gt;</code>（如果有具体的属性，也要过滤掉，比如 <code>&lt;ref name=&quot;iaf-ifa.org&quot;/&gt;</code> 整个都要过滤掉 - 考虑用正则撸掉）</li>
<li>移除所有的 URL，也就是以 HTTP/HTTPS/FTP 开头的内容</li>
<li>保留单词中的 <code>&#39;</code> 号，比如 <code>it&#39;s</code> 合法，但是在单词外的，比如 <code>students&#39;</code> 就要过滤掉，但除了 <code>&#39;</code> 之外其他都必须是字母 [A-Za-z]，其他的标点符号（包括下划线 <code>_</code>）和数字都可以截取掉，需要去掉的字符都可以用空格代替，但是不要把换行符弄掉</li>
<li>单词之间不要有连续两个以上的空格</li>
<li>所有的字母都应该是小写字母（<code>toLower</code>）</li>
<li>以行作为计算 n-gram 的最小单位，跨行的都不需要考虑</li>
</ul>
</li>
<li>使用清洗后的数据，在同一个 MapReduce job 中生成 1-gram, 2-gram, 3-gram, 4-gram, 5-gram（尽量使用竞价实例，不能用 EMR streaming）<ul>
<li>不要输出空字符</li>
<li>先在小数据上测试，没有问题才继续做</li>
<li>EMR 每个小时不要超过 <code>$2</code>（使用on-demand 价格）</li>
</ul>
</li>
<li>可以把处理完成的数据保存在到 S3 中，在 MapReduce 程序中可以直接是用 <code>s3cmd</code>进行 S3 写入<ul>
<li>如果要在 S3 和 HDFS 间传输数据，可以使用 <code>hadoop distcp</code> 命令</li>
<li>如果本地存储不够的话，可以把结果拷贝到 <code>/mnt</code> 中（外置存储）</li>
</ul>
</li>
</ol>
<p>输入数据中的一行：</p>
<p><code>&#39;&#39;&#39;Anarchism&#39;&#39;&#39; is a [[political philosophy]] that advocates [[self-governance|self-governed]] societies with voluntary institutions. These are often described as [[stateless society|stateless societies]],&lt;ref&gt;&quot;ANARCHISM, a social philosophy that rejects authoritarian government and maintains that voluntary institutions are best suited to express man&#39;s natural social tendencies.&quot; George Woodcock. &quot;Anarchism&quot; at The Encyclopedia of Philosophy&lt;/ref&gt;&lt;ref name=&quot;iaf-ifa.org&quot;/&gt;&quot;In a society developed on these lines, the voluntary associations which already now begin to cover all the fields of human activity would take a still greater extension so as to substitute themselves for the state in all its functions.&quot; [http://www.theanarchistlibrary.org/HTML/Petr_Kropotkin___Anarchism__from_the_Encyclopaedia_Britannica.html Peter Kropotkin. &quot;Anarchism&quot; from the Encyclopædia Britannica]&lt;/ref&gt;&lt;ref&gt;&quot;Anarchism.&quot; The Shorter Routledge Encyclopedia of Philosophy. 2005. p. 14 &quot;Anarchism is the view that a society without the state, or government, is both possible and desirable.&quot;&lt;/ref&gt; &lt;ref&gt;&quot;anarchists are opposed to irrational (e.g., illegitimate) authority, in other words, hierarchy — hierarchy being the institutionalisation of authority within a society.&quot; [http://www.theanarchistlibrary.org/HTML/The_Anarchist_FAQ_Editorial_Collective__An_Anarchist_FAQ__03_17_.html#toc2 &quot;B.1 Why are anarchists against authority and hierarchy?&quot;] in [[An Anarchist FAQ]]&lt;/ref&gt;</code></p>
<p>清洗之后应该是</p>
<p><code>anarchism is a political philosophy that advocates self governance self governed societies with voluntary institutions these are often described as stateless society stateless societies anarchism a social philosophy that rejects authoritarian government and maintains that voluntary institutions are best suited to express man&#39;s natural social tendencies george woodcock anarchism at the encyclopedia of philosophy in a society developed on these lines the voluntary associations which already now begin to cover all the fields of human activity would take a still greater extension so as to substitute themselves for the state in all its functions peter kropotkin anarchism from the encyclop dia britannica anarchism the shorter routledge encyclopedia of philosophy p anarchism is the view that a society without the state or government is both possible and desirable anarchists are opposed to irrational e g illegitimate authority in other words hierarchy hierarchy being the institutionalisation of authority within a society b why are anarchists against authority and hierarchy in an anarchist faq</code></p>
<p>操作步骤</p>
<ol>
<li>开启一个 EMR 集群，确保 Hive, HBase 和 Hadoop 都要安装，使用 AMI version 3.10.0</li>
<li>计算完成后，把前 100 个次数最多的 ngram 结果保存在名为 <code>ngrams</code> 的文件中</li>
</ol>
<p>把 <code>ngrams</code> 文件和 MapReduce 代码（以及排序的代码）放到一个文件夹中（这里就是命令中的 <code>submit</code>），用下面的命令来提交</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/submit</span><br><span class="line"><span class="built_in">cd</span> ~/submit</span><br><span class="line">wget https://s3.amazonaws.com/<span class="number">15</span>-<span class="number">319</span><span class="operator">-s</span>16/ngram_submitter</span><br><span class="line">chmod +x ngram_submitter</span><br><span class="line">./ngram_submitter</span><br></pre></td></tr></table></figure>
<h2 id="u4EFB_u52A1_2__u6784_u9020_u8BED_u8A00_u6A21_u578B"><a href="#u4EFB_u52A1_2__u6784_u9020_u8BED_u8A00_u6A21_u578B" class="headerlink" title="任务 2 构造语言模型"></a>任务 2 构造语言模型</h2><p>直接上公式</p>
<p><img src="/images/14597892672911.jpg" alt="Probability of a word appearing after a phrase"></p>
<p>举个例子</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>                   <span class="number">1000</span></span><br><span class="line"><span class="keyword">this</span> is                 <span class="number">500</span></span><br><span class="line"><span class="keyword">this</span> is a               <span class="number">125</span></span><br><span class="line"><span class="keyword">this</span> is a blue           <span class="number">60</span></span><br><span class="line"><span class="keyword">this</span> is a blue house     <span class="number">20</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/14597894033284.jpg" alt="Probabilities to be calculated"></p>
<p>根据上面的要求，我们需要使用 MapReduce job，从 HDFS 读取前一个阶段生成的 ngram，计算所有单词和短语的语言模型，然后直接写入到 HBase 中。</p>
<ul>
<li>需要自己设计 HBase 的 schema，也就是在某个短语后面出现某个单词的概率，也需要考虑界面展示，用户会输入一个短语，然后要显示一个预测下一个次的列表</li>
<li>集群可以直接使用上一步中开启的 EMR（如果没有关掉的话）</li>
<li>作为短语，长度必须大于等于两个单词（也就是两个单词之后才会有输入预测）</li>
<li>对于每个短语，保存 n 个最可能的输入，如果有概率相同的，按字母排序，n 的具体的数值由命令行参数指定，下面有一个排序的例子可以参考</li>
<li>使用 <code>apache.commons.cli</code> 包中的 <code>GenericOptionsParser</code> 类来解析命令行参数</li>
<li>先在小数据上测试，没有问题才继续做</li>
<li>EMR 每个小时不要超过 <code>$2</code>（使用on-demand 价格），也不要超过 5 个实例</li>
</ul>
<p><img src="/images/14597906061008.jpg" alt="Sorting probabilities"></p>
<h2 id="u4EFB_u52A1_3__u7528_web__u5C55_u793A_u8BED_u8A00_u6A21_u578B"><a href="#u4EFB_u52A1_3__u7528_web__u5C55_u793A_u8BED_u8A00_u6A21_u578B" class="headerlink" title="任务 3 用 web 展示语言模型"></a>任务 3 用 web 展示语言模型</h2><p>总体的架构为</p>
<p><img src="/images/14597911497608.jpg" alt="architecture"></p>
<p>SSH 到 master 节点，开启 HBase 的 RESTful API 服务 <code>hbase-daemon.sh start rest</code>，开启/重启 Apache 服务 <code>sudo service httpd restart</code>。</p>
<p>用下面的命令下载样例代码</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo su</span><br><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">wget https://s3.amazonaws.com/<span class="number">15</span>-<span class="number">319</span><span class="operator">-s</span>16/proj4_web.tgz</span><br><span class="line"><span class="comment"># 解压文件，这里包含 样例代码和submitter</span></span><br><span class="line">sudo tar xzf ~/proj4_web.tgz</span><br><span class="line"><span class="built_in">cd</span> /var/www/html</span><br><span class="line"><span class="comment"># 如果遇到权限问题，执行</span></span><br><span class="line">sudo chmod -R <span class="number">777</span> /var/www/html/</span><br></pre></td></tr></table></figure>
<p>开启服务之后，可以访问 <code>http://masterdns/proj4_web/info.php</code> 来测试（注意安全组允许所有流量）。如果不能见到正常的页面，重启 apache 服务器并查看 <code>/var/log/httpd/error_log</code> 中的错误日志。</p>
<p>我们只需要在 PHP 代码中根据自己设计的 schema 来进行修改对应接口即可（master 的 dns，表名，列名）。默认的设计中，短语是 rowkey，所有可能出现的单词是对应的列</p>
<blockquote>
<p>特别提醒</p>
</blockquote>
<p>这次的作业有两个评分组件</p>
<ul>
<li>任务 1 中 ngram 使用 <code>ngram_submitter</code></li>
<li>任务 2 与 3 中使用 <code>submitter</code></li>
</ul>
<p>需要把所有的 MapReduce 代码放到与 <code>submitter</code> 同一个文件夹统一进行提交</p>
<h2 id="u989D_u5916_u4EFB_u52A1__u5355_u8BCD_u81EA_u52A8_u5B8C_u6210"><a href="#u989D_u5916_u4EFB_u52A1__u5355_u8BCD_u81EA_u52A8_u5B8C_u6210" class="headerlink" title="额外任务 单词自动完成"></a>额外任务 单词自动完成</h2><p>这一部分是额外任务，用户输入单词的一部分，给出最可能的完整单词</p>
<ol>
<li>使用 Wiki 数据集来统计每个单词出现的次数，确保完成了数据清洗工作（和前面一样）</li>
<li>用 HBase 来保存模型（类似前面的语言模型）</li>
<li>输入是一部分的单词，展示的结果是自动完成的建议（给出 5 个单词建议），界面和之前 ngram 的类似</li>
<li>使用 <code>wget https://s3.amazonaws.com/15-319-s16/bonus_submitter</code> 下载，并用 <code>bonus_submitter</code> 来提交（注意各种代码也要一并附上）</li>
</ol>
<p>一个例子，如果输入是 <code>carne</code>，那么建议可能是 <code>carnegie</code>, <code>carney</code>, <code>carnes</code>, <code>carneiro</code> and <code>carnell</code></p>
<h2 id="u53C2_u8003_u8D44_u6599"><a href="#u53C2_u8003_u8D44_u6599" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="external">MapReduce Tutorial</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html" target="_blank" rel="external">HDFS Command Guide</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="external">Apache Hadoop tutorial</a></li>
<li><a href="http://hbase.apache.org/book/" target="_blank" rel="external">HBase</a></li>
<li><hadoop the="" definitive="" guide=""> Tom White</hadoop></li>
<li><hbase the="" definitive="" guide=""> Lars George</hbase></li>
</ul>
</div><div data-thread-key="trash/cc-19.html" data-title="云计算 第 18 课 用 MapReduce 进行批处理" data-url="http://wdxtub.com/trash/cc-19.html" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search" class="search-form-input"/><input type="hidden" name="sitesearch" value="http://wdxtub.com"/></form></div><div class="widget"><div class="widget-title">分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Game/">Game</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Gossip/">Gossip</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Memory/">Memory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Movie/">Movie</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading/">Reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Story/">Story</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Technique/">Technique</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Traveling/">Traveling</a></li></ul></div><div class="widget"><div class="comments-title">最近评论</div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title">友情链接</div><ul></ul><a href="http://jackqdyulei.github.io" title="Lei YU" target="_blank">Lei YU</a><ul></ul><a href="http://wdxtub.com/bookclips/" title="我的书摘" target="_blank">我的书摘</a><ul></ul><a href="http://wdxtub.com/interview/" title="刷题笔记" target="_blank">刷题笔记</a></div></div></div></div><div id="footer">© <a href="/." rel="nofollow">小土刀.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><a id="rocket" href="#top" class="show"></a><script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/totop.js" type="text/javascript"></script><script src="/js/fancybox.pack.js" type="text/javascript"></script>
<script src="/js/jquery.fancybox.js" type="text/javascript"></script><link rel="stylesheet" href="/css/jquery.fancybox.css" type="text/css"><script>var duoshuoQuery = {short_name:'wdxblog'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></body></html>