<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="一个逗比的碎碎念"><title>云计算 第 19 课 用 MapReduce 进行批处理 | 小土刀</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">云计算 第 19 课 用 MapReduce 进行批处理</h1><a id="logo" href="/.">小土刀</a><p class="description">Agony is my triumph</p></div><div id="nav-menu"><a href="/."><i class="icon-home"> 首页</i></a><a href="/about/"><i class="icon-power-cord"> 技术</i></a><a href="/life/"><i class="icon-pacman"> 生活</i></a><a href="/portfolio/"><i class="icon-infinite"> 作品</i></a><a href="/archives/"><i class="icon-floppy-disk"> 归档</i></a><a href="/atom.xml"><i class="icon-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">云计算 第 19 课 用 MapReduce 进行批处理</h1><div class="post-content"><p>从这节课开始我们进入了一个新的阶段，开始具体来应用 MapReduce 编程模型，这次主要是计算文本的 N-Gram 及语言模型并连接到 web 服务中。</p>
<a id="more"></a>
<hr>
<h2 id="u5B66_u4E60_u76EE_u6807"><a href="#u5B66_u4E60_u76EE_u6807" class="headerlink" title="学习目标"></a>学习目标</h2><ol>
<li>列举不同的并行和分布式编程模型</li>
<li>解释 MapReduce 编程的执行流程</li>
<li>使用 MapReduce 处理文本数据集</li>
<li>使用 MapReduce 计算 n-gram 已经构造语言模型</li>
<li>直接把 MapReduce 的结果载入到后端存储</li>
<li>搭建前端用来连接后端并显示结果</li>
</ol>
<p>一般来说我们根据运行时的延迟以及执行的频率会把分布式编程模型分为以下三种：</p>
<ul>
<li>批量数据处理系统 Batch Data Processing Systems    <ul>
<li>用于批量处理历史数据</li>
<li>MapReduce</li>
</ul>
</li>
<li>内存中迭代批量数据处理系统 In-Memory Iterative Batch Data Processing Systems<ul>
<li>MapReduce 需要在每次迭代后保存当前计算结果</li>
<li>对于需要多次迭代直到收敛的问题，不够高效</li>
<li>这种处理方式会把数据保存在内存中来解决这个问题</li>
<li>Spark</li>
</ul>
</li>
<li>流/实时处理系统 Streaming or Real-time processing systems<ul>
<li>前两种都是处理历史数据的</li>
<li>这种处理方式则能够实时处理数据</li>
<li>Spark Streaming, Apache Storm, Apache Samza</li>
</ul>
</li>
</ul>
<h2 id="u80CC_u666F_u77E5_u8BC6"><a href="#u80CC_u666F_u77E5_u8BC6" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="MapReduce__u4ECB_u7ECD"><a href="#MapReduce__u4ECB_u7ECD" class="headerlink" title="MapReduce 介绍"></a>MapReduce 介绍</h3><p>更加详细的介绍可以看我的<a href="/./2016/03/20/hadoop-guide/">Hadoop 指南</a></p>
<p><a href="http://hadoop.apache.org/" target="_blank" rel="external">Hadoop</a> 是 Google MapReduce 的开源实现。在 MapReduce 程序中，数据以键值对形式存储，然后通过 Mapper 和 Reducer 进行处理：</p>
<p><img src="/images/14597702456209.jpg" alt="MapReduce Overview"></p>
<p><a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/InputFormat.html" target="_blank" rel="external">InputFormat</a> 定义了 Mapper 如何从文件读入数据，并写入为 <a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/io/Writable.html" target="_blank" rel="external">Writable</a> 类型</p>
<p>Mapper: <code>Map(k1, v1) --&gt; list(k2, v2)</code></p>
<p>然后会进行 Shuffle 和 Sort（按照 key 的值），接着就到 Reducer，会针对同一个 key 的所有 value 进行处理</p>
<p>Reducer: <code>Reduce(k2, list (v2)) --&gt; list(v3)</code></p>
<p>具体的单词统计的例子可以参考以下资料，这里不赘述</p>
<ul>
<li>代码：<a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v1.0" target="_blank" rel="external">简单的例子</a>；<a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v2.0" target="_blank" rel="external">复杂的例子</a></li>
<li>视频：<a href="https://www.youtube.com/watch?v=3O5e6zGb1dw" target="_blank" rel="external">代码讲解</a>；<a href="https://www.youtube.com/watch?v=iWGqAhViyfY" target="_blank" rel="external">EMR 使用指南</a></li>
</ul>
<p>在这个例子中，打包代码的时候不建议使用 maven 或者 eclipse，因为可能会弄错 Hadoop 包的版本，使用下面的命令（代码文件名为 <code>WordCount.java</code>）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~</span><br><span class="line">mkdir wordcount_classes</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">javac -classpath hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar <span class="operator">-d</span> wordcount_classes WordCount.java</span><br><span class="line">jar -cvf wordcount.jar -C wordcount_classes/ .</span><br></pre></td></tr></table></figure>
<ul>
<li>然后需要把输入数据放入到 HDFS 中，如 <code>hadoop fs -put /input</code></li>
<li>然后执行 <code>hadoop jar wordcount.jar WordCount /input /output</code> 来进行 MapReduce 工作</li>
<li>查看结果 <code>hadoop fs -ls /output</code></li>
</ul>
<p>上面的部分是用命令行来进行执行，实际我们可以直接在 web 界面操作</p>
<ul>
<li>创建 EMR 的时候选择 Advanced Opitons</li>
<li>在 Steps 中选择 Custom JAR，然后 Configure and Add 具体的 JAR 包以及参数（比如 JAR 在 S3 中的位置）</li>
<li>然后执行即可</li>
</ul>
<h3 id="N-Grams__u4ECB_u7ECD"><a href="#N-Grams__u4ECB_u7ECD" class="headerlink" title="N-Grams 介绍"></a>N-Grams 介绍</h3><p>N-Grams 的定义在<a href="https://en.wikipedia.org/wiki/N-gram" target="_blank" rel="external">这里</a>，不过直接看下图也就很清晰了</p>
<p><img src="/images/14597807939944.jpg" alt=""></p>
<p>我们这次的任务只需要计算从 1-gram 到 5-gram（虽然图中也写了 6-gram）</p>
<h2 id="u4EFB_u52A1_u76EE_u6807"><a href="#u4EFB_u52A1_u76EE_u6807" class="headerlink" title="任务目标"></a>任务目标</h2><p>这里我们需要构建一个输入文本预测器，通过 n-gram 及对应的语言模型，预测用户之后可能会输入的内容，具体步骤如下：</p>
<ol>
<li>[40%] 在 Wiki 数据上计算 ngram</li>
<li>[40%] 构造语言模型</li>
<li>[20%] 代码质量</li>
<li>[10%(bonus)] 词语自动完成</li>
</ol>
<p>环境要求</p>
<ul>
<li>打上标签：<code>Project:4.1</code></li>
<li>AWS Elastic MapReducer(EMR)，用 <code>m3</code> 开头的机器</li>
<li>预算 <code>$20</code></li>
<li>MapReduce 的 java 程序需要用 JRE 1.7 编译（因为 Amazon EMR 只支持这个）</li>
</ul>
<h2 id="u4EFB_u52A1_1__u6784_u9020_n-gram__u6A21_u578B"><a href="#u4EFB_u52A1_1__u6784_u9020_n-gram__u6A21_u578B" class="headerlink" title="任务 1 构造 n-gram 模型"></a>任务 1 构造 n-gram 模型</h2><ul>
<li>数据集 <code>s3://cmucc-datasets/enwiki-20160204-pages</code></li>
<li>n-gram 格式 <code>&lt;phrase&gt;&lt;\t&gt;&lt;count&gt;</code></li>
</ul>
<p>格式的一个例子</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>        <span class="number">1000</span></span><br><span class="line"><span class="keyword">this</span> is     <span class="number">500</span></span><br><span class="line"><span class="keyword">this</span> is a   <span class="number">250</span></span><br></pre></td></tr></table></figure>
<p>算出所有的 n-gram 之后，需要选出出现次数最多的 100 个 n-gram，如果次数一样就按照字母序排列，完成之后保存到文件中，之后会用来评分。这里最好使用 Hive 的 SQL 语法来选择，不过其他任何方法都行。</p>
<p>具体步骤：</p>
<ol>
<li>因为原始数据（总大小 6.2 GB）是 XML 格式，所以需要进行数据清洗<ul>
<li>移除 <code>&lt;ref&gt;</code> 和 <code>&lt;/ref&gt;</code>（如果有具体的属性，也要过滤掉，比如 <code>&lt;ref name=&quot;iaf-ifa.org&quot;/&gt;</code> 整个都要过滤掉 - 考虑用正则撸掉）</li>
<li>移除所有的 URL，也就是以 HTTP/HTTPS/FTP 开头的内容（这里千万要注意）</li>
<li>保留单词中的 <code>&#39;</code> 号，比如 <code>it&#39;s</code> 合法，但是在单词外的，比如 <code>students&#39;</code> 就要过滤掉，但除了 <code>&#39;</code> 之外其他都必须是字母 [A-Za-z]，其他的标点符号（包括下划线 <code>_</code>）和数字都可以截取掉，需要去掉的字符都可以用空格代替，但是不要把换行符弄掉</li>
<li>单词之间不要有连续两个以上的空格</li>
<li>所有的字母都应该是小写字母（<code>toLower</code>）</li>
<li>以行作为计算 n-gram 的最小单位，跨行的都不需要考虑</li>
</ul>
</li>
<li>使用清洗后的数据，在同一个 MapReduce job 中生成 1-gram, 2-gram, 3-gram, 4-gram, 5-gram（尽量使用竞价实例，不能用 EMR streaming）<ul>
<li>不要输出空字符</li>
<li>先在小数据上测试，没有问题才继续做</li>
<li>EMR 每个小时不要超过 <code>$2</code>（使用on-demand 价格）</li>
</ul>
</li>
<li>可以把处理完成的数据保存在到 S3 中，在 MapReduce 程序中可以直接是用 <code>s3cmd</code>进行 S3 写入<ul>
<li>如果要在 S3 和 HDFS 间传输数据，可以使用 <code>hadoop distcp</code> 命令</li>
<li>如果本地存储不够的话，可以把结果拷贝到 <code>/mnt</code> 中（外置存储）</li>
</ul>
</li>
</ol>
<p>输入数据中的一行：</p>
<p><code>&#39;&#39;&#39;Anarchism&#39;&#39;&#39; is a [[political philosophy]] that advocates [[self-governance|self-governed]] societies with voluntary institutions. These are often described as [[stateless society|stateless societies]],&lt;ref&gt;&quot;ANARCHISM, a social philosophy that rejects authoritarian government and maintains that voluntary institutions are best suited to express man&#39;s natural social tendencies.&quot; George Woodcock. &quot;Anarchism&quot; at The Encyclopedia of Philosophy&lt;/ref&gt;&lt;ref name=&quot;iaf-ifa.org&quot;/&gt;&quot;In a society developed on these lines, the voluntary associations which already now begin to cover all the fields of human activity would take a still greater extension so as to substitute themselves for the state in all its functions.&quot; [http://www.theanarchistlibrary.org/HTML/Petr_Kropotkin___Anarchism__from_the_Encyclopaedia_Britannica.html Peter Kropotkin. &quot;Anarchism&quot; from the Encyclopædia Britannica]&lt;/ref&gt;&lt;ref&gt;&quot;Anarchism.&quot; The Shorter Routledge Encyclopedia of Philosophy. 2005. p. 14 &quot;Anarchism is the view that a society without the state, or government, is both possible and desirable.&quot;&lt;/ref&gt; &lt;ref&gt;&quot;anarchists are opposed to irrational (e.g., illegitimate) authority, in other words, hierarchy — hierarchy being the institutionalisation of authority within a society.&quot; [http://www.theanarchistlibrary.org/HTML/The_Anarchist_FAQ_Editorial_Collective__An_Anarchist_FAQ__03_17_.html#toc2 &quot;B.1 Why are anarchists against authority and hierarchy?&quot;] in [[An Anarchist FAQ]]&lt;/ref&gt;</code></p>
<p>清洗之后应该是</p>
<p><code>anarchism is a political philosophy that advocates self governance self governed societies with voluntary institutions these are often described as stateless society stateless societies anarchism a social philosophy that rejects authoritarian government and maintains that voluntary institutions are best suited to express man&#39;s natural social tendencies george woodcock anarchism at the encyclopedia of philosophy in a society developed on these lines the voluntary associations which already now begin to cover all the fields of human activity would take a still greater extension so as to substitute themselves for the state in all its functions peter kropotkin anarchism from the encyclop dia britannica anarchism the shorter routledge encyclopedia of philosophy p anarchism is the view that a society without the state or government is both possible and desirable anarchists are opposed to irrational e g illegitimate authority in other words hierarchy hierarchy being the institutionalisation of authority within a society b why are anarchists against authority and hierarchy in an anarchist faq</code></p>
<p>操作步骤</p>
<ol>
<li>开启一个 EMR 集群，确保 Hive, HBase 和 Hadoop 都要安装，使用 AMI version 3.10.0<ul>
<li>连接 <code>ssh -i ../demo.pem hadoop@ec2-54-86-122-167.compute-1.amazonaws.com</code></li>
<li>安装 tmux <code>sudo yum install tmux</code></li>
<li>复制代码到服务器 <code>scp -i ../demo.pem ./WordCount.java hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:~/ngram/</code></li>
</ul>
</li>
<li>计算完成后，把前 100 个次数最多的 ngram 结果保存在名为 <code>ngrams</code> 的文件中</li>
</ol>
<p>所用命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line">$ <span class="built_in">cd</span> ~; mkdir ngram; <span class="built_in">cd</span> ngram</span><br><span class="line"><span class="comment"># 拷贝相关 jar 包</span></span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line"><span class="comment"># 这里需要多拷贝一个文件，不然会有警告（虽然不知道会不会有影响，但是没有警告总是好的）</span></span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-annotations-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line"><span class="comment"># 浏览 jar 包</span></span><br><span class="line">ls /usr/share/aws/emr/hadoop-state-pusher/lib/</span><br><span class="line"><span class="comment"># 编译</span></span><br><span class="line">mkdir class</span><br><span class="line">javac -classpath hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-annotations-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar <span class="operator">-d</span> class WordCount.java</span><br><span class="line"><span class="comment"># 生成 jar 包</span></span><br><span class="line">jar -cvf wordcount.jar -C ./class .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新编译系列脚本</span></span><br><span class="line">rm -r class/*</span><br><span class="line">rm wordcount.jar</span><br><span class="line">javac -classpath hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-annotations-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar <span class="operator">-d</span> class WordCount.java</span><br><span class="line">jar -cvf wordcount.jar -C ./class .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝数据到 hdfs</span></span><br><span class="line"><span class="built_in">cd</span> /mnt</span><br><span class="line"><span class="comment"># 这里我还是用原来的方法，因为本地有一个备份，也可以直接用 hadoop distcp 命令</span></span><br><span class="line">hadoop fs -mkdir /ngram</span><br><span class="line"><span class="comment"># 注意空间可能不够，去 /mnt 比较好</span></span><br><span class="line">aws s3 cp s3://cmucc-datasets/enwiki-<span class="number">20160204</span>-pages ./</span><br><span class="line"><span class="comment"># S3 上的大小为 6663676215</span></span><br><span class="line">wget http://s3.amazonaws.com/cmucc-datasets/enwiki-<span class="number">20160204</span>-pages</span><br><span class="line">head -n <span class="number">1000</span> enwiki-<span class="number">20160204</span>-pages &gt; testset</span><br><span class="line">hadoop fs -put ./testset /ngramtest</span><br><span class="line">hadoop fs -put ./enwiki-<span class="number">20160204</span>-pages /ngram</span><br><span class="line"><span class="comment"># 查看文件</span></span><br><span class="line">hadoop fs -ls /ngram</span><br><span class="line"><span class="comment"># 在 jar 包所在的文件夹</span></span><br><span class="line"><span class="comment"># 测试数据集，注意 output 文件夹不能存在</span></span><br><span class="line">hadoop jar wordcount.jar WordCount /ngramtest /output</span><br><span class="line"><span class="comment"># 完整数据集 </span></span><br><span class="line"><span class="comment"># [1st period 1+4] 21:29-22:00 </span></span><br><span class="line"><span class="comment"># [2nd period 1+3] 10:46-11:30</span></span><br><span class="line"><span class="comment"># [3rd period 1+4] 17:23-17:55</span></span><br><span class="line">hadoop jar wordcount.jar WordCount /ngram /ngramresult</span><br><span class="line"><span class="comment"># 查看测试结果</span></span><br><span class="line">hadoop fs -ls /output</span><br><span class="line">hadoop fs -cat /output/part-r-<span class="number">00000</span></span><br><span class="line"><span class="comment"># 查看完整数据集结果</span></span><br><span class="line">hadoop fs -ls /ngramresult</span><br><span class="line"><span class="comment"># 复制到本地</span></span><br><span class="line">hadoop fs -get /output ./</span><br><span class="line"><span class="comment"># 这个命令我直接空间不够了</span></span><br><span class="line">hadoop fs -get /ngramresult ./</span><br><span class="line"><span class="comment"># 直接从 hdfs 导入到 s3</span></span><br><span class="line">hadoop distcp /ngramresult/ s3://project4dawang/ngram</span><br></pre></td></tr></table></figure>
<p>用 Hive 进行统计要比自己手动排序方便很多，这里直接上命令，解释在注释中</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入 hive shell</span></span><br><span class="line">hive</span><br><span class="line"><span class="comment"># 这里之后的命令都是在 hive  shell 中操作，创建一个 EXTERNAL 表</span></span><br><span class="line"><span class="comment"># 好处是导入数据只需要把 mapreduce 得到的结果复制到 /data/ngram 文件夹</span></span><br><span class="line">CREATE EXTERNAL TABLE ngram(gram string, num bigint)</span><br><span class="line">ROW FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED BY <span class="string">'\t'</span></span><br><span class="line">STORED AS TEXTFILE</span><br><span class="line">LOCATION <span class="string">'/data/ngram'</span>;</span><br><span class="line"><span class="comment"># 测试表</span></span><br><span class="line">CREATE EXTERNAL TABLE <span class="built_in">test</span>(gram string, num bigint)</span><br><span class="line">ROW FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED BY <span class="string">'\t'</span></span><br><span class="line">STORED AS TEXTFILE</span><br><span class="line">LOCATION <span class="string">'/data/test'</span>;</span><br><span class="line"><span class="comment"># 查看所有表</span></span><br><span class="line">show tables;</span><br><span class="line"><span class="comment"># 查看表结构</span></span><br><span class="line">desc ngram;</span><br><span class="line">desc <span class="built_in">test</span></span><br><span class="line"><span class="comment"># 删除某个表</span></span><br><span class="line">drop table ngram;</span><br><span class="line">drop table <span class="built_in">test</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里是在 terminal 中执行</span></span><br><span class="line"><span class="comment"># 复制文件并删掉没用的 _SUCCESS 文件，只要复制过去就算是导入完成了</span></span><br><span class="line">hadoop fs -mv /ngramresult/part-* /data/ngram</span><br><span class="line">hadoop fs -rm /data/ngram/_SUCCESS</span><br><span class="line"><span class="comment"># 复制测试表的文件</span></span><br><span class="line">hadoop fs -mv /output/part-* /data/<span class="built_in">test</span></span><br><span class="line"><span class="comment"># 从本地复制过去</span></span><br><span class="line"><span class="built_in">cd</span> /mnt; mkdir data</span><br><span class="line">aws s3 cp s3://project4dawang/ngram/ngramresult/ ./data --recursive</span><br><span class="line"><span class="built_in">cd</span> data</span><br><span class="line">hadoop fs -put ./ /data/ngram</span><br><span class="line">hadoop fs -mv /data/ngram/data/* /data/ngram</span><br><span class="line">hadoop fs -rm -r /data/ngram/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件</span></span><br><span class="line">hadoop fs -ls /data/ngram</span><br><span class="line">hadoop fs -ls /data/<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hive sql 语句</span></span><br><span class="line"><span class="comment"># ORDER BY 全局排序，只有一个Reduce任务</span></span><br><span class="line"><span class="comment"># SORT BY 只在本机做排序</span></span><br><span class="line">select * from ngram order by num desc <span class="built_in">limit</span> <span class="number">200</span>;</span><br><span class="line"><span class="comment"># 测试语句 </span></span><br><span class="line">select * from <span class="built_in">test</span> order by num desc <span class="built_in">limit</span> <span class="number">100</span>;</span><br><span class="line"><span class="comment"># 把查询写入到本地文件，这里使用 200 保证不出问题（因为后面还要按字母排序）</span></span><br><span class="line"><span class="comment"># 大概一次要 25 分钟的样子</span></span><br><span class="line"><span class="comment"># 这个命令中的输出文件夹必须不存在（因为会递归删除该文件夹所有内容），保险做法直接显示在命令行里复制粘贴</span></span><br><span class="line">INSERT OVERWRITE LOCAL DIRECTORY <span class="string">'/mnt/ngram'</span> select * from ngram order by num desc <span class="built_in">limit</span> <span class="number">200</span>;</span><br><span class="line"><span class="comment"># 测试导出</span></span><br><span class="line">INSERT OVERWRITE LOCAL DIRECTORY <span class="string">'/mnt/test'</span> select * from <span class="built_in">test</span> order by num desc <span class="built_in">limit</span> <span class="number">100</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制回本地进行处理</span></span><br><span class="line"><span class="comment"># 默认的分隔符是 ^A，需要后期处理一下</span></span><br><span class="line">scp -i ../demo.pem -r hadoop@ec2-<span class="number">54</span>-<span class="number">86</span>-<span class="number">122</span>-<span class="number">167</span>.compute-<span class="number">1</span>.amazonaws.com:/mnt/ngram/* ./</span><br></pre></td></tr></table></figure>
<p>把 <code>ngrams</code> 文件和 MapReduce 代码（以及排序的代码）放到一个文件夹中（这里就是命令中的 <code>submit</code>），用下面的命令来提交</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/submit</span><br><span class="line"><span class="built_in">cd</span> ~/submit</span><br><span class="line">wget https://s3.amazonaws.com/<span class="number">15</span>-<span class="number">319</span><span class="operator">-s</span>16/ngram_submitter</span><br><span class="line">chmod +x ngram_submitter</span><br><span class="line">cp ~/ngram/WordCount.java ./</span><br><span class="line">./ngram_submitter</span><br></pre></td></tr></table></figure>
<p>根据 TPZ 上的提示，我是 <code>cite web</code> 这里出问题了，只好把数据集下载下来，看看到底出了啥问题</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 过滤出带有 cite web 的行</span></span><br><span class="line">grep <span class="string">'cite'</span> <span class="built_in">test</span> &gt; greprs; grep <span class="string">'web'</span> greprs &gt; result</span><br><span class="line"><span class="comment"># 大概知道问题所在，是过滤 url 的时候没处理好，找测试用例试验一下</span></span><br></pre></td></tr></table></figure>
<h2 id="u4EFB_u52A1_2__u6784_u9020_u8BED_u8A00_u6A21_u578B"><a href="#u4EFB_u52A1_2__u6784_u9020_u8BED_u8A00_u6A21_u578B" class="headerlink" title="任务 2 构造语言模型"></a>任务 2 构造语言模型</h2><p>直接上公式</p>
<p><img src="/images/14597892672911.jpg" alt="Probability of a word appearing after a phrase"></p>
<p>举个例子</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>                   <span class="number">1000</span></span><br><span class="line"><span class="keyword">this</span> is                 <span class="number">500</span></span><br><span class="line"><span class="keyword">this</span> is a               <span class="number">125</span></span><br><span class="line"><span class="keyword">this</span> is a blue           <span class="number">60</span></span><br><span class="line"><span class="keyword">this</span> is a blue house     <span class="number">20</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/14597894033284.jpg" alt="Probabilities to be calculated"></p>
<p>根据上面的要求，我们需要使用 MapReduce job，从 HDFS 读取前一个阶段生成的 ngram，计算所有单词和短语的语言模型，然后直接写入到 HBase 中。</p>
<ul>
<li>需要自己设计 HBase 的 schema，也就是在某个短语后面出现某个单词的概率，也需要考虑界面展示，用户会输入一个短语，然后要显示一个预测下一个次的列表</li>
<li>集群可以直接使用上一步中开启的 EMR（如果没有关掉的话）</li>
<li>作为短语，出现次数必须大于 2 次才进行计算，不然就跳过</li>
<li>对于每个短语，保存 n 个最可能的输入，如果有概率相同的，按字母排序，n 的具体的数值由命令行参数指定，下面有一个排序的例子可以参考</li>
<li>使用 <code>apache.commons.cli</code> 包中的 <code>GenericOptionsParser</code> 类来解析命令行参数</li>
<li>先在小数据上测试，没有问题才继续做</li>
<li>EMR 每个小时不要超过 <code>$2</code>（使用on-demand 价格），也不要超过 5 个实例</li>
</ul>
<p><img src="/images/14597906061008.jpg" alt="Sorting probabilities"></p>
<p>这一部分的 Mapper 和 Reducer 没有第一步这么直观，所以这里简要介绍一下我的思路。</p>
<h3 id="Mapper"><a href="#Mapper" class="headerlink" title="Mapper"></a>Mapper</h3><p>同样用上面的例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>                   <span class="number">1000</span></span><br><span class="line">that is                   <span class="number">1</span></span><br><span class="line"><span class="keyword">this</span> is                 <span class="number">500</span></span><br><span class="line"><span class="keyword">this</span> is a               <span class="number">125</span></span><br></pre></td></tr></table></figure>
<p>我们要做的实际上就是把目前的键值对，拆成 phrase 和 word 的形式，但是这里有两个地方要注意：</p>
<ol>
<li>如果 key 拆分之后只有一个单词，需要过滤掉</li>
<li>如果 value 为 1，需要过滤掉</li>
</ol>
<p>然后我们要做的就比较简单了，假设 key 拆分之后有 <code>n</code> 个词，那么把前 <code>n-1</code> 词拼成 phrase，最后一个词作为 word，不过需要注意的是，还要把之前的计数加上去（不然就丢失信息了）</p>
<p>分别过一遍上面的四个例子</p>
<figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">这里为了方便看，在 <span class="string">\t</span> 两边各加了一个空格，实际上是不需要的 </span><br><span class="line"><span class="keyword">this</span> <span class="string">\t</span> <span class="number">1000</span><span class="function"> -&gt;</span> 扔掉，key 只有 <span class="number">1</span> 个单词</span><br><span class="line"><span class="literal">that</span> <span class="keyword">is</span> <span class="string">\t</span> <span class="number">1</span><span class="function"> -&gt;</span> 扔掉，value 为 <span class="number">1</span></span><br><span class="line"><span class="keyword">this</span> <span class="keyword">is</span> <span class="string">\t</span> <span class="number">500</span><span class="function"> -&gt;</span> <span class="keyword">this</span> <span class="string">\t</span> <span class="keyword">is</span> <span class="number">500</span></span><br><span class="line"><span class="keyword">this</span> <span class="keyword">is</span> a <span class="string">\t</span> <span class="number">125</span><span class="function"> -&gt;</span> <span class="keyword">this</span> <span class="keyword">is</span> <span class="string">\t</span> a <span class="number">125</span></span><br></pre></td></tr></table></figure>
<h3 id="Reducer"><a href="#Reducer" class="headerlink" title="Reducer"></a>Reducer</h3><p>Reducer 这里主要需要进行的工作就是排序，另外因为需要直接写入到 HBase 中，建议使用 <code>TableReducer</code>。具体的步骤如下：</p>
<ol>
<li>遍历一个 key 的 value，计算出出现的总次数</li>
<li>给 value 中的各个不同的 word 进行排序（虽然后面的 PHP 代码也有排序，但是这里需要选取前 n 个，所以还是得排序）</li>
<li>计算概率（需要除以总数），然后写入到记录中，包括单词和具体的概率</li>
<li>具体组织代码的形式要参考下一个任务中 PHP 代码的访问形式（我觉得尽量别改，面得增加复杂度）</li>
</ol>
<h3 id="u5DE5_u4F5C_u65E5_u5FD7"><a href="#u5DE5_u4F5C_u65E5_u5FD7" class="headerlink" title="工作日志"></a>工作日志</h3><p>所用的命令参考 </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HBase 建表</span></span><br><span class="line">hbase shell</span><br><span class="line">&gt; create <span class="string">'wp'</span>,<span class="string">'data'</span></span><br><span class="line">&gt; list</span><br><span class="line">&gt; describe <span class="string">'wp'</span></span><br><span class="line">&gt; <span class="built_in">exit</span></span><br><span class="line"><span class="comment"># 删除表</span></span><br><span class="line">&gt; <span class="built_in">disable</span> <span class="string">'wp'</span></span><br><span class="line">&gt; drop <span class="string">'wp'</span></span><br><span class="line"><span class="comment"># 查询</span></span><br><span class="line">&gt; get <span class="string">'wp'</span>, <span class="string">'the'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line"><span class="built_in">cd</span> ~; mkdir lmodel; <span class="built_in">cd</span> lmodel</span><br><span class="line"><span class="comment"># 这句在本地执行，复制代码</span></span><br><span class="line">scp -i ../demo.pem ./LanguageModel.java hadoop@ec2-<span class="number">54</span>-<span class="number">86</span>-<span class="number">122</span>-<span class="number">167</span>.compute-<span class="number">1</span>.amazonaws.com:~/lmodel/</span><br><span class="line"><span class="comment"># 拷贝相关 jar 包</span></span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/hadoop-annotations-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar .</span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/commons-cli-<span class="number">1.2</span>.jar .</span><br><span class="line">cp /home/hadoop/hbase/hbase-<span class="number">0.94</span>.<span class="number">18</span>.jar .</span><br><span class="line"><span class="comment"># 浏览 jar 包</span></span><br><span class="line">ls /usr/share/aws/emr/hadoop-state-pusher/lib/</span><br><span class="line"><span class="comment"># 编译</span></span><br><span class="line">mkdir class</span><br><span class="line">javac -classpath hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-annotations-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hbase-<span class="number">0.94</span>.<span class="number">18</span>.jar:commons-cli-<span class="number">1.2</span>.jar <span class="operator">-d</span> class LanguageModel.java </span><br><span class="line"><span class="comment"># 生成 jar 包</span></span><br><span class="line">jar -cvf languagemodel.jar -C ./class .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新编译系列脚本</span></span><br><span class="line">rm -r class/*</span><br><span class="line">rm languagemodel.jar</span><br><span class="line">javac -classpath hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-annotations-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hbase-<span class="number">0.94</span>.<span class="number">18</span>.jar:commons-cli-<span class="number">1.2</span>.jar <span class="operator">-d</span> class LanguageModel.java</span><br><span class="line">jar -cvf languagemodel.jar -C ./class .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 jar 包所在的文件夹</span></span><br><span class="line"><span class="comment"># 测试数据集，注意是根据前面的文件夹来设置参数的</span></span><br><span class="line"><span class="comment"># 没有 output 文件夹，因为直接写入到 hbase</span></span><br><span class="line">hadoop jar languagemodel.jar LanguageModel /data/<span class="built_in">test</span></span><br><span class="line"><span class="comment"># 完整数据集 </span></span><br><span class="line"><span class="comment"># [1st period 1+4] 19:42-20:00</span></span><br><span class="line"><span class="comment"># [2nd period 1+4] 20:18-20:40</span></span><br><span class="line"><span class="comment"># [3rd period 1+4] 20:58-21:22</span></span><br><span class="line"><span class="comment"># [4th period 1+4] 20:40-21:02</span></span><br><span class="line"><span class="comment"># [5th period 1+4] 21:12-21:34</span></span><br><span class="line"><span class="comment"># [6th period 1+4] 23:37-23:59</span></span><br><span class="line"><span class="comment"># [7th period 1+4] 00:10-00:32</span></span><br><span class="line"><span class="comment"># [8th period 1+4] 00:36-00:48</span></span><br><span class="line"><span class="comment"># [9th period 1+4] 00:55-01:07</span></span><br><span class="line">hadoop jar languagemodel.jar LanguageModel /data/ngram</span><br></pre></td></tr></table></figure>
<h2 id="u4EFB_u52A1_3__u7528_web__u5C55_u793A_u8BED_u8A00_u6A21_u578B"><a href="#u4EFB_u52A1_3__u7528_web__u5C55_u793A_u8BED_u8A00_u6A21_u578B" class="headerlink" title="任务 3 用 web 展示语言模型"></a>任务 3 用 web 展示语言模型</h2><p>总体的架构为</p>
<p><img src="/images/14597911497608.jpg" alt="architecture"></p>
<p>SSH 到 master 节点，开启 HBase 的 RESTful API 服务 <code>hbase-daemon.sh start rest</code>，开启/重启 Apache 服务 <code>sudo service httpd restart</code>。</p>
<p>用下面的命令下载样例代码并启动对应服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo su</span><br><span class="line"><span class="built_in">cd</span> /home/hadoop/hbase/bin</span><br><span class="line">./hbase-daemon.sh start rest</span><br><span class="line"><span class="built_in">cd</span> /var/www/html</span><br><span class="line">wget https://s3.amazonaws.com/<span class="number">15</span>-<span class="number">319</span><span class="operator">-s</span>16/proj4_web.tgz</span><br><span class="line"><span class="comment"># 解压文件，这里包含 样例代码和submitter</span></span><br><span class="line">sudo tar xzf proj4_web.tgz</span><br><span class="line"><span class="comment"># 如果遇到权限问题，执行</span></span><br><span class="line">sudo chmod -R <span class="number">777</span> /var/www/html/</span><br><span class="line">sudo chmod -R <span class="number">777</span> /var/www/html/proj4_web</span><br></pre></td></tr></table></figure>
<p>开启服务之后，可以访问 <code>http://masterdns/proj4_web/info.php</code> 来测试（注意安全组允许所有流量）。如果不能见到正常的页面，重启 apache 服务器并查看 <code>/var/log/httpd/error_log</code> 中的错误日志。</p>
<p><img src="/images/14598964785155.jpg" alt="测试结果"></p>
<p>需要修改的代码是 <code>request.php</code>，输入测试的页面是 <code>http://masterdns/proj4_web/index.html</code></p>
<p>我们把代码下载下来 </p>
<ul>
<li><code>scp -i ../demo.pem hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:/var/www/html/proj4_web/request.php ./</code></li>
<li><code>scp -i ../demo.pem hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:/var/www/html/proj4_web/index.html ./</code></li>
</ul>
<p>需要修改的部分有：</p>
<ul>
<li>表名（和生成语言模型中的一样）</li>
<li>列族名（和生成语言模型中的一样）</li>
<li>EMR Master 的 DNS</li>
</ul>
<p>上传回去 <code>scp -i ../demo.pem ./request.php hadoop@ec2-54-86-122-167.compute-1.amazonaws.com:/var/www/html/proj4_web/index.html</code></p>
<p>我们只需要在 PHP 代码中根据自己设计的 schema 来进行修改对应接口即可（master 的 dns，表名，列名）。默认的设计中，短语是 rowkey，所有可能出现的单词是对应的列（这句话我还是不懂到底 schema 是什么）</p>
<p>完成之后应该可以看到一些推荐结果：</p>
<p><img src="/images/14599011423568.jpg" alt=""></p>
<p>需要把文件复制过来，命令为 </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 submitter 所在文件夹</span></span><br><span class="line">cp /home/hadoop/lmodel/LanguageModel.java .</span><br><span class="line">cp /home/hadoop/ngram/WordCount.java .</span><br><span class="line"><span class="comment"># 创建一个文件存命令</span></span><br><span class="line">vim <span class="built_in">command</span></span><br><span class="line"><span class="comment"># 提交</span></span><br><span class="line">./submitter</span><br></pre></td></tr></table></figure>
<blockquote>
<p>特别提醒</p>
</blockquote>
<p>这次的作业有两个评分组件</p>
<ul>
<li>任务 1 中 ngram 使用 <code>ngram_submitter</code></li>
<li>任务 2 与 3 中使用 <code>submitter</code></li>
</ul>
<p>需要把所有的 MapReduce 代码放到与 <code>submitter</code> 同一个文件夹统一进行提交</p>
<h2 id="u989D_u5916_u4EFB_u52A1__u5355_u8BCD_u81EA_u52A8_u5B8C_u6210"><a href="#u989D_u5916_u4EFB_u52A1__u5355_u8BCD_u81EA_u52A8_u5B8C_u6210" class="headerlink" title="额外任务 单词自动完成"></a>额外任务 单词自动完成</h2><p>这一部分是额外任务，用户输入单词的一部分，给出最可能的完整单词</p>
<ol>
<li>使用 Wiki 数据集来统计每个单词出现的次数，确保完成了数据清洗工作（和前面一样）</li>
<li>用 HBase 来保存模型（类似前面的语言模型）</li>
<li>输入是一部分的单词，展示的结果是自动完成的建议（给出 5 个单词建议），界面和之前 ngram 的类似</li>
<li>使用 <code>wget https://s3.amazonaws.com/15-319-s16/bonus_submitter</code> 下载，并用 <code>bonus_submitter</code> 来提交（注意各种代码也要一并附上）</li>
</ol>
<p>一个例子，如果输入是 <code>carne</code>，那么建议可能是 <code>carnegie</code>, <code>carney</code>, <code>carnes</code>, <code>carneiro</code> and <code>carnell</code></p>
<p><img src="/images/14599117908402.jpg" alt=""></p>
<p>具体需要完成的有</p>
<ul>
<li>用之前 WordCount.java 的代码生成 1-gram，并保存到结果中</li>
<li>计算模型<ul>
<li>Mapper 部分：假设一个键值对是 <code>abcd \t 1</code>，那么需要变成 <code>a \t abcd 1</code>, <code>ab \t abcd 1</code>, <code>abc \t abcd 1</code>（这里在 <code>\t</code> 两边加了空格，实际不需要）</li>
<li>Reducer 部分：针对每个 key，的所有 value，进行排列，并保存到数据库中</li>
</ul>
</li>
<li>具体怎么测试还没弄清楚，得研究一下 PHP 代码，但感觉应该是和前面的两步无关的</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这句在本地执行，复制代码</span></span><br><span class="line">scp -i ../demo.pem ./BonusWordCount.java hadoop@ec2-<span class="number">54</span>-<span class="number">86</span>-<span class="number">122</span>-<span class="number">167</span>.compute-<span class="number">1</span>.amazonaws.com:~/bonus/</span><br><span class="line">scp -i ../demo.pem ./BonusWordModel.java hadoop@ec2-<span class="number">54</span>-<span class="number">86</span>-<span class="number">122</span>-<span class="number">167</span>.compute-<span class="number">1</span>.amazonaws.com:~/bonus/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译 bonuswordcount</span></span><br><span class="line">mkdir class</span><br><span class="line">javac -classpath hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-annotations-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar <span class="operator">-d</span> class BonusWordCount.java</span><br><span class="line"><span class="comment"># 生成 jar 包</span></span><br><span class="line">jar -cvf bonuswordcount.jar -C ./class .</span><br><span class="line"><span class="comment"># 测试数据集</span></span><br><span class="line">hadoop jar bonuswordcount.jar BonusWordCount /ngramtest /bonusoutput</span><br><span class="line">hadoop fs -cat /bonusoutput/part-r-<span class="number">00000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 完整数据集</span></span><br><span class="line"><span class="comment"># [1st period 1+4] 10:48-10:54</span></span><br><span class="line">hadoop jar bonuswordcount.jar BonusWordCount /ngram /bonusresult</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译 bonuswordmodel</span></span><br><span class="line">cp /usr/share/aws/emr/hadoop-state-pusher/lib/commons-cli-<span class="number">1.2</span>.jar .</span><br><span class="line">cp /home/hadoop/hbase/hbase-<span class="number">0.94</span>.<span class="number">18</span>.jar .</span><br><span class="line">rm -r class/*</span><br><span class="line">rm bonuswordcount.jar</span><br><span class="line">rm bonuswordmodel.jar</span><br><span class="line">javac -classpath hadoop-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-core-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-mapreduce-client-common-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hadoop-annotations-<span class="number">2.4</span>.<span class="number">0</span>-amzn-<span class="number">3</span>.jar:hbase-<span class="number">0.94</span>.<span class="number">18</span>.jar:commons-cli-<span class="number">1.2</span>.jar <span class="operator">-d</span> class BonusWordModel.java </span><br><span class="line">jar -cvf bonuswordmodel.jar -C ./class .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据集</span></span><br><span class="line">hadoop jar bonuswordmodel.jar BonusWordModel /bonusoutput</span><br><span class="line"><span class="comment"># 完整数据集</span></span><br><span class="line"><span class="comment"># [1st period 1+4] 11:00-11:02</span></span><br><span class="line">hadoop jar bonuswordmodel.jar BonusWordModel /bonusresult</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 submitter 文件夹</span></span><br><span class="line">wget https://s3.amazonaws.com/<span class="number">15</span>-<span class="number">319</span><span class="operator">-s</span>16/bonus_submitter</span><br><span class="line">cp /home/hadoop/bonus/BonusWordModel.java .</span><br><span class="line">cp /home/hadoop/bonus/BonusWordCount.java .</span><br><span class="line">chmod <span class="number">777</span> bonus_submitter</span><br><span class="line">./bonus_submitter</span><br></pre></td></tr></table></figure>
<h2 id="u53C2_u8003_u8D44_u6599"><a href="#u53C2_u8003_u8D44_u6599" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="external">MapReduce Tutorial</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html" target="_blank" rel="external">HDFS Command Guide</a></li>
<li><a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html" target="_blank" rel="external">Apache Hadoop tutorial</a></li>
<li><a href="http://hbase.apache.org/book/" target="_blank" rel="external">HBase</a></li>
<li><a href="http://blog.csdn.net/yfkiss/article/details/7776406" target="_blank" rel="external">hive数据导入</a></li>
<li><a href="https://sites.google.com/site/hadoopandhive/home/how-to-output-a-table-to-a-local-file-in-hive" target="_blank" rel="external">How to output a table or result of a query to a local file or HDFS in Hive</a></li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7256833" target="_blank" rel="external">Hadoop Hive sql语法详解</a></li>
<li><a href="http://www.iteblog.com/archives/955" target="_blank" rel="external">Hive几种数据导出方式</a></li>
<li><hadoop the="" definitive="" guide=""> Tom White</hadoop></li>
<li><hbase the="" definitive="" guide=""> Lars George</hbase></li>
</ul>
</div><div data-thread-key="vault/cc-19.html" data-title="云计算 第 19 课 用 MapReduce 进行批处理" data-url="http://wdxtub.com/vault/cc-19.html" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search" class="search-form-input"/><input type="hidden" name="sitesearch" value="http://wdxtub.com"/></form></div><div class="widget"><div class="widget-title">分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Game/">Game</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Gossip/">Gossip</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Memory/">Memory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Movie/">Movie</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading/">Reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Story/">Story</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Technique/">Technique</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Traveling/">Traveling</a></li></ul></div><div class="widget"><div class="comments-title">最近评论</div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title">友情链接</div><ul></ul><a href="http://jackqdyulei.github.io" title="Lei YU" target="_blank">Lei YU</a><ul></ul><a href="http://wdxtub.com/bookclips/" title="我的书摘" target="_blank">我的书摘</a><ul></ul><a href="http://wdxtub.com/interview/" title="刷题笔记" target="_blank">刷题笔记</a></div></div></div></div><div id="footer">© <a href="/." rel="nofollow">小土刀.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><a id="rocket" href="#top" class="show"></a><script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/totop.js" type="text/javascript"></script><script src="/js/fancybox.pack.js" type="text/javascript"></script>
<script src="/js/jquery.fancybox.js" type="text/javascript"></script><link rel="stylesheet" href="/css/jquery.fancybox.css" type="text/css"><script>var duoshuoQuery = {short_name:'wdxblog'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></body></html>