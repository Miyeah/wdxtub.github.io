<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="一个逗比的碎碎念"><title>云计算 第 14 课 文件 vs 数据库 | 小土刀</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">云计算 第 14 课 文件 vs 数据库</h1><a id="logo" href="/.">小土刀</a><p class="description">Agony is my triumph</p></div><div id="nav-menu"><a href="/."><i class="icon-home"> 首页</i></a><a href="/about/"><i class="icon-power-cord"> 技术</i></a><a href="/life/"><i class="icon-pacman"> 生活</i></a><a href="/portfolio/"><i class="icon-infinite"> 作品</i></a><a href="/archives/"><i class="icon-floppy-disk"> 归档</i></a><a href="/atom.xml"><i class="icon-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">云计算 第 14 课 文件 vs 数据库</h1><div class="post-content"><p>从这一课起，我们要开始使用数据库了。通过数据库和文件的性能对比以及 SQL 与 NoSQL 的对比，学会选择最合适的技术。</p>
<a id="more"></a>
<hr>
<h2 id="u5B66_u4E60_u76EE_u6807"><a href="#u5B66_u4E60_u76EE_u6807" class="headerlink" title="学习目标"></a>学习目标</h2><ol>
<li>了解使用文件来存储信息的优势和劣势</li>
<li>增加使用 <code>awk</code>, <code>grep</code> 等命令修改文件的经验</li>
<li>了解使用数据库来存储信息的优势和劣势</li>
<li>了解 MySQL (SQL) 和 HBase (NoSQL) 的不同</li>
<li>学会如何把数据载入到数据库中(MySQL, HBase)</li>
<li>学会使用 JDBC 连接 MySQL</li>
<li>学会使用 Java API 来操作 HBase</li>
<li>了解 vertical scaling 的在持久云存储（磁盘, 固态硬盘）的性能</li>
</ol>
<p>这次的作业主要用 Bash 和 Java(MySQL &amp; HBase) 在 AWS 平台上完成。</p>
<h2 id="u80CC_u666F_u77E5_u8BC6"><a href="#u80CC_u666F_u77E5_u8BC6" class="headerlink" title="背景知识"></a>背景知识</h2><p>近年来『数据』越来越被重视，这之中很重要的一环就是——如何存储这些数据。这一课中我们会接触常见的存储数据的方式，并学会在实际场景中根据需要选择合适的技术。</p>
<p>我们先会介绍<a href="https://en.wikipedia.org/wiki/Flat_file_database" target="_blank" rel="external">『文件』</a>以及『关系型数据库』</p>
<p>通常来说，我们用文件来保存非结构化的数据，用数据库来保存结构化的数据，我们来看看下面这个例子</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件中的一行</span></span><br><span class="line">Name: Carnegie, Course: Cloud Computing, Section: A, Year: <span class="number">2015</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据库中的一行（有四列）</span></span><br><span class="line">Name           Course         Section     Year</span><br><span class="line">Carnegie    Cloud Computing      A        <span class="number">2015</span></span><br></pre></td></tr></table></figure>
<p>在数据库中，数据以表的形式存储，访问不同的元素比较简单，但是在文件中，就需要做一定的解析工作。文件和数据库各有所长，重要的还是具体问题具体分析，不能一概而论。</p>
<p>除了文件和传统的关系型数据库，NoSQL 数据库现在也越来越流行了。因为大数据面临的挑战，NoSQL 数据库在扩展性上比传统方法更好，但是却不得牺牲一些一致性和结构性来换取性能和可拓展性。</p>
<p>这节课我们同样会尝试在 HBase 上做一些操作。完成之后，应该能够对这三种方式有更加清晰的理解，以及能够根据实际使用场景来选择对应的方法。</p>
<h2 id="u80CC_u666F_u8BBE_u5B9A"><a href="#u80CC_u666F_u8BBE_u5B9A" class="headerlink" title="背景设定"></a>背景设定</h2><p>我们的目标是打造一个关于音乐和电影的社交网络，作为一个菜鸟，我拿到的第一个任务是分析音乐数据。在把服务部署到云上之前，公司希望我能评估一下用文件和用关系型数据库的性能比较。提供的数据文件如下：</p>
<p><img src="/images/14561603284363.jpg" alt=""></p>
<p>其中 <code>million_songs_metadata.csv</code> 包含所有歌曲的信息，<code>million_songs_sales_data.csv</code> 包含一段时间内每首歌的每日销量。具体的格式如下：</p>
<p><img src="/images/14561643038821.jpg" alt="Schema for file `million_songs_metadata.csv`"></p>
<p><img src="/images/14561643402177.jpg" alt="Schema for file `million_songs_sales_data.csv`"></p>
<p>最后注意要给所有用到的资源打上 <code>Project: 3.1</code> 的标签</p>
<h2 id="u6587_u4EF6_u64CD_u4F5C"><a href="#u6587_u4EF6_u64CD_u4F5C" class="headerlink" title="文件操作"></a>文件操作</h2><p>这一部分主要是使用 <code>grep</code> 和 <code>awk</code> 来进行一些简单的数据处理工作，关于这两个命令的使用，本来是打算专门写日志来说明的（然而一直没抽出时间），所以就尽量在这里介绍得清晰一点。</p>
<p>Grep 命令可以用来查找文件中出现的关键词或者某种固定的模式，如果我们要找到一个文件中包含 “The Beatles” 的记录，那么可以用以下命令：</p>
<p><code>grep -P &#39;The Beatles&#39; million_songs_metadata.csv</code></p>
<p>具体的查找过程可以有不同的参数进行设置，比方说下面的语句就会忽略大小写进行匹配：</p>
<p><code>grep -i -P &#39;The Beatles&#39; million_songs_metadata.csv</code></p>
<p>关于不同参数的意义，可以直接在命令行中输入 <code>man grep</code> 进行查看。</p>
<p>利用管道，我们可以统计具体的行数，比如：</p>
<p><code>grep -P &#39;The Beatles&#39; million_songs_metadata.csv | wc -l</code></p>
<p>用 grep 得到的结果，只要某一行出现了要找的内容，就算找到，但是如果我们想在指定的列中寻找特定的字符，就可以使用 awk 命令了。比方说，我们只想找出 <code>artist_name</code> 那一列中出现 “The Beatles” 的记录，就可以用下面的命令：</p>
<p><code>awk &#39; BEGIN {FS = &quot;,&quot;} ; {if ($7 ~ /The Beatles/) { print; }}&#39; million_songs_metadata.csv</code></p>
<p>这里 <code>$7</code> 表示是第 7 列，而 <code>FS = &quot;,&quot;</code> 表示分隔符是 <code>,</code></p>
<p>如果我们想要更复杂一点的逻辑，比如要找到 Michael Jacksn 80 年代的歌曲，就可以用这个命令：</p>
<p><code>awk &#39; BEGIN {FS = &quot;,&quot;} ; {if (tolower($7) ~ /michael jackson/ &amp;&amp; $11 &gt;= 1980 &amp;&amp; $11 &lt; 1990) { print; }}&#39; million_songs_metadata.csv</code></p>
<p>随着问题越来越复杂，可能很多时候都没有办法在一行内解决问题，不过在这一部分，我们还是尽量试试看用 grep 和 awk 解决问题。</p>
<p>做好准备之后可以开启一个 <code>ami-ca685ba0</code> 的 <code>t1.micro</code> 实例来完成下面的任务。</p>
<p>基本我们要做的就是把 <code>runner.sh</code> 补充完整，仔细读题，仔细读题，仔细读题（比如是否区别大小写）。</p>
<p>第五题可以写一个程序或者若干命令，以 <code>million_songs_metadata.csv</code> 和 <code>million_songs_sales_data.csv</code> 中相同 <code>track_id</code> 为标准，合并两个文件。生成一个 <code>million_songs_metadata_and_sales.csv</code> 数据集，其中第 1 列是 <code>track_id</code>，第 2 列是 <code>sales_date</code>，第 3 列是 <code>sales_count</code>，第 4 - 13 列是 <code>million_songs_metadata.csv</code> 的其他列。</p>
<p>完成问题之后，可以使用 <code>./runner.sh files</code> 来检查输出结果</p>
<p><strong>提示</strong></p>
<ol>
<li>搜索找到一个能够完成合并文件的 unix 命令</li>
<li>只能用命令行脚本完成</li>
<li>不要使用 Java 和 Python</li>
<li>没有特别声明，所有的匹配都是大小写敏感的</li>
<li>第六题中，一个歌手可能有多个 <code>artist_names</code>，但是只会有一个唯一的 <code>artist_id</code>，应该根据 <code>artist_id</code> 来找到最大的销量，并返回所有 <code>artist_name</code></li>
<li>注意保存好 <code>runner.sh</code></li>
</ol>
<h3 id="u89E3_u9898_u653B_u7565"><a href="#u89E3_u9898_u653B_u7565" class="headerlink" title="解题攻略"></a>解题攻略</h3><p>首先先创建一个 <code>ami-ca685ba0</code> 的 <code>t1.micro</code> 实例。就绪之后 ssh 过去：<code>ssh -i demo.pem ubuntu@ec2-54-175-177-74.compute-1.amazonaws.com</code>，即可见到这次作业的相关文件：</p>
<p><img src="/images/14561733687303.jpg" alt=""></p>
<p>这部分我们只需要用 <code>runner.sh</code>，所以把它搞到本地 <code>scp -i demo.pem ubuntu@dns.compute-1.amazonaws.com:~/Project3_1/runner.sh ./</code></p>
<p>打开 <code>runner.sh</code> 文件，可以看到需要回答的问题是：</p>
<ol>
<li>在文件 <code>million_songs_metadata.csv</code> 中，有多少行包含 <code>Aerosmith</code>，大小写敏感</li>
<li>在文件 <code>million_songs_metadata.csv</code> 中，<code>artist_name</code> 包含 <code>Bob Marley</code> 的 <code>track_id</code> 有多少个，大小写敏感</li>
<li>在文件 <code>million_songs_metadata.csv</code> 中，第 7 列中包含 <code>The Beatles</code> 的有多少行，大小写敏感</li>
<li>写出与 SQL 命令 <code>SELECT AVG(duration) FROM songs</code> 等价的命令行命令</li>
<li>把两个 csv 文件合并为 <code>million_songs_metadata_and_sales.csv</code>，以相同 <code>track_id</code> 为标准</li>
<li>在文件 <code>million_songs_metadata_and_sales.csv</code> 中，找到销量最高的 artist，一个歌手可能有多个 <code>artist_names</code>，但是只会有一个唯一的 <code>artist_id</code>，应该根据 <code>artist_id</code> 来找到最大的销量，并返回所有 <code>artist_name</code></li>
</ol>
<p>写好之后传到服务器上：<code>scp -i demo.pem ./runner.sh ubuntu@dns.compute-1.amazonaws.com:~/Project3_1/</code></p>
<p>测试的话用 <code>./runner.sh files</code>，确定无误后使用 <code>./submitter -a dawang</code> 来进行提交，代码运行完成后输入提交密码即可。</p>
<h2 id="MySQL__u64CD_u4F5C"><a href="#MySQL__u64CD_u4F5C" class="headerlink" title="MySQL 操作"></a>MySQL 操作</h2><p>同样是使用 <code>ami-ca685ba0</code> 的 <code>t1.micro</code> 实例来完成这部分的内容</p>
<p>先通过<a href="https://youtu.be/x73HknyUGIM" target="_blank" rel="external">视频</a>来了解 MySQL 的基础知识</p>
<p>远程机器中已经安装配置好了 MySQL，使用下面的命令可以开启 MySQL 命令行客户端并且连接到数据库：</p>
<p><code>mysql -u root -pdb15319root song_db</code></p>
<p>上面的命令中，用户名是 <code>root</code> 密码是 <code>db15319root</code>，使用是数据名称是 <code>song_db</code></p>
<p>数据库的相关知识可以参考<a href="https://zh.wikipedia.org/wiki/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93" target="_blank" rel="external">这里</a>，就不在日志中赘述。</p>
<p>我们需要根据前面给出 schemas 来创建对应的表，作业文件中提供了 <code>~/Project3_1/create_tables.sql</code> 文件，可以从这里开始</p>
<blockquote>
<p>远程主机中的 MySQL 版本是 5.5，注意查看对应的文档</p>
</blockquote>
<p>创建好之后，可以使用下面的命令来查看表的 schema</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">DESCRIBE</span> songs;</span></span><br><span class="line"><span class="operator"><span class="keyword">DESCRIBE</span> sales;</span></span><br></pre></td></tr></table></figure>
<p>然后和前面给出的表格进行比较，看看是否一致。</p>
<p>所以我要做的是找到合适的命令，把 <code>million_songs_metadata.csv</code> 和 <code>million_songs_sales.csv</code> 导入到 MySQL 中。可以在 MySQL 命令行工具中使用 SQL 命令导入，也可以用 mysqlimport 工具来导入，记下所使用的命令即可。</p>
<p>想要验证是否导入成功的话，可以列出前十条记录：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> songs </span><br><span class="line"><span class="keyword">LIMIT</span> <span class="number">10</span>;</span></span><br></pre></td></tr></table></figure>
<p>SQL 的语法可以参考<a href="http://www.w3school.com.cn/sql/sql_syntax.asp" target="_blank" rel="external">这里</a>，下面选出一些简单的例子进行介绍。</p>
<p>比如说下面的命令就会从表中选出 <code>artist_name</code> 一列中包含 <code>The Beatles</code> 的表项：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> songs </span><br><span class="line"><span class="keyword">WHERE</span> artist_name </span><br><span class="line"></span><br><span class="line"><span class="keyword">LIKE</span> <span class="string">'%The Beatles%'</span>;</span></span><br></pre></td></tr></table></figure>
<p>这里的 <code>%</code> 表示任何字符出现任意次数，前面提到的寻找 <code>Michael Jackson</code> 的例子可以写成：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> songs </span><br><span class="line"><span class="keyword">WHERE</span> artist_name </span><br><span class="line"><span class="keyword">LIKE</span> <span class="string">'%michael jackson%'</span><span class="keyword">AND</span> <span class="keyword">year</span> &gt;= <span class="number">1980</span> <span class="keyword">AND</span> <span class="keyword">year</span> &lt; <span class="number">1990</span>;</span></span><br></pre></td></tr></table></figure>
<p>如果需要计算平均时间，就不需要使用 <code>awk</code> 命令那么复杂，可以直接</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> <span class="keyword">AVG</span>(<span class="keyword">duration</span>) <span class="keyword">FROM</span> songs;</span></span><br></pre></td></tr></table></figure>
<p>带索引的数据库可以极大提高查询的性能，在 MySQL 中，所有的主键都会自动成为索引。</p>
<p>Aggregate Functions 允许你在多个记录中执行运算并返回一个单一值，比较常用的有 <code>SUM</code>, <code>AVG</code>, <code>MAX</code>, <code>MIN</code> 和 <code>COUNT</code>. Aggregate functions 通常和 MySQL 的 GROUP BY 关键字一起使用来为不同的 subgroup 执行运算并返回对应结果。GROUP BY 非常有用，下面是一个例子：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> c1, c2, ... cn, aggregate_function(expression)</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">table</span></span><br><span class="line"><span class="keyword">WHERE</span> where_conditions</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> c1, c2, ... cn;</span></span><br></pre></td></tr></table></figure>
<p>例如，要统计最近十天总销量排名，可以用下面的 SQL 语句：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> sales_date, <span class="keyword">SUM</span>(sales_count) <span class="keyword">AS</span> total_sales</span><br><span class="line"><span class="keyword">FROM</span> sales</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> sales_date</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> sales_date <span class="keyword">DESC</span></span><br><span class="line"><span class="keyword">LIMIT</span> <span class="number">10</span>;</span></span><br></pre></td></tr></table></figure>
<p>MySQL 的 JOIN 关键字可以用来在两个或两个以上相关的表中进行查询。在 MySQL 中 <code>JOIN</code>, <code>CROSS JOIN</code> 和 <code>INNER JOIN</code> 是等价的，下面是一个例子：syntax:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#<span class="operator"><span class="keyword">select</span> <span class="keyword">statement</span></span><br><span class="line">    <span class="keyword">SELECT</span> c1,c2,....cn</span><br><span class="line">    <span class="keyword">FROM</span> join_table;</span></span><br><span class="line">#join_table</span><br><span class="line">    table1 [INNER|CROSS] JOIN table2 [join_condition]</span><br><span class="line">#join_condition:</span><br><span class="line">    ON conditional_expr</span><br><span class="line">  | USING (column_list)</span><br></pre></td></tr></table></figure>
<p><code>INNER JOIN</code> 会构造指定的表的笛卡尔乘积，也就是第一个表中的每一行通过 join condition 和第二个表中的每一行组合。因为我们的 songs 表中的所有 <code>track_ids</code> 在 sales 表中都有对应的记录，所以这里只用 <code>INNER JOIN</code> 即可。</p>
<p>例如，下面的 SQL 语句会返回销量最高的 10 首歌的名字：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> songs.title, <span class="keyword">SUM</span>(sales_count) <span class="keyword">AS</span> total_sale</span><br><span class="line"><span class="keyword">FROM</span> songs</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> sales <span class="keyword">ON</span> songs.track_id = sales.track_id</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> sales.track_id</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> total_sale</span><br><span class="line"><span class="keyword">DESC</span> <span class="keyword">LIMIT</span> <span class="number">10</span>;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 OUTER JOIN</p>
</blockquote>
<p>和 INNER JOIN 不同的是可能会出现列的值为空的情况，根据提供不匹配的数据的表所在的位置，分为 LEFT 和 RIGHT JOINS。在 LEFT JOIN 中，会返回左边表中不匹配的记录，反之亦然。没有匹配的话，会把对应的列设为 NULL。如果想要保留不匹配的数据，这种方法就很有用了。</p>
<h3 id="JDBC__u548C_MySQL"><a href="#JDBC__u548C_MySQL" class="headerlink" title="JDBC 和 MySQL"></a>JDBC 和 MySQL</h3><p>Java Database Connectivity (JDBC) API 可以用来访问数据库，并且由于是一个跨平台的标准，在不同的平台上可以使用相同的代码。这一部分我们会使用 MySQL Connector/J。</p>
<p>第一步就是与数据建立连接，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">Connection conn = DriverManager.getConnection(URL, DB_USER, DB_PWD);</span><br></pre></td></tr></table></figure>
<p>第一行载入并初始化 MySQL 的 JDBC 驱动，然后我们就可以建立与数据库的连接（参数比较简单这里略过）</p>
<p>为了执行 SQL 操作以及获取执行完毕的结果，我们需要创建 Statement(用来执行 SQL 命令的对象)，并且在执行完成后得到一个 ResultSet 对象，下面是一个例子：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Statement stmt = conn.createStatement();</span><br><span class="line">ResultSet rs = stmt.executeQuery(<span class="string">"select count(*) as cnt from songs;"</span>);</span><br></pre></td></tr></table></figure>
<p>可以通过调用 <code>rs.next()</code> 来遍历结果：</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (rs.<span class="keyword">next</span>()) &#123;</span><br><span class="line">    <span class="keyword">int</span> rowCount = rs.getInt(<span class="string">"cnt"</span>);</span><br><span class="line">    System.out.<span class="keyword">println</span>(<span class="string">"Total number of lines in songs is "</span> + rowCount);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>执行完对应操作后，还需要关闭我们用到的 Statement 和 Connection，注意关闭 Statement 之后对应的 ResultSet 也会被关闭。</p>
<h3 id="u89E3_u9898_u653B_u7565-1"><a href="#u89E3_u9898_u653B_u7565-1" class="headerlink" title="解题攻略"></a>解题攻略</h3><p>和之前一样，我们要做的就是完成 7-11 题，需要修改 <code>MySQLTasks.java</code> 文件。仔细读题，仔细读题，仔细读题。</p>
<p>第 7-9 题我们需要为 songs 表创建索引。那么应该选择哪一列作为索引呢？</p>
<p>记录下使用的命令已经对应更新 <code>INDEX_NAME</code> 变量，建立索引需要花一点时间，不过可以换取比较大的性能提升，建立完索引后，使用下面的命令重启 mysql：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql restart</span><br></pre></td></tr></table></figure>
<p>在第 9 题中，我们会使用和第 7 题一样的指令，就可以看到建立索引之后的性能提升。</p>
<p>开始写代码之前，可以先运行一下样例，了解 java 如何和 MySQL 交互。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">javac MySQLTasks<span class="class">.java</span></span><br><span class="line">java MySQLTasks demo</span><br></pre></td></tr></table></figure>
<p>会输出 songs 表中的行数（如果存在的话），做完之后可以用 <code>./runner.sh mysql</code> 来检查。</p>
<p><strong>Bonus</strong></p>
<p>如果完成 <code>MySQLTasks.java</code> 中的 <code>loadData</code> 函数，有 5 分的加分，可以通过下面代码进行测试：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">javac MySQLTasks.java</span><br><span class="line">java MySQLTasks load_data</span><br></pre></td></tr></table></figure>
<p><strong>一些提示</strong></p>
<ul>
<li>SQL 的 LIKE 操作符默认是大小写不敏感的</li>
<li>记得把所有的答案输出到同一行</li>
<li>下一部分也可以用同一个远程机器，终止之前保存好所有的代码</li>
</ul>
<p>文件更灵活，可以存放结构或非结构数据，并且容易实现和修改；数据库则稍微笨重一些。对于文件来说，安全只能通过文件权限来控制，但是数据库有更加完善的权限管理。对文件的访问没有办法并行，但是数据库访问则可以。其他的不同基本上可以认为数据库有一套完整的管理接口和语法，而文件的话都需要自己实现，下表是一个总结：</p>
<p><img src="/images/14561824166535.jpg" alt="文件 vs 数据库"></p>
<p>首先我们把对应的文件复制到本地：<code>scp -i demo.pem ubuntu@dns.compute-1.amazonaws.com:~/Project3_1/MySQLTasks.java ./</code></p>
<p>然后我们可以用给出的 <code>create_tables.sql</code> 来新建数据表，先进入 MySQL 的命令行：<code>mysql -u root -pdb15319root song_db</code></p>
<p>然后输入 <code>source ./create_tables.sql</code> 来执行新建表格的命令。接着可以用 <code>DESCRIBE songs;</code> 和 <code>DESCRIBE sales;</code> 来查看是否成功创建（注意一定要最后的分号），如图</p>
<p><img src="/images/14561876662637.jpg" alt="songs 表"><br><img src="/images/14561876858760.jpg" alt="sales 表"></p>
<p>然后需要找到合适的命令，把 <code>million_songs_metadata.csv</code> 和 <code>million_songs_sales.csv</code> 导入到 MySQL 中（这里推荐用 <code>mysqlimport</code> 来导入，另一个有点问题）。命令为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要先更改两个 scv 文件的名字，这样才能载入到对应的表中</span></span><br><span class="line">cp million_songs_metadata.csv songs.csv</span><br><span class="line">cp million_songs_sales_data.csv sales.csv</span><br><span class="line"><span class="comment"># 然后进行载入</span></span><br><span class="line">mysqlimport -u root -pdb15319root --local --fields-terminated-by=<span class="string">","</span> --lines-terminated-by=<span class="string">"\n"</span> song_db songs.csv</span><br><span class="line">mysqlimport -u root -pdb15319root --local --fields-terminated-by=<span class="string">","</span> --lines-terminated-by=<span class="string">"\n"</span> song_db sales.csv</span><br></pre></td></tr></table></figure>
<p>然后我们用下面的命令来看看是否成功（如果不成功，就重新用前面的脚本生成一次对应的表）：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> songs <span class="keyword">LIMIT</span> <span class="number">10</span>;</span></span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> sales <span class="keyword">LIMIT</span> <span class="number">10</span>;</span></span><br></pre></td></tr></table></figure>
<p>大概应该看到</p>
<p><img src="/images/14561903279207.jpg" alt=""></p>
<p>然后就可以进入写 java 代码的阶段了，先大概看一下已有的代码，发现已经帮我们初始化过了，实际上只要在代码中填写对应的 SQL 语句即可。问题为（第 7-11 题）：</p>
<ol>
<li>(7)返回 duration 最长的歌的 trackid</li>
<li>(8)选择一列作为索引，并建立索引</li>
<li>(9)返回 duration 最长的歌的 trackid（和第 1 个题目相同，用来比较性能）</li>
<li>(10)写一条与 <code>grep -P &#39;The Beatles&#39; million_songs_metadata.csv | wc -l</code> 等价的 sql 语句，这里注意大小写的问题，提示：<code>BINARY</code>（感谢 @jiexing）</li>
<li>(11)哪个 artist 的歌曲数目是第三多的，返回其名字，如果有多个，任意一个都可以</li>
</ol>
<p>其实主要就是写出对应的 SQL 语句，执行起来都是一样的，前面也有给出例子。然后就可以上传回服务器：<code>scp -i demo.pem ./MySQLTasks.java ubuntu@dns.compute-1.amazonaws.com:~/Project3_1/</code></p>
<p>测试的话用 <code>./runner.sh mysql</code>，确定无误后使用 <code>./submitter -a dawang</code> 来进行提交，代码运行完成后输入提交密码即可。</p>
<h2 id="Vertical_Scaling__u5B58_u50A8"><a href="#Vertical_Scaling__u5B58_u50A8" class="headerlink" title="Vertical Scaling 存储"></a>Vertical Scaling 存储</h2><p>那么问题来了，我们的数据到底保存在哪里呢？当然是物理世界的硬盘上，但是我们之前好像都没有考虑到这个事情，事实上，不同的硬盘对性能也有极大的影响。</p>
<p>接下来的部分我们会了解一些 Linux 下的磁盘操作命令并且利用 AWS 提供的存储设备来进行 vertical scaling。并且用常见的 benchmarking 工具来进行测试，通过整个过程，应该就能了解为什么实际存储数据的设备也对性能有极大的影响。</p>
<p>因为大多数命令都需要 root 权限，所以开始之前 <code>sudo su</code> 一下是比较方便的选择。</p>
<p>这个<a href="https://youtu.be/8Bwg_wUVhkE" target="_blank" rel="external">视频</a>介绍如何在 EC2 实例中使用 EBS</p>
<ul>
<li>一般来说在创建 EC2 实例的时候会自动创建一个 EBS 并挂载到 EC2 实例上</li>
<li>先进入 EBS Volume 页面，Create Volume -&gt; 选择不同的大小 -&gt; 选择 Availablility Zone(要和 EC2 在同一个区域) -&gt; Create</li>
<li>点击 Action -&gt; Attach Volumn -&gt; 选择已有的实例 -&gt; 填写挂载点 <code>/dev/sdf</code></li>
<li>ssh 到机器上，输入命令 <code>sudo parted -l</code> 可以发现并没有成功挂载</li>
<li>我们在磁盘上新建一个文件系统：<code>sudo mkfs.ext4 /dev/xvdf</code></li>
<li>再次运行 <code>sudo parted -l</code>，发现一切正常</li>
<li>然后创建文件夹用来挂载 <code>sudo mkdir /mnt/ebs1</code></li>
<li>接着进行挂载 <code>sudo mount /dev/xvdf /mnt/ebs1</code></li>
<li>就可以访问对应文件夹了 <code>cd /mnt/ebs1/</code></li>
<li>最后可以用 <code>df -h</code> 来进行查看</li>
</ul>
<p>GNU <code>parted</code> 是用来创建、销毁、改变大小、检查状态、复制分区的命令，可以操作分区表（取代原来的 <code>fdisk</code>），并支持如 GUID Partition Table(GPT) 等的新特性。想要了解更多可以参考<a href="https://www.gnu.org/software/parted/manual/html_chapter/parted_1.html" target="_blank" rel="external">这里</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parted <span class="operator">-l</span></span><br><span class="line">/dev/xvda1 – this is the OS partition</span><br><span class="line">/dev/xvdb – this is the first Ephemeral (instance store) drive</span><br><span class="line">/dev/xvdc – this is the second Ephemeral (instance store) drive</span><br></pre></td></tr></table></figure>
<p>创建并格式化一个分区</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">umount /dev/xvdX <span class="comment">#where “X” - is a,b,c..etc (You should use your device’s name)</span></span><br><span class="line">parted /dev/xvdX mklabel gpt</span><br><span class="line">parted /dev/xvdX mkpart db ext4 <span class="number">0</span>% <span class="number">10</span>G</span><br><span class="line">mkfs.ext4 /dev/xvdX1</span><br></pre></td></tr></table></figure>
<p>对于比较小的 volume，可以直接整个格式化，不用分区</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkfs<span class="class">.ext4</span> /dev/xvdX</span><br></pre></td></tr></table></figure>
<p>创建挂载点并挂载</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="regexp">/storage/m</span>ountpoint</span><br><span class="line">mount <span class="regexp">/dev/y</span>ourdevice <span class="regexp">/storage/m</span>ountpoint</span><br></pre></td></tr></table></figure>
<p>到底用不用挂载点可以自己决定，不过一般来说 Linux 会挂载到 <code>/mnt</code>（EC2 也是这么做的）</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount</span><br></pre></td></tr></table></figure>
<p>不带任何参数的话会显示所有的挂载点，可以用来判断是否挂载成功。</p>
<h3 id="u89E3_u9898_u653B_u7565-2"><a href="#u89E3_u9898_u653B_u7565-2" class="headerlink" title="解题攻略"></a>解题攻略</h3><p>简单来说就是比较传统硬盘和固态硬盘的性能差别，测试的场景如下：</p>
<p><img src="/images/14561980518607.jpg" alt=""></p>
<p>Sysbench 是一个包含多个测试的评测。这里我们使用的评测程序和 sysbench 唯一不同的是可以选择 [SSD|Magnetic]。</p>
<p>根据下面的指示完成不同配置的测试，记录下不同的 RPS，把数字填写到对应的位置即可。</p>
<blockquote>
<p>提示：使用比较慢的硬件时，准备 10GB 的数据可能要花费很长时间，用最好的的机器（比如 large）来准备数据</p>
</blockquote>
<p><strong>准备测试数据</strong></p>
<p>步骤如下</p>
<ol>
<li>用 <code>ami-ca685ba0</code> 启动一个 <code>t1.micro</code> 或 <code>m3.large</code> 的实例</li>
<li>创建一个 20GB 的 EBS volume (磁盘或固态硬盘) 。确保和 EC2 实例在同一个区域</li>
<li>把 EBS volume 挂载到 EC2 实例上</li>
<li>SSH 到 EC2 实例，格式化并挂载 EBS volume</li>
<li>进入挂载文件夹</li>
<li>用下面的命令生成测试数据 <code>sudo /home/ubuntu/Project3_1/sysbench --test=fileio --file-total-size=10G prepare</code></li>
</ol>
<p>上面的命令会在 EBS volume 上生成 10GB 的测试数据，在接下来的步骤中都可以重复使用</p>
<p><strong>试验 1 (上表中 Scenarios 1 &amp; 2 )</strong></p>
<p>执行以下步骤：</p>
<ol>
<li>启动一个 <code>ami-ca685ba0</code> 的 <code>t1.micro</code> 实例</li>
<li>挂载上 EBS volume </li>
<li>确保挂载成功</li>
<li>执行下面的代码 3 次（中间不要间隔太长时间）<code>sudo /home/ubuntu/Project3_1/sysbench --test=fileio --file-total-size=10G --file-test-mode=rndrw --max-time=300 --max-requests=0 run</code></li>
<li>把结果写到 <code>runner.sh</code> 中</li>
<li>使用另外的磁盘类型进行测试</li>
</ol>
<blockquote>
<p>暂时不要删除 EBS volume，之后还有用</p>
</blockquote>
<p><strong>试验 2 (上表中 Scenarios 3 &amp; 4 )</strong></p>
<ol>
<li>启动一个 <code>ami-ca685ba0</code> 的 <code>m3.large</code> 实例</li>
<li>挂载上 EBS volume </li>
<li>确保挂载成功</li>
<li>执行下面的代码 3 次（中间不要间隔太长时间）<code>sudo /home/ubuntu/Project3_1/sysbench --test=fileio --file-total-size=10G --file-test-mode=rndrw --max-time=300 --max-requests=0 run</code></li>
<li>把结果写到 <code>runner.sh</code> 中</li>
<li>使用另外的磁盘类型进行测试</li>
</ol>
<p>测试的话用 <code>./runner.sh scaling</code>，确定无误后使用 <code>./submitter -a dawang</code> 来进行提交，代码运行完成后输入提交密码即可。</p>
<h2 id="HBase__u64CD_u4F5C"><a href="#HBase__u64CD_u4F5C" class="headerlink" title="HBase 操作"></a>HBase 操作</h2><p>Apache HBase 是一个开源版本的 Google BigTable 分布式存储系统，其特点是分布式，可拓展，高性能，为大数据而生，在 Hadoop Distributed File System (HDFS) 上工作。HBase 在不同的服务器上把文件保存为重复的块，HDFS 保证其扩展性和可靠性。</p>
<p>在 HBase 中，输入按照行列排列，如下图所示：</p>
<p><img src="/images/14562066176431.jpg" alt="HBase table 的典型架构"></p>
<p>HBase 中的每一行都有对应的 row key，类似于主键，必须是唯一的。HBase 会自动根据 row key 来排列数据，默认按照字节顺序排序。</p>
<p>如上图所示，每一行包括：<code>rowkey</code>, <code>column_family</code>, <code>column</code> 和 <code>timestamp</code>，所以整个的映射变成 <code>(rowkey, column family, column, timestamp) -&gt; value</code>。Rowkey 和 value 都是简单的字节，所以只要能序列化成字节的都可以保存在 cell 中。这些 cell 会按照字典序排列，这是一个非常重要的特性，使得 HBase 支持快速搜索。</p>
<p>HBase 中的每一列都有列名，还可以进一步组织成 column family。所有的 column family 成员拥有共同的前缀，如上图所示，列 Metadata:Type 和列 Metadata:Language 都是 Metadata column family 的成员，而列 Content:Data 则属于 Content family。默认来说用冒号来分隔 column family 的前缀，这个前缀必须由能够打印的字符组成，后面的部分可以是任何字节。</p>
<p><strong>HBase 操作</strong></p>
<p>HBase 有四个主要的操作：Get, Put, Scan, 和 Delete.</p>
<ul>
<li>Get 操作会返回指定行的所有 cell</li>
<li>Put 操作可以添加新的记录或者更新已有记录</li>
<li>Scan 操作会根据条件遍历多行记录</li>
<li>Delete 操作会移除一条记录</li>
</ul>
<p>Get 和 Scan 操作的返回都是排好序的，依据为 rowkey, column family, family 成员，和时间戳（也就是最新的值会在最前面）。默认来说，Get, Scan 和  Delete 操作都是在数据最新的版本上的（也可以指定其他版本的数据）。Delete 操作一般来说会删除整行，但是也可以删除指定的 cell。</p>
<p><strong>HBase 架构</strong></p>
<p>HBase 是以 HBase 节点集群来进行组织的，节点有两种类型：master 和 slave（也叫 RegionServers）</p>
<p><img src="/images/14562066977793.jpg" alt="HBase 集群架构"></p>
<p>HBase 会动态分配数据表，这样支持大量的并行访问。一个 HBase 表在太大时会被分成多个 Region，一个 HBase Region 是一个 HBase 表的子集，但是 rowkey 的范围是连续的。每个  RegionServer 可以保存多个 Regions，但是一个 Region 只会在一个 RegionServer 上。</p>
<p>虽然一个 Region 只会在一个 RegionServer 上，但是这不意味着该 Region 部分的数据只能存在于一个 RegionServer 上。事实上，因为 HDFS 的复制机制，每个 Region 都会在其他 RegionServer 上有几份一模一样的拷贝。想要了解更多？查看 <a href="http://hbase.apache.org/book.html" target="_blank" rel="external">HBase Reference Guide</a> 以及 <a href="https://blogs.apache.org/hbase/" target="_blank" rel="external">HBase 博客</a>.</p>
<p>HBase 使用 Apache ZooKeeper 来协调控制整个 HBase 集群。Apache ZooKeeper 需要做的事情有：选择 master 节点，寻找 -ROOT- catalog table 以及节点注册（当新的 RegionServer 加入的时候）。由 ZooKeeper 选择出来的 master 节点会处理诸如 region 分配，失败处理，负载均衡等任务。</p>
<p>HBase 使用 HDFS 作为存储，但是同样支持其他文件系统（本地文件系统，甚至 Amazon S3）。</p>
<p>这个 <a href="https://youtu.be/lUOFLa0DKdc" target="_blank" rel="external">HBase Demo</a> 视频会介绍 HBase 的基本使用，虽然视频中的 EMR 版本较旧，但是对我们这次的任务没有什么影响。</p>
<ul>
<li>Costs = Instance + EMR costs</li>
<li>进入 EMR 页面 -&gt; 创建集群 -&gt; 输入名字 -&gt; 开启关闭保护 </li>
<li>选择 S3 的 log bucket -&gt; 打上标签 -&gt; 去掉 pig 和 hive，改为 HBase </li>
<li>选择 spot -&gt; 指定 keypair -&gt; 需要等待一段时间开启</li>
<li>ssh 到 master public dns，注意这里用户名是 hadoop 而不是 ubuntu</li>
<li><code>hbase shell</code> -&gt; <code>&gt; help</code> 查看帮助 -&gt; <code>&gt; status</code> 查看状态</li>
<li><code>create &#39;users&#39;, &#39;info&#39;</code> 创建表格</li>
<li><code>describe &#39;users&#39;</code> 可以查看表格内容</li>
<li><code>put &#39;user&#39;, &#39;johndeo&#39;, &#39;info&#39;, &#39;regularUser&#39;</code> 插入一条记录</li>
<li><code>get &#39;users&#39;, &#39;johndoe&#39;</code> 获取一条记录</li>
<li><code>scan &#39;users&#39;</code> 遍历某个表并输出</li>
<li><code>count &#39;users&#39;</code> 统计表的行数</li>
</ul>
<blockquote>
<p>注意：EMR 很贵，最好使用 spot instance</p>
</blockquote>
<h3 id="u4F7F_u7528_EMR__u521B_u5EFA_HBase__u96C6_u7FA4"><a href="#u4F7F_u7528_EMR__u521B_u5EFA_HBase__u96C6_u7FA4" class="headerlink" title="使用 EMR 创建 HBase 集群"></a>使用 EMR 创建 HBase 集群</h3><p>我们将使用 EMR 创建 HBase 集群。HBase 使用 Hadoop Distributed File System (HDFS) 来存储数据。默认来说 AWS 会直接用 EC2 内置的存储给 HDFS 使用，下面是具体的使用步骤：</p>
<ol>
<li>启动 EMR 集群：1 master &amp; 1 core <ul>
<li>在创建页面中选择 “Go to advanced options”</li>
<li>确保所有的实例都是 m1.large</li>
<li>确保 EMR 集群和存放 <code>runner.sh</code> 的实例在同一个区域</li>
<li>选择 AMI version 3.11.0 (hadoop version 2).</li>
<li>移除所有的已有服务(Pig &amp; Hive)并选择安装 HBase version 0.94.</li>
<li>指定 key-pair 以便 SSH 到 master 实例，ssh 的时候注意用户名是 hadoop</li>
<li>不要忘记设置标签</li>
<li>开启 “termination protection” 和 “keep-alive”</li>
</ul>
</li>
<li>master 和 core 节点的安全组都允许所有流量，使用 Master public DNS 来进行连接</li>
<li>ssh 到 master 节点之后，运行 <code>hadoop dfsadmin -report</code> 检查 HDFS 的状态</li>
</ol>
<h3 id="u8F7D_u5165_u6570_u636E_u5230_HBase"><a href="#u8F7D_u5165_u6570_u636E_u5230_HBase" class="headerlink" title="载入数据到 HBase"></a>载入数据到 HBase</h3><p>HBase 支持多种数据导入方法，这里我们介绍 Bulk Load 方法。</p>
<p>最直接的载入办法可以是在 MapReduce job 中使用 <code>TableOutputFormat</code> 类，也可以使用client APIs，但是这可能不是最有效率，因为 API 不支持 bulk loading.</p>
<p>Bulk Importing 会越过 HBase API 直接写入到数据文件中(HFiles)。使用 bulk load 可以减少 CPU 和网络带宽的占用。<code>ImportTsv</code> 就可以完成这个任务，虽然原本是为 TSV (Tab Separated Value) 格式设计的，但是通过设置参数，同样支持 CSV 文件，步骤如下：</p>
<ol>
<li>把 TSV/CSV 格式的数据集上传到 HDFS (Hadoop Distributed File System)<ul>
<li>File System (FS) shell 支持基本的文件操作比如 <code>Local FS</code>, <code>HFTP FS</code>, <code>S3 FS</code> 等等，可以通过 <code>hadoop fs &lt;args&gt;</code> 来调用</li>
<li>从 S3 bucket 获取 <code>million_songs_metadata.csv</code> 文件<ul>
<li><code>mkdir P3_1</code></li>
<li><code>cd P3_1</code></li>
<li><code>wget https://s3.amazonaws.com/15319-p31/million_songs_metadata.csv</code></li>
</ul>
</li>
<li>把下载下来的文件保存到 HDFS 中以便导入，具体命令需要自己寻找</li>
<li>可以用 <code>hadoop fs -ls /path/containing/your/uploaded/file</code> 来检测是否上传成功</li>
</ul>
</li>
<li>打开 HBase shell (<code>HBase shell</code>)并新建一个名为 songdata 的表(使用 <code>create</code> 命令，后面跟 column family name 的名字)。建立成功之后使用 <code>exit</code> 命令退出</li>
<li>为 HBase 表准备好 HFiles。使用 <code>ImportTsv</code> 命令把文件 <code>million_songs_metadata.csv</code> 中的数据传到 HDFS 中，名为 <code>importtsv.bulk.outputHbase</code>。这些 StoreFiles 之后会被载入到 HBase 中。注意这里我们使用 <code>track_id</code> 作为 row key，其他的列会成为 column family name (这里使用 ‘data’)。要了解 <code>ImportTsv</code> 的更多信息，请参考 <a href="http://hbase.apache.org/0.94/book/ops_mgt.html#importtsv" target="_blank" rel="external">official reference</a>.</li>
<li>正常启动的话，我们可以看到 MapReduce 工作的进程</li>
<li>检查 Map 步骤的输出来验证结果。通常来说应该会与数据集中的数据数量相等。注意，对应的输出文件应该是不存在的（不然会导致任务失败）</li>
<li>前面所做的所有工作都只是为了把数据保存到 HBase 中，但是此时 HBase 的表仍旧是空的（还没有添加对应的记录）</li>
<li>需要使用 CompleteBulkLoad 工具来完成数据上传，参考官方文档来使用</li>
<li>现在可以验证数据是否成功上传，打开 HBase shell 然后用以下命令来查看 <code>scan &#39;songdata&#39;</code></li>
<li>用 Ctrl-C 结束输出</li>
</ol>
<p>当然，除了这个方法，也可以在 MapReduce job 中使用 <code>TableOutputFormat</code> 或者其他 HBase client API。</p>
<h3 id="HBase__u67E5_u8BE2"><a href="#HBase__u67E5_u8BE2" class="headerlink" title="HBase 查询"></a>HBase 查询</h3><p>与 MySQL 类似，HBase 提供了查询的工具。在 HBase 中，数据存在 column 中，多个 column 组成 column family。我们可以用下面的指令来进行查询：</p>
<p><code>scan ‘table_name’, {COLUMNS =&gt; [‘column1’, ‘column2’, …], FILTER =&gt; “(FILTER1) … (FILTER2)”}</code></p>
<p>我们来做一个和之前类似的查询，找到所有 <code>artist_name</code> 以 “The Beatles” 开头的记录（是一个前缀匹配，不是子串匹配），查询如下：</p>
<p><code>scan &#39;songdata&#39;, {COLUMNS =&gt; &#39;data:artist_name&#39;, FILTER =&gt; &quot;SingleColumnValueFilter(&#39;data&#39;, &#39;artist_name&#39;, = , &#39;regexstring:^The Beatles.*&#39;)&quot;}</code></p>
<p>这里的列名的格式是 <code>(column family name):(column qualifier name)</code>。并且返回的数据中只包含了 <code>artist_name</code> 的数据，如果我们想多看一些数据，在 COLUMNS 部分多加一些内容，如：</p>
<p><code>scan &#39;songdata&#39;, {COLUMNS =&gt; [&#39;data:artist_name&#39;, &#39;data:title&#39;], FILTER =&gt; &quot;SingleColumnValueFilter(&#39;data&#39;, &#39;artist_name&#39;, = , &#39;regexstring:^The Beatles.*&#39;)&quot;}</code></p>
<p>同样，我们也可以添加更多的 FILTER，用逻辑运算符 AND, OR, WHILE 等来进行组合。比如说，如果我们想在原来条件的基础上增加另一个条件：其 title 以 W 或者以 W 之后的字母开头，那么命令就可以这么写：</p>
<p><code>scan &#39;songdata&#39;, {COLUMNS =&gt; [&#39;data:artist_name&#39;, &#39;data:title&#39;], FILTER =&gt; &quot;SingleColumnValueFilter(&#39;data&#39;, &#39;artist_name&#39;, = , &#39;regexstring:^The Beatles.*&#39;) AND SingleColumnValueFilter(&#39;data&#39;, &#39;title&#39;, &gt;= , &#39;binaryprefix:W&#39;)&quot;}</code></p>
<p>另外提一点，在 FILTER 中使用了某一列，就需要在 COLUMNS 列表中也加入对应的列名，不然就会被忽略的，更多信息可以参阅<a href="http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/admin_hbase_filtering.html" target="_blank" rel="external">这篇日志</a>.</p>
<h3 id="HBase_Java_API"><a href="#HBase_Java_API" class="headerlink" title="HBase Java API"></a>HBase Java API</h3><p>HBase 也有其 Java API，可以用来创建、查看、修改和删除表，同样也可以插入和查询。</p>
<p><strong>建立连接</strong></p>
<p>首先我们需要建立连接，例如：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">conf.set(<span class="string">"hbase.zookeeper.quorum"</span>, zookeeperAddress);</span><br><span class="line">conf.set(<span class="string">"hbase.zookeeper.property.clientport"</span>, <span class="string">"2181"</span>);</span><br><span class="line">HConnection conn = HConnectionManager.createConnection(conf);</span><br><span class="line">HTableInterface table = conn.getTable(tableName);</span><br></pre></td></tr></table></figure>
<p>前三行配置地址和端口，这里需要填写 master node 的 IP 地址。然后就可以创建 <code>HConnection</code> 并得到一个 <code>HTableInterface</code> 对象（用来处理特定 HBase 表）。</p>
<p>另一个创建 HBase table handler 的方法是</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HTable table = <span class="keyword">new</span> HTable(conf, tableName);</span><br></pre></td></tr></table></figure>
<p>不过在新版本中已经被弃用了（所以直接不写出来不就好了嘛）</p>
<p>最常见的操作是 Get 和 Scan，get 用来获取某一行，scan 用来对多行操作，一般来说 scan 比 get 慢。不过我们这里会使用 scan。</p>
<p>下面是一个简单的例子，我们打印出所有 <code>artist_name</code> 以 “The Beatles” 开头的记录。更多详细的使用方法请参考 <a href="https://hbase.apache.org/0.94/apidocs/" target="_blank" rel="external">HBase Java API 文档</a>.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create a new Scan object. By calling the default constructor, the entire table will be scanned.</span></span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Binary representation of the column family name</span></span><br><span class="line"><span class="keyword">byte</span>[] bColFamily = Bytes.toBytes(<span class="string">"data"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Binary representation of the column name.</span></span><br><span class="line"><span class="keyword">byte</span>[] bCol = Bytes.toBytes(<span class="string">"artist_name"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// This is used for regular expression matching. You should use different comparators based on specific requirements.</span></span><br><span class="line">RegexStringComparator comp = <span class="keyword">new</span> RegexStringComparator(<span class="string">"^The Beatles.*"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// This defines the filtering rules of our Scan object.</span></span><br><span class="line">Filter filter = <span class="keyword">new</span> SingleColumnValueFilter(bColFamily, bCol, CompareFilter.CompareOp.EQUAL, comp);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Associate the filtering rules to our Scan object.</span></span><br><span class="line">scan.setFilter(filter);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use this if your query will return multiple rows.</span></span><br><span class="line">scan.setBatch(<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get the scan result.</span></span><br><span class="line">ResultScanner rs = songsTable.getScanner(scan);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Each call of rs.next() will return one row.</span></span><br><span class="line"><span class="keyword">for</span> (Result r = rs.next(); r != <span class="keyword">null</span>; r = rs.next()) &#123;</span><br><span class="line">    <span class="comment">// r represents one row in the table. r.getValue returns the specific cell (determined by column family</span></span><br><span class="line">    <span class="comment">// and column name.</span></span><br><span class="line">    System.out.println(Bytes.toString(r.getValue(bColFamily, bCol)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Cleanup</span></span><br><span class="line">rs.close();</span><br></pre></td></tr></table></figure>
<p>看看 <a href="http://hbase.apache.org/0.94/book/client.filter.html" target="_blank" rel="external">HBase tutorial on Client Request Filters</a> 对完成这部分的任务也很有帮助。</p>
<h3 id="u89E3_u9898_u653B_u7565-3"><a href="#u89E3_u9898_u653B_u7565-3" class="headerlink" title="解题攻略"></a>解题攻略</h3><p>这部分的任务就是完成 <code>runner.sh</code> 中的 17-21 题，需要改动的文件是 <code>HBaseTasks.java</code>。可以用下面的代码来运行 demo</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">javac HBaseTasks.java</span><br><span class="line">java HBaseTasks demo</span><br></pre></td></tr></table></figure>
<p>会打印出所有 <code>artist_name</code> 以 “The Beatles” 开头的记录（大小写敏感）</p>
<p>先把需要的文件 scp 到本地 <code>scp -i demo.pem ubuntu@ec2-54-209-165-121.compute-1.amazonaws.com:~/Project3_1/HBaseTasks.java ./</code></p>
<p>问题列表(17 题开始)：</p>
<ol>
<li>(17)找到以 “Total” 开头 “Water 结尾的歌名</li>
<li>(18)找到 “Kanye West” 的歌曲的歌名，名称以 “Apologies” 或 “Confessions” 开头，大小写敏感</li>
<li>(19)找到歌手名以 “Bob Marley” 为前缀的一首歌的歌名，长度大于 400，年份是 2000 年之后（包括 2000 年）</li>
<li>(20)找到歌手名包含 “Consequence” 的一首歌的歌名，歌名包含 “Family” 并且 <code>artist_hotttnesss</code> 要大于 1</li>
<li>(21)找到歌手名以 “Gwen Guthrie” 为前缀的一首歌的歌名，歌名包含 “Love” 但不包含 “Bitter” 或者 “Never”，年份为 1990</li>
</ol>
<p>然后按照前面的指引开一个 EMR，注意一定要开启 SSH，不然开了等于白开，开启之后连接上去 <code>ssh -i demo.pem hadoop@ec2-52-90-21-43.compute-1.amazonaws.com</code></p>
<p>然后用 <code>hadoop dfsadmin -report</code> 检查状态，不过说已经弃用这种命令写法了，如下：</p>
<p><img src="/images/14562658234752.jpg" alt=""></p>
<p>然后我们创建一个文件夹并下载对应的 csv 文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir P3_1</span><br><span class="line"><span class="built_in">cd</span> P3_1</span><br><span class="line">wget https://s3.amazonaws.com/<span class="number">15319</span>-p31/million_songs_metadata.csv</span><br></pre></td></tr></table></figure>
<p>然后创建对应的 HDFS 目录，再把 csv 文件移过去：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /dawang</span><br><span class="line">hadoop fs -mkdir /dawang/csv</span><br><span class="line">hadoop fs -put ./million_songs_metadata.csv /dawang/csv/</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">hadoop fs -ls /dawang/csv/</span><br></pre></td></tr></table></figure>
<p><img src="/images/14562663033381.jpg" alt="hadoop fs -ls 结果"></p>
<p>然后进入 HBase Shell 操作 <code>hbase shell</code></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">001</span>:<span class="number">0</span>&gt; create <span class="string">'songdata'</span>,<span class="string">'data'</span></span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">002</span>:<span class="number">0</span>&gt; list</span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">003</span>:<span class="number">0</span>&gt; describe <span class="string">'songdata'</span></span><br><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">004</span>:<span class="number">0</span>&gt; exit</span><br></pre></td></tr></table></figure>
<p>结果如下所示：</p>
<p><img src="/images/14562664950809.jpg" alt=""></p>
<p>然后就需要具体的导入了，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=<span class="string">","</span> -Dimporttsv.bulk.output=/hfile_p31 -Dimporttsv.columns=HBASE_ROW_KEY,data:title,data:song_id,data:release,data:artist_id,data:artist_mbid,data:artist_name,data:duration,data:artist_familiarity,data:artist_hotttnesss,data:year songdata /dawang/csv/million_songs_metadata.csv</span><br><span class="line"></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /hfile_p31 songdata</span><br></pre></td></tr></table></figure>
<p>完成之后测试一下 <code>hbase shell</code>：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">hbase</span><span class="params">(main)</span></span>:<span class="number">001</span>:<span class="number">0</span>&gt; scan <span class="string">'songdata'</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/14562672790636.jpg" alt="结果输出"></p>
<p>有很多需要注意的地方，尤其是比较字符串的时候，有些坑是一定要踩的（爆炸感谢 @jiexing）。</p>
<p>测试的话用 <code>./runner.sh hbase</code>，确定无误后使用 <code>./submitter -a dawang</code> 来进行提交，代码运行完成后输入提交密码即可。</p>
<p>一些需要注意的地方：</p>
<ol>
<li>Java 代码中需要填写 HBase 的 master node 的 dns</li>
<li>每题的答案在一行里输出</li>
<li>设置正确的日志级别来防止不必要的输出</li>
</ol>
<blockquote>
<p>SCAN 操作是 O(N) 的，GET 操作是 O(logN)，比较好的方式是，通过精心设计的数据库，用两次 GET 操作拿到起始和结束的 rowkey，这样就有极大的效率提高，详情参考<a href="https://blog.cloudera.com/blog/2013/04/how-scaling-really-works-in-apache-hbase/" target="_blank" rel="external">这里</a></p>
</blockquote>
<h2 id="u53C2_u8003_u8D44_u6599"><a href="#u53C2_u8003_u8D44_u6599" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="http://www.runoob.com/linux/linux-comm-join.html" target="_blank" rel="external">Linux join命令</a></li>
</ol>
</div><div data-thread-key="vault/cc-14.html" data-title="云计算 第 14 课 文件 vs 数据库" data-url="http://wdxtub.com/vault/cc-14.html" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search" class="search-form-input"/><input type="hidden" name="sitesearch" value="http://wdxtub.com"/></form></div><div class="widget"><div class="widget-title">分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Game/">Game</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Gossip/">Gossip</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Memory/">Memory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Movie/">Movie</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading/">Reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Story/">Story</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Technique/">Technique</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Traveling/">Traveling</a></li></ul></div><div class="widget"><div class="comments-title">最近评论</div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title">友情链接</div><ul></ul><a href="http://jackqdyulei.github.io" title="Lei YU" target="_blank">Lei YU</a><ul></ul><a href="http://wdxtub.com/bookclips/" title="我的书摘" target="_blank">我的书摘</a><ul></ul><a href="http://wdxtub.com/interview/" title="刷题笔记" target="_blank">刷题笔记</a></div></div></div></div><div id="footer">© <a href="/." rel="nofollow">小土刀.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><a id="rocket" href="#top" class="show"></a><script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/totop.js" type="text/javascript"></script><script src="/js/fancybox.pack.js" type="text/javascript"></script>
<script src="/js/jquery.fancybox.js" type="text/javascript"></script><link rel="stylesheet" href="/css/jquery.fancybox.css" type="text/css"><script>var duoshuoQuery = {short_name:'wdxblog'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div><!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body></html>