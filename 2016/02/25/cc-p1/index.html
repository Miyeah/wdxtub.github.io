<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="一个逗比的碎碎念"><title>云计算 课程项目 1 搭建高性能数据获取网络服务 | 小土刀</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">云计算 课程项目 1 搭建高性能数据获取网络服务</h1><a id="logo" href="/.">小土刀</a><p class="description">Agony is my triumph</p></div><div id="nav-menu"><a href="/." class="current"><i class="icon-home"> 首页</i></a><a href="/about/"><i class="icon-guestbook"> 技术</i></a><a href="/life/"><i class="icon-about"> 生活</i></a><a href="/archives/"><i class="icon-archive"> 归档</i></a><a href="/atom.xml"><i class="icon-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post post-page"><h1 class="post-title">云计算 课程项目 1 搭建高性能数据获取网络服务</h1><div class="post-meta">2016-02-25 | <span class="categories">分类于<a href="/categories/Technique/"> Technique</a></span></div><span data-thread-key="2016/02/25/cc-p1/" class="ds-thread-count"></span><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#u4EFB_u52A1_u76EE_u6807"><span class="toc-number">1.</span> <span class="toc-text">任务目标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#u5E38_u89C1_u95EE_u9898"><span class="toc-number">1.1.</span> <span class="toc-text">常见问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u4EFB_u52A1_u6982_u89C8"><span class="toc-number">2.</span> <span class="toc-text">任务概览</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#u524D_u7AEF"><span class="toc-number">2.1.</span> <span class="toc-text">前端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ETL"><span class="toc-number">2.2.</span> <span class="toc-text">ETL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#u540E_u7AEF"><span class="toc-number">2.3.</span> <span class="toc-text">后端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Additional_Resources_and_References"><span class="toc-number">2.4.</span> <span class="toc-text">Additional Resources and References</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u9636_u6BB5_u4E00"><span class="toc-number">3.</span> <span class="toc-text">阶段一</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Query_1__28Heartbeat_and_Authentication_29"><span class="toc-number">4.</span> <span class="toc-text">Query 1 (Heartbeat and Authentication)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Query_2__28Text_Cleaning_and_Analysis_29"><span class="toc-number">5.</span> <span class="toc-text">Query 2 (Text Cleaning and Analysis)</span></a></li></ol></div></div><div class="post-content"><p>从这次作业开始，就要小组作业和个人作业并行了。这次的项目主要是在云上分析 Twitter 的相关内容，与以前 GB 级数据不一样，这次我们要处理 TB 级的数据，还是很刺激的。另，这是小组作业，在此先感谢我的队友 @leiyu 和 @shushanc</p>
<a id="more"></a>
<hr>
<p>主要任务是：Twitter Analytics on the Cloud</p>
<h2 id="u4EFB_u52A1_u76EE_u6807"><a href="#u4EFB_u52A1_u76EE_u6807" class="headerlink" title="任务目标"></a>任务目标</h2><ol>
<li>在一定预算限制下利用所学知识搭建一个性能高可靠性又好的 web 服务</li>
<li>设计、开发、部署和优化服务器以处理比较高的负载（大约每秒上万次请求）</li>
<li>在一个大数据集上（约 1TB）实现 Extract Transform and Load (ETL) 并载入到 MySQL  和 HBase 中</li>
<li>设计 MySQL 和 HBase 的 schema 并优化配置来提高性能</li>
<li>探索寻找基于云的 web 服务中潜在瓶颈的方法，并提高性能</li>
</ol>
<p>我们需要搭建并优化一个有两个组件的 web 服务，前端负责处理请求，后端负责查询数据，架构如图：</p>
<p><img src="/images/14564545973314.jpg" alt="系统架构"></p>
<ol>
<li>前端：能够接收和响应查询请求的 web 服务<ul>
<li>用过通过指定网址发送 HTTP GET 请求来访问 web 服务。不同的请求有不同的地址，后面跟有不同的参数</li>
<li>要返回适当的响应，并且一定要按照指定的格式</li>
<li>Web 服务需要在持续若干小时的测试中正常运行</li>
<li>Web 服务不能拒绝请求，应该能够承受高负载</li>
</ul>
</li>
<li>后端：保存用来查询的数据文件<ul>
<li>需要评估 SQL(MySQL) 和 NoSQL(HBase)</li>
<li>比较不同数据集不同查询类型的性能表现，然后由此来决定如何实现后端</li>
</ul>
</li>
<li>Web 服务应该在不超过预算的情况下达到指定的吞吐量</li>
<li>钱花得越少越好</li>
<li>前端和后端均使用 M 系列的实例，批量处理的时候注意使用竞价实例（总之就是要省钱）</li>
</ol>
<blockquote>
<p>数据集</p>
</blockquote>
<ul>
<li>数据集地址为：<code>s3://cmucc-datasets/twitter/s16/</code></li>
<li>大小超过 1 TB，还会有重复和损坏的记录</li>
<li><a href="http://en.wikipedia.org/wiki/JSON" target="_blank" rel="external">JSON</a> 格式，每行表示一个 tweet，具体看<a href="https://dev.twitter.com/docs/platform-objects/tweets" target="_blank" rel="external">Twitter API</a>.</li>
<li>字符编码是 unicode，建议使用下面的库<ul>
<li>simple json/gson(Java)</li>
<li>标准库中的 json module(python)</li>
</ul>
</li>
</ul>
<blockquote>
<p>进度安排及制品</p>
</blockquote>
<p>项目分三个阶段，每个阶段完成不同的任务，每个阶段完成之后都需要提交制品：</p>
<ol>
<li>性能数据</li>
<li>开销分析</li>
<li>源代码</li>
<li>问答题的答案</li>
<li>阶段报告，包括设计选择和制品描述 </li>
</ol>
<h3 id="u5E38_u89C1_u95EE_u9898"><a href="#u5E38_u89C1_u95EE_u9898" class="headerlink" title="常见问题"></a>常见问题</h3><blockquote>
<p>如何提交测试请求？</p>
</blockquote>
<p>提交 web 服务的地址即可开始测试，提供不同时间长度的测试，可以有针对性进行选择，比方说如果只是为了检测服务能否正常运行，那么可能几分钟的测试就够了；如果想要看看长时间能否工作，就需要长时间的测试。</p>
<blockquote>
<p>到底测试什么？</p>
</blockquote>
<p>简单来说，每次提交测试请求之后，系统会产生特定的请求并发送到之前填写的地址，会检测性能和正确性。</p>
<blockquote>
<p>为什么提交不了请求了？</p>
</blockquote>
<p>为了省钱，每个队伍同时只能用一个测试在跑（或者在排队）</p>
<p>还可以取消当前的测试请求并重新提交</p>
<blockquote>
<p>如何计算分数？</p>
</blockquote>
<p>主要考察下面几点</p>
<ul>
<li>吞吐量：测试期间平均 RPS</li>
<li>延迟：平均每个请求的延迟</li>
<li>错误率：不返回 2XX 都是错误</li>
<li>正确率：检测是否返回正确的内容，注意仔细检查格式</li>
</ul>
<p>具体计算公式为：</p>
<ul>
<li>有效吞吐量 = 吞吐量 <em> (100 - 错误率 / 100) </em> (正确率 / 100)</li>
<li>原始分 = 有效吞吐量 / 目标吞吐量</li>
</ul>
<p>注意，错误率与正确率都会极大影响最后的分数</p>
<h2 id="u4EFB_u52A1_u6982_u89C8"><a href="#u4EFB_u52A1_u6982_u89C8" class="headerlink" title="任务概览"></a>任务概览</h2><h3 id="u524D_u7AEF"><a href="#u524D_u7AEF" class="headerlink" title="前端"></a>前端</h3><p>This task requires you to build the front end system of the web service. The front end should accept RESTful requests and send back responses.</p>
<p>Design constraints</p>
<ul>
<li>Execution model: you can use any web framework you want, but you must compare at least two web frameworks and summarize them in the your report for this phase.</li>
<li>Spot instances are highly recommended during the development period, otherwise it is very likely that you will exceed the overall budget for this phase and fail the project.</li>
</ul>
<p>Recommendations</p>
<p>You might want to consider using auto-scaling because there could be fluctuations in the load over the test period. To test your front end system, use the Heartbeat query (q1). It will be wise to ensure that your system comes close to satisfying the minimum throughput requirement of heartbeat requests before you move forward. However, as you design the front end, make sure to account for the cost. Write an automatic script, or make a new AMI to configure the whole front end instance. It can help to rebuild the front end quickly, which may happen several times in the building process. Please terminate your instances when not needed. Save time or your cost will increase higher than the budget and lead to failure.</p>
<p>Hints</p>
<p>Although we do not have any constraints on the front end, performance of different web frameworks vary a lot. Choosing a slow web framework may have a negative impact on the throughput of every query. Therefore, we strongly recommend that you do some investigation about this topic before you start. <a href="https://www.techempower.com/benchmarks/" target="_blank" rel="external">Techempower</a> provides a very complete benchmark for mainstream frameworks, you may find it helpful.</p>
<p>You can also compare the performance of different frameworks under our testing environment by testing q1 since it is just a heartbeat message and has no interactions with the back end. Please also think about whether the front end framework you choose has API support for MySQL and HBase.</p>
<p>In your report, you need to be able to convince us that your team has done enough exploration and detailed comparison of different frameworks which led your team to make an informed decision.</p>
<h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><p>This task requires you to load the Twitter dataset into the database using the extract, transform and load (ETL) process in data warehousing. In the extract step, you will extract data from an outside source. For this phase, the outside source is a JSON Twitter dataset of tweets stored on S3, containing about 200 million tweets. The transform step applies a series of functions on the extracted data to realize the data needed in the target database. The transform step relies on the schema design of the target database. The load phase loads the data into the target database.</p>
<p>You will have to carefully design the ETL process using AWS resources. Considerations include the programming model used for the ETL job, the type and number of instances needed to execute the job. Given the above, you should be able to come up with an expected time to complete the job and hence an expected overall cost of the ETL job.</p>
<p>Once this step is completed, you should backup your database to save cost. If you use EMR, you can backup HBase on S3 using the command:</p>
<p><code>aws emr create-hbase-backup --cluster-id j-3AEXXXXXX16F2 --dir s3://mybucket/backups/j-3AEXXXXXX16F2 --consistent</code></p>
<p>This backup command will run a Hadoop MapReduce job and you can monitor it from both Amazon EMR debug tool or by accessing the Jobtracker. To learn more about HBase S3 backup, please refer to the following link:</p>
<p><a href="http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hbase-backup-restore.html" target="_blank" rel="external">http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hbase-backup-restore.html</a></p>
<p>Design constraints</p>
<ul>
<li>Programming model: you can use any programming model you see fit for this job.</li>
<li>AWS resources: You should use SPOT instances for this step otherwise it is very likely that you will exceed the budget and fail the project.</li>
</ul>
<p>Recommendations</p>
<p>Always test the correctness of your system using a tiny dataset (example 200MB). If your ETL job fails or produces wrong results, you will be burning through your budget. After the database is populated with data, it will be wise to test the throughput of your back end system for different types of queries. Ensure that your system produces correct query responses for all query types.</p>
<p>Hints</p>
<p>Think about the schema design of your database before attempting the ETL job. How to do ETL correctly and efficiently will be a critical part for your success in this project. Notice that ETL on a large dataset could take 10 - 30 hours for just a single run, so it will be very painful to do it more than once, although this may be inevitable since you will be refining your schema throughout your development.</p>
<p>You may find your ETL job extremely time consuming because of the massive dataset and the poor design of your ETL process. Due to many reasons that could lead to the failure of your ETL job, please start thinking about your database schema and your ETL job as early as possible.</p>
<p>Try to utilize parallelism as much as possible, loading the data with a single process/thread is a waste of time and computing resources, MapReduce may be your friend, though other methods work so long as they can accelerate your ETL process.</p>
<h3 id="u540E_u7AEF"><a href="#u540E_u7AEF" class="headerlink" title="后端"></a>后端</h3><p>For your system to provide responses for q2-q4, you need to store the required data in a back-end database. Your front end system connects to the back end and queries it in order to get the response and then sends the response to the requester.</p>
<p>You are going to use both HBase and MySQL in this phase. We will provide you with some references that will accelerate your learning process. You are expected to read and learn about these database systems on your own in order to finish this task.</p>
<p>This task requires you to build the back end system. It should be able to store whatever data you need to satisfy the query requests. You should use spot instances for the back end system development. You need to consider the design of the table structure for the database. The design of the table significantly affects the performance of a database.</p>
<p>In this task you should also test and make sure that your front end system connects to your back end database and can get responses for queries.</p>
<p>Recommendations</p>
<p>Test the functionality of the database with a few dummy entries before loading your entire dataset. The functionality test ensures that your database can correctly produce the response to the q2-q4 queries.</p>
<p>References</p>
<ul>
<li>MySQL<ul>
<li><a href="http://dev.mysql.com/doc/" target="_blank" rel="external">http://dev.mysql.com/doc/</a></li>
<li>Project 3</li>
</ul>
</li>
<li>HBase<ul>
<li><a href="https://hbase.apache.org/" target="_blank" rel="external">https://hbase.apache.org/</a></li>
<li>Project 3</li>
</ul>
</li>
</ul>
<h3 id="Additional_Resources_and_References"><a href="#Additional_Resources_and_References" class="headerlink" title="Additional Resources and References"></a>Additional Resources and References</h3><p>Resources</p>
<ol>
<li><a href="https://www.techempower.com/benchmarks/" target="_blank" rel="external">Benchmarks of web servers</a></li>
<li><a href="http://www.percona.com/blog/2010/05/04/goal-driven-performance-optimization-white-paper-available/" target="_blank" rel="external">Schwartz, B., and P. Zaitsev. “A brief introduction to goal-driven performance optimization.” White paper, Percona (2010).</a></li>
<li><a href="http://www.percona.com/resources/mysql-webinars/practical-mysql-performance-optimization" target="_blank" rel="external">Practical MySQL Performance Optimization</a></li>
<li><a href="http://refcardz.dzone.com/refcardz/hbase" target="_blank" rel="external">HBase Cheat Sheet</a></li>
</ol>
<p>Additional References</p>
<p>These are interesting papers that deal with the theory and the core problems that you will be solving for the 15619Poject. You may choose to read them if you want to understand more about how Internet-scale companies (Google, Facebook, Twitter) achieve performance at scale.</p>
<p>Architecting web servers</p>
<ol>
<li><a href="http://vts.uni-ulm.de/docs/2012/8082/vts_8082_11772.pdf" target="_blank" rel="external">Erb, Benjamin. “Concurrent programming for scalable web architectures.” Informatiktage. 2012.</a></li>
<li><a href="https://www.ece.cmu.edu/~ece845/docs/pariag-2007.pdf" target="_blank" rel="external">Pariag, David, et al. “Comparing the performance of web server architectures.” ACM SIGOPS Operating Systems Review. Vol. 41. No. 3. ACM, 2007.</a></li>
<li><a href="http://mmcgrana.github.io/2010/07/threaded-vs-evented-servers.html" target="_blank" rel="external">McGranaghan, Mark. “Threaded vs Evented Servers”</a></li>
<li><a href="https://www.dre.vanderbilt.edu/~schmidt/PDF/globalinternet.pdf" target="_blank" rel="external">Hu, James C., Irfan Pyarali, and Douglas C. Schmidt. “Measuring the impact of event dispatching and concurrency models on web server performance over high-speed networks.” Global Telecommunications Conference, 1997. GLOBECOM’97., IEEE. Vol. 3. IEEE, 1997.</a></li>
</ol>
<p>Clustering web servers</p>
<ol>
<li><a href="http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1083&amp;context=csearticles" target="_blank" rel="external">Schroeder, Trevor, Steve Goddard, and Byrov Ramamurthy. “Scalable web server clustering technologies.” Network, IEEE 14.3 (2000): 38-45.</a></li>
<li><a href="http://www.ics.uci.edu/~cs230/reading/DLB.pdf" target="_blank" rel="external">Cardellini, Valeria, Michele Colajanni, and S. Yu Philip. “Dynamic load balancing on web-server systems.” IEEE Internet computing 3.3 (1999): 28-39.</a></li>
<li><a href="http://uu.diva-portal.org/smash/get/diva2:443102/FULLTEXT01.pdf" target="_blank" rel="external">Paudyal, Umesh. “Scalable web application using node.js and couchdb.” (2011).</a></li>
</ol>
<p>Optimizing a Multi-tier System</p>
<ol>
<li><a href="http://www.linuxjournal.com/article/7451" target="_blank" rel="external">Fitzpatrick, Brad. “Distributed caching with memcached.” Linux journal 2004.124 (2004): 5.</a></li>
<li><a href="http://www.linuxjournal.com/content/speed-your-web-site-varnish" target="_blank" rel="external">Graziano, Pablo. “Speed up your web site with Varnish.” Linux Journal 2013.227 (2013): 4.</a></li>
<li><a href="http://www.linuxjournal.com/magazine/nginx-high-performance-web-server-and-reverse-proxy" target="_blank" rel="external">Reese, Will. “Nginx: the high-performance web server and reverse proxy.” Linux Journal 2008.173 (2008): 2.</a></li>
</ol>
<p>Scalable and Performant Data Stores</p>
<ol>
<li><a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" target="_blank" rel="external">DeCandia, Giuseppe, et al. “Dynamo: amazon’s highly available key-value store.” ACM SIGOPS Operating Systems Review. Vol. 41. No. 6. ACM, 2007.</a></li>
<li><a href="http://www.cattell.net/datastores/Datastores.pdf" target="_blank" rel="external">Cattell, Rick. “Scalable SQL and NoSQL data stores.” ACM SIGMOD Record 39.4 (2011): 12-27.</a></li>
</ol>
<p>Web Server Performance Measurement</p>
<ol>
<li><a href="http://www.oocities.org/webserverperformance/webmodel.pdf" target="_blank" rel="external">Slothouber, Louis P. “A model of web server performance.” Proceedings of the 5th International World wide web Conference. 1996.</a></li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.61.3268&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">Banga, Gaurav, and Peter Druschel. “Measuring the Capacity of a Web Server.” USENIX Symposium on Internet Technologies and Systems. 1997.</a></li>
<li><a href="https://www.mnot.net/blog/2011/05/18/http_benchmark_rules" target="_blank" rel="external">Nottingham, Mark. “On HTTP Load Testing”</a></li>
</ol>
<h2 id="u9636_u6BB5_u4E00"><a href="#u9636_u6BB5_u4E00" class="headerlink" title="阶段一"></a>阶段一</h2><p>基本要求</p>
<ul>
<li>Tag all of your instances with Key: 15619project and Value: phase1 for all resources.</li>
<li>In addition to the tag above, all instances in your HBase cluster should be tagged with Key: 15619backend and Value: hbase, and all instances with MySQL installed should be tagged with Key: 15619backend and Value: mysql.</li>
<li>In addition to the project tag above, no other tag is required for instances used to do ETL jobs.</li>
<li>You can use ANY instances for ETL and debugging.</li>
<li>You can ONLY use instances in the M family which are smaller than or equal to large for your web service (both your front-end and back-end systems).</li>
<li>You can use any free AMI as your base. You should be building your own AMIs for this project.</li>
<li>Your web service (each test submitted to the website) must have a maximum cost of $0.85 per hour (this includes on-demand EC2, storage, EMR and ELB costs. Ignore Network and Disk I/O costs). Even if you use spot pricing, the constraints that apply to your web service pertain to on-demand pricing.</li>
<li>You will have a budget $40/team for all the tasks in this phase (Phase 1).</li>
</ul>
<p>Phase 1 accounts for 10% of the total grade for this 15619Project. You need to finish all the tasks in order to move on to the next phase which will add a new query. Your success in the next phases is highly dependent on how much you have explored, learned and achieved in Phase 1. So, even though it only accounts for 10%, Phase 1 has a huge impact on the overall success of the project and should be taken very seriously. Think of it as an opportunity to learn and explore at a reduced cost.</p>
<p><img src="/images/14564602178732.jpg" alt="任务要求"></p>
<p><img src="/images/14564603584769.jpg" alt="惩罚措施"></p>
<p>Reference Server</p>
<p>We have also provided you with a <a href="http://q1-1848733628.us-east-1.elb.amazonaws.com/" target="_blank" rel="external">reference server</a> in order to check the correctness of your results. We highly recommend that you use this server to check the correctness of your output files before loading the data into your database. You can also use this server to figure out possible encoding problems that you may encounter in Q2.</p>
<p>In the first phase of the 15619Project, you will build a web service from scratch. Your web service has to respond to two types of queries. For one of the queries, data should fetched from a storage system, which you must design and control. Your web service should be able to connect to two different back end storage systems (MySQL and HBase). A query is an input generated by a test system, which requires a fixed response. Grading depends on the accuracy and performance of your web service’s response to these queries.</p>
<p>Your web service’s front end will have to handle the following query types through HTTP GET requests on port 80 [you cannot use any other port].</p>
<p>This phase is designed to ensure that you have the required skills to cope with more complicated queries, so please allocate enough time to finish the requirements by the deadline.</p>
<p>Specifically, you will design and develop a front-end system (a highly parallel, concurrent web server) that can attach to either of two back-end systems (HBase and MySQL). In this phase you will be expected to learn about the advantages and disadvantages of each type of back-end database system.</p>
<p>A report must be written for this phase that conforms to a <a href="https://docs.google.com/document/d/1VOjU9JRAZG49PSrKnJtMOjSj5e1Ugbv_cQj7Mc_krd0/edit?usp=sharing" target="_blank" rel="external">template</a>.</p>
<h2 id="Query_1__28Heartbeat_and_Authentication_29"><a href="#Query_1__28Heartbeat_and_Authentication_29" class="headerlink" title="Query 1 (Heartbeat and Authentication)"></a>Query 1 (Heartbeat and Authentication)</h2><p>Target Throughput: 25000 rps</p>
<p>This query asks about the state of the web service. The front end server responds with the project team id, AWS account id, the current timestamp and a decrypted message. It is generally used as a heartbeat mechanism, but it could be used here to test whether your front end system can handle varying loads. It also authenticates the server, as a client can send it encrypted messages, which are only accessible with the correct secret key.</p>
<p>For each request to q1, the load generator will generate a large message key <code>Y</code>, and encrypted text. The text is encrypted by key <code>Z</code>, and <code>Z</code> is the greatest common divisor (GCD) of <code>X</code> and <code>Y</code>.</p>
<p>Now that you have the key <code>Z</code> used to encrypt the message. You may wonder what protocol to use. AES? DES? No!!! Despite warnings from the SIGINT community, we realize that proprietary encryption is the only secure way to hide from the MSB (since they’ve cracked everything else!!!) Hence we use the mythical Phaistos Disc Cipher (PDC).</p>
<p>PDC is designed for uppercase English (A-Z) messages that have a perfect square length (4,9,16,25…). We will use a toy example to demonstrate how the encryption scheme works. Remember, you will implement decryption - given the key and the ciphertext, you must retrieve the original message.</p>
<p>PDC has three steps: KeyGen, Caesarify and Spiralize.</p>
<ol>
<li>In the KeyGen step, a big integer <code>Y</code> is randomly selected, we calculate the largest common divisor of <code>X</code> and Y, and note it as <code>Z</code>.</li>
<li>In the Caesarify step, a minikey <code>K = 1 + Z % 25</code> is derived from key <code>Z</code>. Using <code>K</code>, the characters of the message <code>M</code> are linearly shifted (see the figure below) to produce the intermediate text <code>I</code>.</li>
<li>In the Spiralize step, the message is now written into a square array in a spiral way (see the example below) and read line by line. This reorders the characters of your message. The final message produced is the ciphertext <code>C</code>.</li>
</ol>
<p>Example of writing 1, 2, 3, 4 … 15, 16 into a 4x4 spiral matrix:</p>
<p><img src="/images/14564607523532.jpg" alt="Spiral Matrix Example"></p>
<p>Example of Phaistos Disc Cipher encryption</p>
<p><img src="/images/14564607621256.jpg" alt="Phaistos Disc Cipher"></p>
<p>Remember, your web service must implement the reverse process: Given the ciphertext <code>C</code> and the key <code>Y</code>, you must derive the intermediate <code>Z</code> using your secret key <code>X</code>. Then use the minikey <code>Z</code> to decrypt the message.</p>
<p>Request Format</p>
<p><code>GET /q1?key=&lt;large_number&gt;&amp;message=&lt;uppercase_ciphertext_message_C&gt;</code></p>
<p>Response Format</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TEAMID,TEAM_AWS_ACCOUNT_ID\<span class="keyword">n</span></span><br><span class="line">yyyy-MM-dd HH:mm:ss\<span class="keyword">n</span></span><br><span class="line">[The decrypted message <span class="keyword">M</span>]\<span class="keyword">n</span></span><br></pre></td></tr></table></figure>
<p>The time in the response should be the current time in Pittsburgh, Eastern Time</p>
<p>Sample Request</p>
<p><code>GET /q1?key=4024123659485622445001958636275419709073611535463684596712464059093821&amp;message=URYEXYBJB</code></p>
<p>Sample Response</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TeamCoolCloud,<span class="number">1234</span>-<span class="number">0000</span>-<span class="number">0001</span></span><br><span class="line"><span class="number">2004</span>-<span class="number">08</span>-<span class="number">15</span> <span class="number">16</span>:<span class="number">23</span>:<span class="number">42</span></span><br><span class="line">HELLOWORK</span><br></pre></td></tr></table></figure>
<p>Requirements</p>
<ol>
<li>Implement the front end service as specified above.</li>
<li>You must satisfy the minimum effective throughput requirement of 25000 rps for full credit.</li>
<li>Compare at least two web frameworks, record your experiences and observations. In the report, you must cover these aspects of the web frameworks: RPS achieved, resource utilization (CPU, Mem), programming difficulty, major difference of the two frameworks, and why one is better than the other (others) for this particular query. Comprehensive comparison is encouraged.</li>
</ol>
<p>Notes</p>
<p>As you can see, for this very first query, you don’t have to implement anything on the backend. But it still takes some effort to design and implement your front end server. You are free to use any language and any web framework you like. But you must compare at least two different web frameworks in order to make an informed decision. In the report of Query 1, your team is required to provide the comparison of at least two web frameworks to reflect the process and evaluation criteria used to reach your decision.</p>
<h2 id="Query_2__28Text_Cleaning_and_Analysis_29"><a href="#Query_2__28Text_Cleaning_and_Analysis_29" class="headerlink" title="Query 2 (Text Cleaning and Analysis)"></a>Query 2 (Text Cleaning and Analysis)</h2><p>Target Throughput: 10000 rps</p>
<p>You are NOT ALLOWED to use ANY existing caching applications (Redis, Memcached, etc.) or ANY existing databases except MySQL and HBase in the team project. However, you are allowed to write your own cache application manually.</p>
<p>ANY violation of this rule will result in penalties (-100% at least).</p>
<p>Hints:</p>
<p>ETL: You can use all kinds of instances in ETL phase. Other instance families in EMR, such as the c family (Compute Optimized) and r family (Memory Optimized), may be appropriate based on your algorithm. Don’t restrict your choice only to the m family (General Purpose) machines.</p>
<p>MySQL: you can refer to this for MySQL 5.5 official optimization document. You may switch to other MySQL versions if needed.</p>
<p>HBase: Please review the writeup of P3.1 (especially the [Did you know] part) and recall what you have done in P3.1. These experiences will be very useful when you design your HBase schema. You can choose to set up an Hbase cluster either using EMR or manually using EC2. Please consider the potential performance benefit of deploying your own HBase cluster as well as the cost before making any decisions. Refer to this if you want to set up an HBase cluster manually. You may also need to install Hadoop and ZooKeeper before installing HBase. You can also try other Hadoop distributions such as CDH developed by Cloudera.</p>
<p>For all queries (Q2 onwards) you will utilize a real Twitter data set that we collected. You can download either part or the whole data set from S3, the URL is <code>s3://cmucc-datasets/twitter/s16/part-00XXX</code>, where XXX is from 000 to 661.</p>
<p>This query asks for the tweet(s) posted by a given user using a specific hashtag. This query tests your ability to apply what you learned in the course to process a large amount of data and to build an efficient database in the back-end.</p>
<p>Q2 provides a user id and a hashtag (refer to this for more information), for which you need to respond with the following four pieces of information of ALL tweet(s) that belong(s) to the user and have the specified hashtag.</p>
<ul>
<li>The sentiment density (will be specified in following section) of the tweet text,</li>
<li>The created time of the tweet,</li>
<li>The tweet id, and</li>
<li>The censored tweet text. Note that there are many corner cases in the tweet text, e.g. emoji, backslashes, characters from other languages, etc. You need to deal with them carefully.</li>
</ul>
<p>Here is how you can obtain this information:</p>
<ol>
<li>The sentiment density should be calculated using the tweet text. Detailed specification will be given later on.</li>
<li>The tweet id can be fetched from the ‘id’ or the ‘id_str’ field.</li>
<li>The created time can be fetched from ‘created_at’ field.</li>
<li>The tweet text can be fetched with the ‘text’ field. Text censoring should be done AFTER sentiment density is calculated (keep reading).</li>
<li>The hashtag(s) can be fetched from the ‘entities’ field. If the same hashtag is used multiple times in one tweet, you should return that tweet only ONCE.</li>
</ol>
<blockquote>
<p>Duplicate Tweets</p>
</blockquote>
<p>There may be duplicate tweets (tweets with the same id) in the raw data. Remove all duplicates. You should return one tweet only ONCE.</p>
<blockquote>
<p>Malformed Tweets</p>
</blockquote>
<p>Any tweets that satisfy any one of the following conditions should be regarded as malformed tweets:</p>
<ul>
<li>Both id and id_str are missing or empty</li>
<li>Any one of ‘created_at’, ‘text’ and ‘entities’ fields is missing or empty.</li>
<li>Cannot be parsed as a JSON object.</li>
</ul>
<p>You should filter them out of the data set.</p>
<blockquote>
<p>Hashtag matching</p>
</blockquote>
<p>The tweets that are returned should use exactly the SAME hashtag as the hashtag that was provided by the query. Meaning, each character in the hashtag is exactly the same. If you regard the hashtag string as a byte array, two hashtags are the same if and only if every byte in the two hashtags are exactly the same.</p>
<p>For example: “Naive”, “naive” and “naïve” are three different hashtags, since ‘n’(U+006E) and ‘N’(U+004E), ‘i’(U+0131) and ‘ï’(U+00EF) are different characters.</p>
<blockquote>
<p>Sentiment Density</p>
</blockquote>
<p>The sentiment density of a tweet is a measurement of its sentimental score per effective word. You may need to follow four steps to calculate the sentiment density.</p>
<p>Text Split</p>
<p>You may need to split the tweet text into a list of words. A word is defined as one or more consecutive alphanumeric characters ([a-zA-Z0-9]+) separated by non-alphanumeric character(s) ([^a-zA-Z0-9]).</p>
<p>Sentiment Score Calculation</p>
<p>We provide a list of lowercase English words that have a corresponding sentiment score. For each word in the tweet’s text, convert it to lowercase and check if it has a sentiment score in the provided list. If it has a score, add the word’s sentiment score to the score for that tweet. The score of all tweets is initialized as zero.</p>
<p>This list is from the AFINN dataset. AFINN is a list of English words rated for valence with an integer between minus five (negative) and plus five (positive). The words have been manually labeled by Finn Årup Nielsen in 2009-2011. Download it here.</p>
<p>For example: “I love Cloud Computing” has a sentimental score of 3, since “love” is in the sentimental score list and has the sentimental score as 3.</p>
<p>Effective Word Count Calculation</p>
<p>We provide a list of lowercase English stop words that should NOT be counted as an effective word. The list of stop words can be downloaded here</p>
<p>Effective Word Count(EWC) should be calculated as: EWC = total number of words - total number of stop words</p>
<p>Here is an example:</p>
<p>“I love Cloud Computing” has EWC as 3, since “I” is a stop word.</p>
<p>Sentiment Density Calculation</p>
<p>If you got a 0 in EWC, Sentiment Density should be 0, regardless of the value of the Sentiment Score.</p>
<p>Otherwise, Sentiment Density should be calculated as: Sentiment Score / Effective Word Count.</p>
<p>All results should be rounded off to 3 decimal places, using the Round 1/2 away from zero method. (1 -&gt; 1.000, 1.1-&gt;1.100, 1.0005 -&gt; 1.001, 1.9999 -&gt; 2.000, -1 -&gt; -1.000, -1.1-&gt; -1.100, -1.0005 -&gt; -1.001, -1.9999 -&gt; -2.000, etc.)</p>
<p>Here is an example:</p>
<p>“I love Cloud Computing” has a Sentiment Density of 1.000, since its Sentimental Score(3) is averaged by its EWC(3), which is 1.000.</p>
<blockquote>
<p>Text Censoring</p>
</blockquote>
<p>Since we have innocent young students also doing this assignment, we did not want to expose them to some of the language that exists on Twitter. So we provide a mechanism to sanitize the text. We give you an <a href="http://en.wikipedia.org/wiki/ROT13" target="_blank" rel="external">ROT13ed</a> version of all banned words that should not occur in your output text. ROT13 is a simple letter-based substitution cipher. For instance, the first line in the list of banned words is <code>15619ppgrfg</code>, which means that the first banned “word” is 15619cctest.</p>
<p>The censored list is filtered from last year’s students’ colorful posts on Piazza near the end of the 15619Project (all these students started working late on their project). Download it <a href="https://cmucc-datasets.s3.amazonaws.com/15619/f15/banned.txt" target="_blank" rel="external">here</a>.</p>
<p>Ensure that in both cases, you use the provided files (links are above) and do not download the original, as we have done some cleanup to make them easy to use for your code.</p>
<p>You must do the steps in order, that is, first calculate the sentiment for a word and then (if required) censor it. A word is censored by replacing the inner characters by asterisks (<code>*</code>).</p>
<p>For example, assume that the word cloud is on our banned list. Consider:</p>
<p><code>I love Cloud compz... cloud TAs are the best... Yinz shld tell yr frnz: TAKE CLOUD COMPUTING NEXT SEMESTER!!! Awesome. It&#39;s cloudy tonight.</code></p>
<p>When you reply, it should have the format:</p>
<p><code>I love C***d compz... c***d TAs are the best... Yinz shld tell yr frnz: TAKE C***D COMPUTING NEXT SEMESTER!!! Awesome. It&#39;s cloudy tonight.</code></p>
<p>And it should have a score of +10 (Best = +3, Love = +3 and Awesome = +4)</p>
<p>Notice the case of all the uncensored letters is maintained. Also notice that <code>cloudy</code> is uncensored (since it is not on the list of banned words).</p>
<p>You can choose to do all these calculations in the ETL (remember that it will drive up the cost and duration of the MapReduce job) or at the end when the query is requested (if you can write fast code).</p>
<blockquote>
<p>Reference the ETL result of a small data set</p>
</blockquote>
<p>There are many corner cases that you need to consider during your ETL process, and confusions may arise for some of the steps easily. Hence, we have provided a highly unoptimized reference ETL result of a small data set. The reference file can be downloaded <a href="https://cmucc-datasets.s3.amazonaws.com/twitter/ref/part-00000-reference" target="_blank" rel="external">here</a>, it is the ETL result for the first part of the data set (<code>s3://cmucc-datasets/twitter/s16/part-00000</code>). Each line in this file corresponds to a line in the input file, in the order it appears in the input file.</p>
<p>Each line in the reference file has several columns, they are separated by tab characters (\t):</p>
<ul>
<li>the first column is the tweet id.</li>
<li>the second column is the user id.</li>
<li>the third column is the tweet date.</li>
<li>the fourth column is the sentiment density.</li>
<li>the fifth column is the censored tweet text. Some characters, like newline (\n), tab (\t) etc. are escaped.</li>
<li>the sixth to the last column are the hashtags used by the tweet. These columns may be absent if the tweet uses no hash tags.</li>
</ul>
<p>YOU SHOULD CAREFULLY CONSIDER ANY CORNER CASE YOURSELF, WE ONLY ENSURE THE CORRECTNESS OF THE DATA IN THE REFERENCE FILE.</p>
<p>Please note that the reference file does not give any suggestion about the ETL step or database design. In fact it’s far from the optimized design.</p>
<p>Request format:</p>
<p><code>GET /q2?userid=uid&amp;hashtag=hashtag</code></p>
<p>Response format: (if there exist matching tweets)</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">TEAMID</span>,<span class="tag">TEAM_AWS_ACCOUNT_ID</span>\<span class="tag">n</span></span><br><span class="line"><span class="tag">Sentiment_density1</span><span class="pseudo">:Tweet_time1</span><span class="pseudo">:Tweet_id1</span><span class="pseudo">:Cencored_text1</span>\<span class="tag">n</span></span><br><span class="line"><span class="tag">Sentiment_density2</span><span class="pseudo">:Tweet_time2</span><span class="pseudo">:Tweet_id2</span><span class="pseudo">:Cencored_text2</span>\<span class="tag">n</span></span><br><span class="line"><span class="tag">Sentiment_density3</span><span class="pseudo">:Tweet_time3</span><span class="pseudo">:Tweet_id3</span><span class="pseudo">:Cencored_text3</span>\<span class="tag">n</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>Response format: (if there is no matching tweet)</p>
<figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TEAMID,TEAM_AWS_ACCOUNT_ID<span class="string">\n</span></span><br><span class="line"><span class="string">\n</span></span><br></pre></td></tr></table></figure>
<p>Details are as follows:</p>
<ul>
<li>Tweet_time Returned date should be in the following format:yyyy-MM-dd HH-mm-ss (UTC time, 24-hour clock)</li>
<li>\n should be replaced with the new line character ‘\n’ instead of keeping it a ‘\’ and ‘n’.</li>
<li>Sorting Keys: Keys for sorting are as follows:<ul>
<li>Primary key - Sentimental_density: Each tweet should be sorted by sentiment density in a DESCENDING order. In case of a tie, the secondary key must be used.</li>
<li>Secondary key - Tweet_time: If the sentiment density is the same, the lines should be sorted by date in an ASCENDING order. The earliest time will come first. In case of a tie, the tertiary key must be used.</li>
<li>Tertiary key - Tweet_id: If sentiment density and tweet time are the same, the lines should be sorted by tweet_id in an ASCENDING order. The smallest tweet id will come first. There is no tie for this key.</li>
</ul>
</li>
</ul>
<p>Sample Request</p>
<p><code>GET /q2?userid=2324314004&amp;hashtag=LinkedIn</code></p>
<p>Sample Response</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TeamSecret,<span class="number">1123</span>-<span class="number">5813</span>-<span class="number">2134</span></span><br><span class="line"><span class="number">0.308</span>:<span class="number">2014</span>-<span class="number">04</span>-<span class="number">15</span> <span class="number">11</span>-<span class="number">42</span>-<span class="number">18</span>:<span class="number">456034778891169793</span>:RT @AlexanderCrepin: How To Find The Best <span class="preprocessor">#LinkedIn Groups To Join - - #personalbranding #jobhunt - - http:<span class="comment">//t.co/ixH5dOf88E</span></span></span><br><span class="line"><span class="number">0.267</span>:<span class="number">2014</span>-<span class="number">06</span>-<span class="number">01</span> <span class="number">19</span>-<span class="number">34</span>-<span class="number">25</span>:<span class="number">473185820636356608</span>:RT @tonyrestell: How To Build Relationships And Win Interviews Through <span class="preprocessor">#LinkedIn http:<span class="comment">//t.co/ELZgPnp4gY #jobhunt tips from @mocksource</span></span></span><br></pre></td></tr></table></figure>
</div><div class="tags"><a href="/tags/CMU/">CMU</a><a href="/tags/云计算/">云计算</a><a href="/tags/数据/">数据</a><a href="/tags/服务/">服务</a></div><div class="post-nav"><a href="/2016/02/24/dsa-9/" class="next">数据结构与算法 第 9 课 递归<i class="icon-next"></i></a></div><div data-thread-key="2016/02/25/cc-p1/" data-title="云计算 课程项目 1 搭建高性能数据获取网络服务" data-url="http://wdxtub.com/2016/02/25/cc-p1/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/02/25/cc-p1/" data-title="云计算 课程项目 1 搭建高性能数据获取网络服务" data-url="http://wdxtub.com/2016/02/25/cc-p1/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search" class="search-form-input"/><input type="hidden" name="sitesearch" value="http://wdxtub.com"/></form></div><div class="widget"><div class="widget-title">分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Game/">Game</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Gossip/">Gossip</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Memory/">Memory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Movie/">Movie</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading/">Reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Story/">Story</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Technique/">Technique</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Traveling/">Traveling</a></li></ul></div><div class="widget"><div class="comments-title">最近评论</div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title">友情链接</div><ul></ul><a href="http://wdxtub.com/library/" title="我的笔记" target="_blank">我的笔记</a><ul></ul><a href="http://wdxtub.com/bookclips/" title="我的书摘" target="_blank">我的书摘</a><ul></ul><a href="http://wdxtub.com/interview/" title="刷题笔记" target="_blank">刷题笔记</a></div></div></div></div><div id="footer">© <a href="/." rel="nofollow">小土刀.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><a id="rocket" href="#top" class="show"></a><script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/totop.js" type="text/javascript"></script><script src="/js/fancybox.pack.js" type="text/javascript"></script>
<script src="/js/jquery.fancybox.js" type="text/javascript"></script><link rel="stylesheet" href="/css/jquery.fancybox.css" type="text/css"><script>var duoshuoQuery = {short_name:'wdxblog'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></body></html>