<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="一个逗比的碎碎念"><title>云计算 第 0 课 阅读材料 | 小土刀</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">云计算 第 0 课 阅读材料</h1><a id="logo" href="/.">小土刀</a><p class="description">Agony is my triumph</p></div><div id="nav-menu"><a href="/." class="current"><i class="icon-home"> 首页</i></a><a href="/about/"><i class="icon-about"> 精选</i></a><a href="/guestbook/"><i class="icon-guestbook"> 留言</i></a><a href="/archives/"><i class="icon-archive"> 归档</i></a><a href="/atom.xml"><i class="icon-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post post-page"><h1 class="post-title">云计算 第 0 课 阅读材料</h1><div class="post-meta">2016-01-16 | <span class="categories">分类于<a href="/categories/Technique/"> Technique</a></span></div><span data-thread-key="2016/01/16/cc-0/" class="ds-thread-count"></span><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Cloud_Computing_Overview"><span class="toc-number">1.</span> <span class="toc-text">Cloud Computing Overview</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Definition_of_Cloud_Computing"><span class="toc-number">1.1.</span> <span class="toc-text">Definition of Cloud Computing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pre-Cloud_Computing_Domains_and_Applications"><span class="toc-number">1.2.</span> <span class="toc-text">Pre-Cloud Computing Domains and Applications</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evolution_of_Cloud_Computing"><span class="toc-number">1.3.</span> <span class="toc-text">Evolution of Cloud Computing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Building_Blocks_and_Service_Models"><span class="toc-number">1.4.</span> <span class="toc-text">Cloud Building Blocks and Service Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Deployment_Models"><span class="toc-number">1.5.</span> <span class="toc-text">Cloud Deployment Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Popular_Cloud_Stacks"><span class="toc-number">1.6.</span> <span class="toc-text">Popular Cloud Stacks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Use_Cases"><span class="toc-number">1.7.</span> <span class="toc-text">Cloud Use Cases</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary"><span class="toc-number">1.8.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Economics_2C_Benefits_2C_Risks_2C_Challenges_and_Solutions"><span class="toc-number">2.</span> <span class="toc-text">Economics, Benefits, Risks, Challenges and Solutions</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Benefits_2C_Risks_2C_and_Challenges_of_Cloud_Computing"><span class="toc-number">2.1.</span> <span class="toc-text">Benefits, Risks, and Challenges of Cloud Computing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Business_Case_for_Cloud_Computing__28for_Users_29"><span class="toc-number">2.2.</span> <span class="toc-text">Business Case for Cloud Computing (for Users)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Economics_of_Cloud_Computing__28for_Providers_29"><span class="toc-number">2.3.</span> <span class="toc-text">Economics of Cloud Computing (for Providers)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Service_Level_Agreements_and_Objectives"><span class="toc-number">2.4.</span> <span class="toc-text">Service Level Agreements and Objectives</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Security_-_Threats"><span class="toc-number">2.5.</span> <span class="toc-text">Cloud Security - Threats</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Security_-_Control__26amp_3B_Auditing"><span class="toc-number">2.6.</span> <span class="toc-text">Cloud Security - Control & Auditing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary-1"><span class="toc-number">2.7.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Data_Center_Trends"><span class="toc-number">3.</span> <span class="toc-text">Data Center Trends</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction_to_Data_Centers"><span class="toc-number">3.1.</span> <span class="toc-text">Introduction to Data Centers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Definition_and_Origins"><span class="toc-number">3.2.</span> <span class="toc-text">Definition and Origins</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Size_2C_Density_and_Efficiency_Growth"><span class="toc-number">3.3.</span> <span class="toc-text">Size, Density and Efficiency Growth</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Challenges_in_Cloud_Data_Centers"><span class="toc-number">3.4.</span> <span class="toc-text">Challenges in Cloud Data Centers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary-2"><span class="toc-number">3.5.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Data_Center_Components"><span class="toc-number">4.</span> <span class="toc-text">Data Center Components</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#IT_Equipment"><span class="toc-number">4.1.</span> <span class="toc-text">IT Equipment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Infrastructure_and_Facilities"><span class="toc-number">4.2.</span> <span class="toc-text">Infrastructure and Facilities</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Geographic_Location_Criteria"><span class="toc-number">4.3.</span> <span class="toc-text">Geographic Location Criteria</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Costs"><span class="toc-number">4.4.</span> <span class="toc-text">Costs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Power_and_Efficiency"><span class="toc-number">4.5.</span> <span class="toc-text">Power and Efficiency</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Redundancy"><span class="toc-number">4.6.</span> <span class="toc-text">Redundancy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reliability_Metrics"><span class="toc-number">4.7.</span> <span class="toc-text">Reliability Metrics</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Summary-3"><span class="toc-number">4.8.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Cloud_Management"><span class="toc-number">5.</span> <span class="toc-text">Cloud Management</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Middleware"><span class="toc-number">5.1.</span> <span class="toc-text">Cloud Middleware</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Resource_Provisioning"><span class="toc-number">5.2.</span> <span class="toc-text">Resource Provisioning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Orchestration_and_Automation"><span class="toc-number">5.3.</span> <span class="toc-text">Cloud Orchestration and Automation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Case_Study__3A_OpenStack"><span class="toc-number">5.4.</span> <span class="toc-text">Case Study : OpenStack</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Software_Stack_Summary"><span class="toc-number">5.5.</span> <span class="toc-text">Cloud Software Stack Summary</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Cloud_Software_Deployment_Considerations"><span class="toc-number">6.</span> <span class="toc-text">Cloud Software Deployment Considerations</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Programming_the_Cloud"><span class="toc-number">6.1.</span> <span class="toc-text">Programming the Cloud</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deploying_Applications_on_the_Cloud"><span class="toc-number">6.2.</span> <span class="toc-text">Deploying Applications on the Cloud</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Build_Fault-tolerant_Cloud_Services"><span class="toc-number">6.3.</span> <span class="toc-text">Build Fault-tolerant Cloud Services</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Load_Balancing"><span class="toc-number">6.4.</span> <span class="toc-text">Load Balancing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scaling_Resources"><span class="toc-number">6.5.</span> <span class="toc-text">Scaling Resources</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dealing_with_Tail_Latency"><span class="toc-number">6.6.</span> <span class="toc-text">Dealing with Tail Latency</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Economics_for_Cloud_Applications"><span class="toc-number">6.7.</span> <span class="toc-text">Economics for Cloud Applications</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Programming_the_Cloud_Summary"><span class="toc-number">6.8.</span> <span class="toc-text">Programming the Cloud Summary</span></a></li></ol></li></ol></div></div><div class="post-content"><p>这一部分内容主要是云计算的理论知识，由于时间缘故使用英文原文，关键要点会作为单独的『课』来进行介绍。</p>
<a id="more"></a>
<hr>
<h1 id="Cloud_Computing_Overview"><a href="#Cloud_Computing_Overview" class="headerlink" title="Cloud Computing Overview"></a>Cloud Computing Overview</h1><p>Welcome to this online course on cloud computing. This domain is emerging and fast-evolving. Here, we will introduce the big picture of cloud computing, as well as explain how it has evolved to its current state. Learning these concepts will give you a better understanding about some of the motivating factors behind cloud computing services and why it is one of the fastest growing technology segments in the industry today.</p>
<p>In the first unit of the course, we will:</p>
<ul>
<li>introduce some of the fundamental ideas behind cloud computing,</li>
<li>explore the evolution of computing and the emergence of the cloud,</li>
<li>compare and contrast the different service models and types of clouds,</li>
<li>discuss some cloud use cases and popular cloud stacks</li>
</ul>
<p>Cloud computing can be thought of as an evolved response to the computing needs of today. Our world is increasingly connected and data driven. Users, whether at home or work, generate and consume large amounts of data from various sources. A massive challenge has arisen in terms of managing and exploiting this data. Starting with this module, and throughout the course, you will see how cloud computing plays a central role in meeting this challenge.</p>
<h2 id="Definition_of_Cloud_Computing"><a href="#Definition_of_Cloud_Computing" class="headerlink" title="Definition of Cloud Computing"></a>Definition of Cloud Computing</h2><p><strong>Cloud Computing</strong></p>
<p>Cloud computing offers the use of computing resources as a service over the network. A cloud computer is simply a large distributed computing infrastructure that users have access to over a network. Similar to some other domains, cloud computing came about through the maturity of enabling technologies while attempting to satisfy economic needs. In this course, we will provide an introduction to cloud computing and then cover relevant topics, in varying detail, including hardware and software infrastructure, resource management (virtualization), cloud storage, and programming models.</p>
<p>In the first unit of this course we will start with a simple overview of cloud computing, its definition, motivations, evolutions, building blocks, service models and use cases. We will also discuss economics, risks, benefits and security.</p>
<p><strong>What is Cloud Computing?</strong></p>
<blockquote>
<p>Cloud Computing(definition)<br>The delivery of computing as a service over a network, whereby distributed resources are provided to the end user as a utility.</p>
</blockquote>
<p>Information technology (IT) has become an essential requirement for most organizations to function effectively. Typically, and depending on a specific organization’s needs, IT has three components associated with it (Figure 1.1) - application software, development platforms and the underlying infrastructure:</p>
<p><img src="/images/14529625854236.jpg" alt=""><br>Figure 1.1: Typical Components of Information Technology.</p>
<p>Traditionally, an organization that needs to deploy a particular IT solution has to procure, setup and maintain the infrastructure and the application; certain organizations may decide to develop their own software, in which case they need to manage development platforms as well. The organization hence “owns” the solution, which allows full control over the solution, including, for example, access security and customization, however, it has some drawbacks:</p>
<ol>
<li>Organizations must pay upfront to buy a particular solution, which commits significant capital for long-lived IT resources.</li>
<li>Organizations are solely responsible for the management of their IT solutions. Organizations must have hardware maintenance contracts for the acquired IT solutions. System administrators will have to be hired to monitor hardware and software which has to be updated and maintained. Organizations also have to pay for power and cooling to keep the hardware running. Therefore, in addition to upfront costs, organizations have to budget for recurring costs.</li>
<li>The IT solution typically has a fixed size and will have to be modified to scale when the needs grow or shrink. (For example, as the number of employees grows, the organization will have to purchase additional hardware and/or software to keep up with increasing demands).</li>
<li>Typically IT systems suffer from low average utilization. Utilization refers to the proportion of time (expressed usually as a percentage) that an IT system is being used to capacity. For example, email services in a large organization typically see the most amount of traffic at 8 am, when people sign in and check email. Utilization tapers off towards close of business and is practically nil after hours. Further, since IT systems consume energy, even at idle, they leave a prominent carbon footprint.</li>
</ol>
<p>Many of the disadvantages listed above emanate from the ownership of IT. However, with the evolution of computing technology, it is no longer necessary for organizations to own IT systems. Many of the IT needs of organizations can be provided to them as services. Cloud computing is the transformation of owned IT products into services that can be availed from a cloud service provider.</p>
<p>The transformation of a certain technology from a product to a service is not new. A similar transformation evolved for electricity, which initially had to be produced near the device or service requiring it. The development of large power plants, electric transmission systems and grids has led to the rise of electric power as a utility, (a service that people can obtain and pay for as needed).</p>
<p>The following video (Video 1.1) discusses the transformation of IT from a product to a service:</p>
<p><a href="http://youtu.be/HaVqHgg7zv4" target="_blank" rel="external">Video 1.1: Introduction to Cloud Computing</a></p>
<p>In cloud computing, users or organizations use computing resources as a service and pay for them as a utility, in a pay-as-you-go model. When a request is made for computing resources, the cloud provider typically provisions these resources, in a rapid manner (minutes or hours). As the need for these resources changes, users or organizations can rapidly scale up or down their resources on demand.</p>
<p>The cloud model offers users and organizations several benefits, including: reduced upfront cost, as IT services can be obtained in a pay-as-you-go model; the convenience of fast resource provisioning, which significantly reduces the time to market for IT solutions; and rapid scalability of computing resources, as they can be scaled up and down on demand. Cloud providers’ resources are shared by multiple users, thereby improving utilization and reducing carbon footprint.</p>
<p>In spite of all of its advantages, cloud computing is still an emerging and maturing technology and comes with many risks and challenges which will be later covered in this unit.</p>
<h2 id="Pre-Cloud_Computing_Domains_and_Applications"><a href="#Pre-Cloud_Computing_Domains_and_Applications" class="headerlink" title="Pre-Cloud Computing Domains and Applications"></a>Pre-Cloud Computing Domains and Applications</h2><p><strong>Domains and Application Examples</strong></p>
<p>Now that we have defined what cloud computing is, let us look at examples of how computing was utilized in different domains such as business computing, scientific computing and personal computing before the emergence of cloud computing.</p>
<p>Business computing: Examples of traditional management information systems include logistics and operations, enterprise resource planning (ERP), customer relation management (CRM), office productivity and business intelligence (BI). Such tools enabled more streamlined processes that led to improved productivity and reduced cost across a variety of enterprises.</p>
<p>As an example, CRM software allows companies to collect, store, manage and interpret a variety of data about past, current and potential future customers. CRM software offers an integrated view (in real-time or near real-time) of all organizational interactions with customers. For example, for a manufacturing company, CRM software could be used by a sales team to schedule meetings, tasks and follow-ups with clients. A marketing team could target clients with campaigns based on specific patterns. Billing teams can track quotes and invoices. As such, it is a centralized repository for storing this information. To enable this functionality, a variety of hardware and software technologies are utilized by the organization and sales teams in order to collect the data which needs to be stored and analyzed using various database and analytics systems.</p>
<p>Scientific computing: Scientific computing uses mathematical models and analysis techniques implemented on computers to attempt to solve scientific problems. A popular example is computer simulation of physical phenomena. This field has disrupted the traditional theoretical and laboratory experimental methods by enabling scientists and engineers to reconstruct known events or to predict future situations through developing programs to simulate and study different systems under different circumstances. Such simulations typically require a very large number of calculations which are often run on expensive supercomputers or distributed computing platforms.</p>
<p>Personal computing: In personal computing, a user runs various applications on a general-purpose computer. Examples of such applications include office productivity software such as word processing and spreadsheets, or communication such as email clients or entertainment such as video games or playing multimedia files. A personal computing user typically owns, installs and maintains the software and hardware utilized to carry out such tasks.</p>
<p><strong>Addressing Scale</strong></p>
<p>hich can provide the necessary upgrades to the service level. In many cases, vertical scaling consists of upgrading or replacing servers and storage systems with newer, faster servers or storage arrays with increased capacity. This process could take months to plan and execute, along with a window where the service might experience some downtime.</p>
<p>In certain types of systems, scaling is also done horizontally, by increasing the amount of resources dedicated to the system. An example of this is in high-performance computing, where additional servers and storage can be added to improve the performance of the system, thereby leading to a higher number of calculations that can be performed per second, or increasing the storage capacity of the system. Just like vertical scaling, this process can take months to plan and execute, with downtimes also a possibility.</p>
<p>Since companies owned and maintained their IT equipment, as the cost of addressing scale continued to rise, companies identified other methods to reduce cost. Large companies consolidated the computing needs of different departments into a single large data center whereby they consolidated real estate, power, cooling, and networking in order to reduce cost. On the other hand, small and medium size companies could lease real-estate, network, power, cooling and physical security by placing their IT equipment in a shared data center. This is typically referred to as a co-location service which was adopted by small to medium sized companies who did not want to build their own data centers in-house. Co-location services continues to be adopted in various domains as a cost-effective approach to reduce operational expenses.</p>
<p>Scale has impacted all aspects of business computing. For example, scale has impacted CRM systems through the increase of clients or through the amount of information that we store and analyse about clients. Business computing has addressed scale through vertical and horizontal scaling as well as consolidation of IT resources to data centers and co-location. In scientific computing, parallel and distributed systems have been adopted in order to scale up the size of the problems and the precision of their numerical simulations. One definition of parallel processing is the use of multiple homogenous computers that share state and function as a single large computer in order to run large scale or high precision calculations. Distributed computing is the use of multiple autonomous computing systems connected by a network in order to partition a large problem into subtasks that are run concurrently and communicate via messages over the network. The scientific community continued to innovate in these domains in order to address scale. In personal computing, scale has impacted it through increased user demands brought on by richer content and diverse applications. Users therefore scale up their owned personal computing device to keep up with these demands.</p>
<p><strong>Rise of Internet Services</strong></p>
<p>The late 90s marked a steady increase in the adoption of these computing applications and platforms across domains. Soon, software was expected to not only be functional, but also capable of producing value and insight for business and personal requirements. The use of these applications became collaborative; applications were mixed and matched to feed information to each other. IT was no longer just a cost center for a company, but a source of innovation and efficiency.</p>
<p><img src="/images/14529628961629.jpg" alt=""><br>Figure 1.2: Comparing Traditional and Internet-Scale Computing.</p>
<p>The 21st century has been marked by an explosion in the volume and capacity of wireless communications, the World Wide Web, and the Internet. These changes have led to a network- and data-driven society, where producing, disseminating and accessing digitized information is simplified. The Internet is estimated to have created a global marketplace of billions of users, up from 25 million in 1994 (Figure 1.3 (a)) [1] . This rise in data and connections is valuable to businesses. Data creates value in several ways, including by enabling experimentation, segmenting populations and supporting decision-making with automation. [2] By embracing digital technologies, the world’s top 10 economies are expected to increase their output by over a trillion dollars by 2020. [3]</p>
<p>The increasing number of connections enabled by the Internet has also driven its value. Researchers have hypothesized that the value of a network varies superlinearly as a function of the number of users. Thus, at internet scale, gaining and retaining customers is a priority, and this is done by building reliable and responsive services, and making changes based on observed data patterns.</p>
<p><img src="/images/14529629235564.jpg" alt=""><br>Figure 1.3 (a): Increasing number of Internet Users per year. [1]</p>
<p><img src="/images/14529629323895.jpg" alt=""><br>Figure 1.3 (b): Increasing number of data stored per year. [5]</p>
<p>Some examples of Internet-scale systems include:</p>
<ol>
<li>Search engines that crawl, store, index, and search (upto petabyte-sized) large data sets. For instance, Google started as a giant web index that crawled and analyzed web traffic, once every few days and matched these indices to keywords. Now, it updates its indices in near-real-time and is one of the most popular ways to access information on the Internet. Their index has trillions of pages with a size of thousands of terabytes. [4]</li>
<li>Social networks like Facebook and LinkedIn that allow users to create personal and professional relationships and build communities based on similar interests. Facebook, for instance, now supports over a billion active users per month.</li>
<li>Online retail services like Amazon maintain a global inventory of millions of products, which are sold to over 200 million customers, with net sale volumes of almost $90 billion annually.</li>
<li>Rich, streaming multimedia applications allow people to watch and share videos and other forms of rich content. One such example, YouTube, handles uploads of 300 minutes of video per second.</li>
<li>Real-time communications systems for audio, video and text chatting like Skype which clock more than 50 billion minutes of calls per month.</li>
<li>Productivity and collaboration suites that serve millions of documents to many concurrent users allowing real-time, persistent updates. For e.g. Office 365 claims to support 50 million monthly active collaborators.</li>
<li>CRM applications by providers like SalesForce are deployed at over a hundred thousand organizations. Large CRMs now provide intuitive dashboards to track status, analytics to find the customers that generate the most business and revenue forecasting to predict future growth.</li>
<li>Data mining and business intelligence applications that analyze the usage of other services (like those above) to find inefficiencies and opportunities for monetization.</li>
</ol>
<p>Clearly, these systems are expected to deal with a high volume of concurrent users. This requires an infrastructure with the capacity to handle large amounts of network traffic, generate and securely store data, all without any noticeable delays. These services derive their value by providing a constant and reliable standard of quality. They also provide rich user interfaces for mobile devices and web browsers, making them easy to use, but harder to build and maintain.</p>
<p>We summarize some of the requirements of Internet-scale systems here:</p>
<ol>
<li>Ubiquity—being accessible from anywhere at any time, from a multitude of devices. For instance, a salesperson will expect their CRM service to provide timely updates on a mobile device to make visits to clients shorter, faster and more effective. The service should function smoothly under a variety of network connections.</li>
<li>High-availability—the service must be “always up”. Uptimes are measured in terms of number of nines. Three nines, or 99.9%, implies that a service will be unavailable for 9 hours a year. Five nines (about 6 minutes a year) is a typical threshold for a high-availability service. Even a few minutes of downtime in online retail applications can impact millions of dollars of sales.</li>
<li>Low latency—fast and responsive access times. Even slightly slower page load times have been shown to significantly reduce the usage of that web page. For instance, increasing search latency from 100 ms to 400 ms decreases the number of searches per user from 0.8% to 0.6%, and the change persisted even after the latency was reduced to original levels.</li>
<li>Scalability—the ability to deal with seasonality and virality, which causes peaks and troughs in the traffic over long and short periods of time. On days like “Black Friday” and “Cyber Monday”, retailers like Amazon must handle several times the network traffic than on average.</li>
<li>Cost effectiveness—an Internet-scale service requires much more infrastructure than a traditional application as well as better management. One way to streamline costs is by making services easier to manage, and reducing the number of administrators handling a service. Smaller services can afford to have a low service-to-admin ratio (e.g. 2:1, meaning a single administrator must maintain two services); however, to maintain profitability, services like Microsoft Bing must have high service-to-admin ratio (e.g. 2500:1, meaning a single administrator maintains 2500 services) [6] .</li>
<li>Interoperability—many of these services are often used together and hence must provide an easy interface for reuse and support standardized mechanisms for importing and exporting data. For example, many other services (like Uber) may integrate Google Maps within their products to provide simplified location and navigation information to users.</li>
</ol>
<p>We will now explore some of the early solutions to the various problems above [7] . The first challenge to be tackled was the large round-trip time for early web services that were mostly located in the United States. The earliest mechanisms to deal with the problems of low latency (due to distant servers) and server failure simply relied on redundancy. One technique for achieving this was by “mirroring” content, whereby copies of popular web pages would be stored at different locations around the world. This minimized the amount of load on the central server, reduced the latency perceived by end-users, and allowed traffic to be switched over to another server in case of failures. The downside of this was an increase in complexity to deal with inconsistencies if even one copy of the data were to be modified. Thus, this technique is more useful for static, read-heavy workloads, such as serving images, videos or music. Due to the effectiveness of this technique, most Internet-scale services use content delivery networks (CDNs) to store distributed global caches of popular content. For example, Cable News Network (CNN) now maintains replicas of their videos on multiple “edge” servers at different locations worldwide, with personalized advertising per location.</p>
<p>Of course, it did not always make sense for individual companies to buy dozens of servers across the world. Cost efficiencies were often gained by using shared hosting services. Here, shares of a single web server would be leased out to multiple tenants, amortizing the cost of server maintenance. Shared hosting services could be highly resource-efficient, as the resources could be over-provisioned under the assumption that not all services would be operating at peak capacity at the same time (an over-provisioned physical server is one where the aggregate capacity of all the tenants is greater than the actual capacity of the server). The downside was that it was nearly impossible to isolate the tenants’ services from those of their neighbors. Thus, a single overloaded or error-prone service could adversely impact all its neighbors. Another problem arose because tenants could often be malicious and try to leverage their co-location advantage to steal data or deny service to other users.</p>
<p>To counter this, Virtual Private Servers were developed as variants of the shared hosting model. A tenant would be provided with a virtual machine (VM) on a shared physical server (we talk more about virtual machines and their properties later). These VMs were often statically allocated and linked to a single physical machine, thus they were difficult to scale and often needed manual recovery from any failures. Though they could no longer be overprovisioned, they had better performance and security isolation between co-located services than simple resource sharing.</p>
<p>Another problem of sharing public resources was that it required storing private data on third-party infrastructure. Some of the Internet-scale services we described above could not afford to lose control over data storage, since any disclosure of their customers private data would have disastrous consequences. Hence, these companies needed to build their own global infrastructure. Before the advent of the public cloud, such services could only be deployed by large corporations like Google and Amazon. Each of these companies would build large, homogeneous data centers across the globe using commodity off-the-shelf components, where a data center could be thought of as a single, massive warehouse-scale computer (WSC). A WSC provided an easy abstraction to globally distribute applications and data, while still maintaining ownership.</p>
<p>Due to the economies of scale, the utilization of a data center could be optimized to reduce costs. This was still not as efficient as publicly sharing resources, these warehouse-scale computers had many desirable properties that served as foundations for building Internet-scale services. The scale of computing applications progressed from serving a fixed user base to serving a dynamic global population. Standardized WSCs allowed large companies to serve such large audiences. An ideal infrastructure would combine the performance and reliability of a WSC, with the sharing hosting model. This would enable even a small corporation to develop and launch a globally competitive application, without the high overhead of building large datacenters.</p>
<p>Another approach to share resources was Grid Computing, which enabled the sharing of autonomous computing systems across institutions and geographical locations. Several academic and scientific institutions would collaborate and pool together their resources towards a common goal. Each institution would then join a “virtual organization” by dedicating a specific set resources via well-defined sharing rules. Resources would often be heterogeneous and loosely coupled requiring complex programming constructs to stitch together. Grids were geared towards supporting non-commercial research and academic projects and relied on existing open source technologies.</p>
<p>The cloud was a logical successor that combined many of the features of the solutions above. For example, instead of universities contributing and sharing access to a pool of resources using a Grid, the cloud allows them to lease computing infrastructure that was centrally administered by a Cloud Service Provider (CSP). As the central provider maintained a large resource pool to satisfy all clients, the cloud made it easier to dynamically scale up and down demand within a short period of time. Instead of open standards like the grid, however, cloud computing relies on proprietary protocols and needs the user to place a certain level of trust in the CSP.</p>
<p>The rest of this unit covers how the cloud evolved to make computing a public utility that could be metered and used.</p>
<p><strong>References</strong></p>
<ol>
<li>Real Time Statistics Project (2015). Internet Live Stats. www.internetlivestats.com/.</li>
<li>IBM (2015). What is Big Data?. www-01.ibm.com/software/data/bigdata/what-is-big-data.html.</li>
<li>Accenture (2015). Increased Use of Digital Technologies Could Add $1.36 Trillion to World’s Top 10 Economies in 2020. newsroom.accenture.com/subjects/strategy/increased-use-of-digital-technologies-could-add-over-1-trillion-dollars-to-worlds-top-10-economies-in-2020-according-to-new-study-from-accenture.htm.</li>
<li>Google Inc. (2015). How Search Works. <a href="https://www.google.com/insidesearch/howsearchworks/thestory/" target="_blank" rel="external">https://www.google.com/insidesearch/howsearchworks/thestory/</a>.</li>
<li>Hilbert, Martin and Lopez, Priscila (2011). The world’s technological capacity to store, communicate, and compute information.</li>
<li>Hamilton, James R and others (2007). On Designing and Deploying Internet-Scale Services.</li>
<li>Brewer, Eric and others (2001). Lessons from giant-scale services.</li>
</ol>
<h2 id="Evolution_of_Cloud_Computing"><a href="#Evolution_of_Cloud_Computing" class="headerlink" title="Evolution of Cloud Computing"></a>Evolution of Cloud Computing</h2><p><strong>Events and Innovations</strong></p>
<p>The cloud-computing concept first appeared during the early 1950s, when several academics, including Herb Grosch, John McCarthy, and Douglas Parkhill, [1] [2] envisioned computing as a utility similar to electric power. Over the next few decades, several emerging technologies laid the foundations for cloud computing (Figure 1.4). More recently, rapid growth of the World Wide Web and the advent of large Internet giants, such as Google and Amazon, finally led to the creation of an economic and business environment that allowed the cloud-computing model to flourish.</p>
<p><img src="/images/14529690211629.jpg" alt=""><br>Figure 1.4: Evolution of cloud computing.</p>
<p><strong>Evolution of Computing</strong></p>
<p>Since the 1960s, some of the earliest forms of computers that were used by organizations were mainframe computers. Multiple users could share and connect to mainframes over basic serial connections using terminals. The mainframe was responsible for all the logic, storage, and processing of data, and the terminals connected to them had limited computational power, if any. These systems continued in widespread use for more than 30 years and, to some degree, continue to exist today.</p>
<p>With the birth of personal computing, cheaper, smaller, more powerful processors and memory led to a swing in the opposite direction, in which users ran their own software and stored data locally. This situation, in turn, led to issues of ineffective data sharing and rules to maintain order within an organization’s IT environment.</p>
<p>Gradually, through the development of high-speed network technologies, local area networks (LANs) were born that allowed computers to connect and communicate with each other. Thus, vendors designed systems that could encapsulate the benefits of both personal computers and mainframes, resulting in client-server applications that became popular over LANs. Clients would typically run client software (and process some data) or a terminal (for legacy applications) that connected to a server. The server, in the client-server model, possessed the application, storage, and data logic.</p>
<p>Eventually, in 1990s, the global information age emerged, with the Internet rapidly being adopted. Network bandwidth improved by many orders of magnitude, from ordinary dial-up access to dedicated fiber connectivity today. In addition, cheaper and more powerful hardware emerged. Furthermore, the evolution of the World Wide Web and dynamic websites necessitated multitier architectures.</p>
<p>Multitier architectures enabled the modularization of software by separating the application presentation, application logic, and storage as individual entities. With this modularization and decoupling, it was not long before these individual software entities were running on distinct physical servers (typically due to differences in hardware and software requirements). This led to an increase of individual servers in organizations; however, it also led to poor average utilization of server hardware. In 2009, the International Data Corporation (IDC) estimates that the average x86 server has a utilization rate of approximately 5 to 10%. [3]</p>
<p>Virtual machine technology matured well enough in the 2000s to become available as commercial software. Virtualization enables an entire server to be encapsulated as an image, which can be run seamlessly on hardware and enable multiple virtual servers to run simultaneously and share hardware resources. Virtualization thus enables servers to be consolidated, which accordingly improves system utilization.</p>
<p>Simultaneously, grid computing gained traction in the scientific community in an effort to solve large-scale problems in a distributed fashion. With grid computing, computer resources from multiple administrative domains work in unison for a common goal. Grid computing brought forth many resource-management tools (e.g., schedulers and load balancers) to manage large-scale computing resources.</p>
<p>As the various computing technologies evolved, so did the economics of computing. Even during the early days of mainframe-based computing, companies such as IBM offered to host and run computers and software for various organizations, such as banks and airlines. In the Internet Age, third-party Web hosting also become popular. With virtualization, however, providers have unparalleled flexibility in accommodating multiple clients on a single server, sharing hardware and resources between them.</p>
<p>The development of these technologies, coupled with the economic model of utility computing, is what eventually evolved into cloud computing.</p>
<p><strong>Enabling Technologies</strong></p>
<p>Cloud computing has various enabling technologies (Figure 1.5), which include networking, virtualization and resource management, utility computing, programming models, parallel and distributed computing, and storage technologies.</p>
<p><img src="/images/14529690849852.jpg" alt=""><br>Figure 1.5: The enabling technologies in cloud computing.</p>
<p>The emergence of high-speed and ubiquitous networking technologies have greatly contributed to cloud computing as a viable paradigm. Modern networks make it possible for computers to communicate in a fast and reliable manner, which is important if we are to use services from a cloud provider. This enabled the user experience with software running in a remote data center to be comparable to the experience of software running on a personal computer. Webmail is a popular example, as is office productivity software. In addition, virtualization is key to enabling cloud computing. As mentioned above, virtualization allows managing the complexity of the cloud via abstracting and sharing its resources across users through multiple virtual machines. Each virtual machine can execute its own operating system and associated application programs. Virtualization for cloud computing is covered in Unit 3.</p>
<p>Technologies such as large-scale storage systems, distributed file systems, as well as novel database architectures are crucial for managing and storing data in the cloud. Cloud storage technologies are covered in Unit 4.</p>
<p>Utility computing offers numerous charging structures for the leasing of compute resources. Examples include pay-per-resource-hour, pay-per-guaranteed-throughput, and pay-per-data stored per month etc.</p>
<p>Parallel and distributed computing allow distributed entities located at networked computers to communicate and coordinate their actions in order to solve certain problems, represented as parallel programs. Writing parallel programs for distributed clusters is inherently difficult. To achieve high programming efficiency and flexibility in the cloud, a programming model is required.</p>
<p>Programming models for clouds give users the flexibility to express parallel programs as sequential computation units (e.g., functions as in MapReduce and vertices as in GraphLab). Such programming models’ runtime systems typically parallelize, distribute, and schedule computational units, manage inter-unit communication, and tolerate failures. Cloud programming models are covered in Unit 5.</p>
<p><strong>RReferences</strong></p>
<ol>
<li>Simson L. Garfinkel (1999). Architects of the Information Society: Thirty-Five Years of the Laboratory for Computer Science at MIT. MIT Press.</li>
<li>Douglas J. Parkhill (1966). The Challenge of the Computer Utility. Addison-Wesley Publishing Company, Reading, MA.</li>
<li>Michelle Bailey (2009). “The Economics of Virtualization: Moving Toward an Application-Based Cost Model.” VMware Sponsored IDC Whitepaper.</li>
</ol>
<h2 id="Cloud_Building_Blocks_and_Service_Models"><a href="#Cloud_Building_Blocks_and_Service_Models" class="headerlink" title="Cloud Building Blocks and Service Models"></a>Cloud Building Blocks and Service Models</h2><p><strong>Cloud Building Blocks</strong></p>
<p>Remember, cloud computing offers the use of computing resources as a service over the network. Before we discuss the service models offered on a cloud, we ought to think about the different layers of hardware and software the are required to build cloud services. Of course, not all service requirements are identical; some cloud users may only desire access to raw infrastructure to build applications on. Others may wish to not deal with the infrastructure at all, but rather, simply develop and deploy applications using an easy-to-use platform. To meet these varied requirements, cloud service providers divide their offerings into various abstract layers.</p>
<p>Here, we introduce a stacked abstraction of the cloud through presenting typical building blocks and discuss their association with three service models in cloud computing. We present four main building blocks in cloud computing: application software, development platforms, resource sharing, and infrastructure, as shown in Figure 1.6. The infrastructure includes the physical resources in a data center. The resource sharing layer typically entails software and hardware techniques that allow the sharing of the physical resources while offering a certain level of isolation. The development platforms are utilized to develop cloud applications.</p>
<p><img src="/images/14529692293425.jpg" alt=""><br>Figure 1.6: Cloud computing building blocks.</p>
<p><strong>Cloud Building Blocks</strong></p>
<p>Application software: The top layer in the stack is the application software, which normally is the system component that the end user utilizes.</p>
<p>Development platforms: The next layer, development platforms, allows application developers to write application software in terms of a cloud’s application programming interface (API). Development platforms typically provide specifications that developers can use for routines, data structures, object classes, libraries and variables.</p>
<p>Resource sharing: Resource sharing mechanisms, the third layer, embody some key cloud ideas:</p>
<ul>
<li>Provide software, computation, network and storage services.</li>
<li>Allow a shared environment whereby multiple hardware images (e.g., virtual machines) and system images (e.g., general-purpose OSs) can run side by side on a single infrastructure along with security, resource, and failure isolations. These isolation properties are provided by a combination of hardware and software techniques that are covered in detail in Unit 3.</li>
<li>Consolidate physical servers into virtual servers that run on fewer physical servers.</li>
<li>Deliver agility and elasticity to rapidly respond to users’ resource and service demands.</li>
</ul>
<p>These ideas usually are addressed through virtualization, a technology discussed in detail in Unit 3.</p>
<p>Infrastructure: Physical resources comprise the bottom layer and, in cloud computing, are primarily deployed on the cloud provider’s side. The broad resource classes, detailed in Unit 2, include the following:</p>
<ul>
<li>Compute resources, typically servers, which are computers designed for enterprise computing (as opposed to user workstations). They usually are rack mounted to utilize space efficiently.</li>
<li>Storage resources maintain the cloud’s data, and application storage use usually is charged in terms of capacity usage (e.g., per gigabyte or terabyte usage).</li>
<li>Network resources enable communication between servers as well as servers and clients.</li>
<li>Software that manages the compute, network and storage infrastructure.</li>
</ul>
<p>Next we will discuss which of these abstractions can be provided as a leased service over a network. For example the services and resources required by a software developer will be different compared to someone who would like to have access to a WebMail application running on the cloud.</p>
<p><img src="/images/cc19.jpg" alt="c"></p>
<p><strong>Cloud Computing Services</strong></p>
<p>In a broad sense, cloud services differ based on the needs of different users. This section reviews three popular types of cloud services:</p>
<ul>
<li>Software as a service (SaaS)</li>
<li>Platform as a service (PaaS)</li>
<li>Infrastructure as a service (IaaS)</li>
</ul>
<p>SaaS is any application in which the end user has access to a software application over the network and pays based on a variety of business models some of which are free. PaaS is the offering of software development platforms as a service which are utilized to develop SaaS applications. Finally, IaaS, is the leasing of virtualized infrastructure over the network. In this last model, the end user has the flexibility to install and use any software they please on the leased infrastructure.</p>
<p>The following video (Video 1.2) reviews these services:</p>
<p><a href="http://youtu.be/ltJmJEI0gGA" target="_blank" rel="external">Video 1.2: Service Models in Cloud Computing</a></p>
<p><strong>The Software-as-a-Service Model</strong></p>
<blockquote>
<p>Software as a Service(definition)<br>Software as a service (SaaS) is a software delivery model in which software and associated data are hosted on a cloud. SaaS applications typically are accessed by users using a thin client via a Web browser.</p>
</blockquote>
<p>SaaS is one of the most common cloud service models in which the cloud provider delivers software as an Internet service (as discussed in Video 1.3). SaaS users simply use their browsers to access the software, thus eliminating the need to install, run, and maintain (update, patch, reconfigure etc.) the application on the user’s computer. The Web browser loads the SaaS application service dynamically and transparently.</p>
<p><a href="http://youtu.be/bzfdewWofSU" target="_blank" rel="external">Video 1.3: Software as a Service</a></p>
<p>SaaS has become a common software delivery model for many business applications, including accounting, collaboration, customer relationship management (CRM), management information systems (MIS), enterprise resource planning (ERP), invoicing, human resource management (HRM), content management (CM) as well as service desk management.</p>
<p>With SaaS, the provider maintains the software and required infrastructure to run it. The provider routinely develops the software, and enhancements are automatically made available to all users the next time a user logs on to the service. In addition, any application data that results from the use of the service resides on the cloud and is available to the user from any location.</p>
<p><strong>Characteristics of SaaS</strong></p>
<p>A vast majority of SaaS solutions are based on what is referred to as multitenant architecture. In this architecture, a single version of the application, with a single configuration, is used for every customer (referred to as a tenant). To enable the service to scale well, it might be installed on several servers at the provider’s side. Dynamic scaling is utilized to allow more users to use the service as it becomes more popular.</p>
<p>Typical characteristics of SaaS include:</p>
<ul>
<li>Web-based access to the software service.</li>
<li>Software is managed from a central location by the cloud provider.</li>
<li>Software is delivered in a one-to-many model in which “one” is the cloud provider and “many” are the cloud users.</li>
<li>The cloud provider handles software upgrades and patches.</li>
</ul>
<p><strong>Pricing Models</strong></p>
<p>Unlike traditional software, which is sold under the software licensing model (with an upfront license cost and an optional ongoing support fee), SaaS providers generally price applications using a monthly or annual subscription fee. This model enables SaaS to fulfill one of the main purported advantages of cloud computing - reducing the capital expenditure or the upfront cost of software. SaaS providers typically charge based on usage parameters, such as the number of users using the application.</p>
<p><strong>Use Cases for SaaS</strong></p>
<p>SaaS is a good model for certain types of applications, such as:</p>
<ul>
<li>Applications that are fairly standardized and do not require custom solutions. E-mail is a good example of a fairly standardized application.</li>
<li>Applications that have a significant need for remote/web/mobile access, such as mobile sales management software.</li>
<li>Applications that have a short-term need, such as collaborative software for a particular project.</li>
<li>Applications in which demand spikes significantly, such as tax or billing software that is used once a month.</li>
</ul>
<p>However, there are situations where SaaS may not be the right choice, such as:</p>
<ul>
<li>Applications that require offline access to data.</li>
<li>Applications that require significant customization.</li>
<li>Applications in which policies or regulations disallow data from being hosted externally.</li>
<li>Applications in which existing in-house solutions satisfy all of the organization’s needs.</li>
</ul>
<p><strong>Examples of SaaS</strong></p>
<p>Web mail is one of the early examples of SaaS. Webmail enabled users with a browser and an Internet connection to access their e-mail anywhere at anytime. Offerings from Hotmail, Yahoo!, and Gmail are extremely popular. These services are based on the “freemium” model, wherein basic services are free, and more advanced features are available with a subscription. Furthermore, providers earn revenue mainly from advertisements that are displayed to the users as they use the service.</p>
<p>Another popular example of SaaS is online office suites, such as Google Drive and Microsoft Office 365, which allow users to create, edit, and share documents online. Google utilizes the freemium model for individual users. Microsoft has a charge model based on the features required and the number of users per month.</p>
<p><strong>The Platform-as-a-Service Model</strong></p>
<blockquote>
<p>Platform as a Service(definition)<br>Platform as a service (PaaS) is a computing platform that allows for the creation of Web applications in a simplified manner without the complexity of purchasing and maintaining any of the underlying software and infrastructure.</p>
</blockquote>
<p>PaaS-based offerings allow users to develop, deploy, and scale applications on platforms that are offered by cloud providers (Video 1.4). PaaS is analogous to SaaS, except that, rather than software delivered over the Web, it is a platform for the creation of software that is delivered over the Web.</p>
<p><a href="http://youtu.be/mxXm5s0hK8A" target="_blank" rel="external">Video 1.4: Platform-as-a-Service</a></p>
<p><strong>Characteristics of PaaS</strong></p>
<p>PaaS offerings vary among providers but usually feature some basic functionality, which includes:</p>
<ul>
<li>Services to develop, test, deploy, host, and maintain applications in the same integrated development environment (IDE).</li>
<li>Web-based user interface (UI) creation tools to help create, modify, and test various UI scenarios.</li>
<li>Multitenant architecture in which multiple concurrent users utilize the same development tools.</li>
<li>Built-in scaling mechanisms of deployed software that can be handled automatically by the cloud provider by load-balancing and failover mechanisms.</li>
</ul>
<p><strong>Pricing Models</strong></p>
<p>Unlike the SaaS pricing model (which is a subscription or advertisement based model), PaaS usually is priced in terms of usage of the platform. For example, Google App Engine’s <a href="https://cloud.google.com/pricing/" target="_blank" rel="external">charge model</a> accounts for an application’s inbound and outbound bandwidth as well as certain API requests. Consequently, the more an application developed using PaaS gets used, the more the PaaS developer gets charged.</p>
<p><strong>Use Cases for PaaS</strong></p>
<p>PaaS is a good model for certain types of applications, such as:</p>
<ul>
<li>Rapid application development scenarios.</li>
<li>Applications that require Web-based infrastructure to handle varying loads from users.</li>
<li>Applications that may not need redeployment or migration to other platforms in the future.</li>
</ul>
<p>There are certain scenarios in which PaaS may not be ideal, such as:</p>
<ul>
<li>When the application needs to be highly portable in terms of where it is hosted because PaaS APIs can vary from one PaaS provider to another.</li>
<li>When proprietary languages or APIs could impact the development process or cause trouble in the future due to vendor lock-in.</li>
<li>When application performance requires customization of the underlying hardware and software.</li>
</ul>
<p><strong>Examples of PaaS</strong></p>
<p>Google App Engine is an example of a PaaS. Using Google’s APIs, developers can create Web and mobile applications that run on Google’s infrastructure.</p>
<p><strong>The Infrastructure-as-a-Service Model</strong></p>
<blockquote>
<p>Infrastructure as a service(definition)<br>Infrastructure as a service (IaaS) is a cloud computing model in which cloud providers make computing resources available to clients, usually in the form of instances or virtual machines.</p>
</blockquote>
<p>In the IaaS model, providers rent out compute resources in the form of instances or virtual machines, which have some form of configurable CPU, memory, disk, and network bandwidth attached to them (Video 1.5). Once provisioned, IaaS users can remotely connect to these instances and configure their choice of platforms and applications. This model offers the most amount of flexibility to the IaaS users in terms of software development and deployment. Rather than purchasing servers, software, data center space, or network equipment, users rent those resources as a fully outsourced service on demand.</p>
<p><a href="http://youtu.be/sjQSV-5RaLU" target="_blank" rel="external">Video 1.5: Infrastructure-as-a-Service</a></p>
<p><strong>Characteristics of IaaS</strong></p>
<p>IaaS has the following characteristics:</p>
<ul>
<li>Computing resources are provided to IaaS users as a service.</li>
<li>IaaS providers provide tools that enable IaaS users to configure the dynamic scaling of resources.</li>
<li>IaaS providers usually have different resource offerings at different costs and follow a utility pricing model (typically calculated hourly).</li>
<li>The same physical resources are shared among multiple users.</li>
</ul>
<p><strong>Pricing Models</strong></p>
<p>Unlike the SaaS pricing model (which is a subscription- or advertisement-based model) or the PaaS model (which usually is priced in terms of number of transactions or bandwidth or storage used), IaaS usually is priced on an hourly basis, per instance. For example, Amazon Elastic Compute Cloud (EC2) offers a spectrum of compute resources as virtualized OS instances, which vary in compute, memory, storage, and bandwidth. At the time of writing, the Amazon EC2 t2.micro instance costs about 1.3 cents an hour when provisioned at Amazon’s Northern Virginia data center.</p>
<p>Cloud providers can also choose to bill on a prorated or non-prorated basis. On a prorated basis, each partial hour is billed partially, while on a non-prorated basis, each partial hour is billed as a full hour. This difference becomes significant when IaaS users need a large number of instances for a short period of time for burst processing. Amazon EC2 instances are billed on a non-prorated basis.</p>
<p><strong>Use Cases for IaaS</strong></p>
<p>IaaS makes sense in a number of situations:</p>
<ul>
<li>When demand for computing resources is volatile. For example, e-commerce sites experience the most demand during holiday seasons.</li>
<li>For new organizations that do not have the capital to invest in infrastructure on site.</li>
<li>When organizations need to grow their IT resources rapidly, such as Internet startup companies.</li>
<li>For temporary projects or temporary infrastructural needs (when organizations require a large amount of compute power for a limited amount of time).</li>
</ul>
<p>IaaS may not be the best option when:</p>
<ul>
<li>Regulatory compliance does not allow data to be offshored or outsourced.</li>
<li>When applications have strict quality-of-service (QoS) requirements.</li>
<li>When organizations have existing in-house customized infrastructure to meet their IT needs.</li>
</ul>
<p><strong>Examples of IaaS</strong></p>
<p>Amazon Web Services (AWS), Microsoft Azure and Rackspace are cloud service providers that offer IaaS products. Specifically, AWS’s Elastic Compute Cloud (EC2) is one of the first commercially successful IaaS products. AWS EC2 rents out instances from various data center locations scattered around the world. Users can choose from various instance types, from a low-memory, single CPU (which costs about several cents an hour), all the way up to multicore, high-performance, GPU-accelerated instances (which can cost upto several US dollars an hour).</p>
<h2 id="Cloud_Deployment_Models"><a href="#Cloud_Deployment_Models" class="headerlink" title="Cloud Deployment Models"></a>Cloud Deployment Models</h2><p><strong>Types of Clouds</strong></p>
<p>There are three well-known deployment models for cloud computing: public, private, and hybrid clouds. A public cloud is owned by a cloud provider but is made available to the public. A private cloud is typically owned by an organization, which also controls the access to the cloud. A hybrid cloud is a combination of public and private clouds. We discuss the different types in terms of ownership, infrastructure, end-user availability, cost, security, and data location.</p>
<p><strong>Public Cloud</strong></p>
<p>In a public cloud, the cloud infrastructure is owned by a cloud provider and is accessible to the public over the Internet (Figure 1.7). The cloud provider hosts the cloud infrastructure, and end users can access it remotely without the need to buy and setup a working environment (i.e., buying hardware and software). Public cloud resources are shared among different end users. Public cloud users are typically charged for the duration for which these services are used. However, public cloud charge models vary across providers. The security and terms of use are defined by the provider, and hence, end users must work within the constraints of the provider when using their services.</p>
<p><img src="/images/14529700702878.jpg" alt=""><br>Figure 1.7: Public cloud.</p>
<p><strong>Private Cloud</strong></p>
<p>In this second type of cloud, the cloud infrastructure is owned by an organization (Figure 1.8). The infrastructure is accessible to specific users via the organization’s intranet. The cloud environment needs to be procured, set up, operated, and maintained by the organization itself. The private cloud resources are typically shared amongst an organizations end users. Unlike the public cloud, security and terms of use of a private cloud are defined by the organization. The entire infrastructure is located in the organization, hence, security can be compliant with the organization’s policies.</p>
<p><img src="/images/14529700996814.jpg" alt=""><br>Figure 1.8: Private cloud.</p>
<p><strong>Hybrid Cloud</strong></p>
<p>In a hybrid cloud, the infrastructure includes an owned private cloud and a leased public cloud (Figure 1.9). Hybrid clouds enable the idea of “cloud bursting,” in which an organization uses its private cloud for most of its needs and dynamically provisions resources in the public cloud when utilization exceeds the capacity of its private cloud.</p>
<p><img src="/images/14529701211775.jpg" alt=""><br>Figure 1.9: Hybrid cloud.</p>
<p>Other types of clouds continue to emerge, for example, Community Clouds which share infrastructure among different organizations that have common security or other concerns. For example, various non-profit organizations that work closely with government might build and share a community cloud. Another type is Distributed Cloud which provides cloud computing using a set of distributed machines located at different geographical locations. An example is Cloud@Home which leverages volunteered resources as a shared resource.</p>
<h2 id="Popular_Cloud_Stacks"><a href="#Popular_Cloud_Stacks" class="headerlink" title="Popular Cloud Stacks"></a>Popular Cloud Stacks</h2><p>We will now do a quick run-down of cloud stacks that are currently popular in the market. We will quickly glance over the services offered by the major cloud providers, viz. Amazon Web Services, Microsoft, Google as well as OpenStack, the open cloud computing platform.</p>
<p><strong>Amazon Web Services (AWS)</strong></p>
<p>As of 2015, AWS is a market leader in several cloud computing segments, particularly in the IaaS space. Amazon Web Services started by commoditizing and leasing out several services that were developed in-house by Amazon’s engineering team to the wider public. AWS started by offering S3, the object storage service, and then went on to provide EC2, the elastic compute cloud. AWS is currently one of the largest cloud computing companies.</p>
<p>AWS’s stack primarily consists of the following components:</p>
<p>Compute: Amazon’s primary compute solution is Elastic Compute Cloud (EC2), which provides users with virtual machines, or instances of various capacities for hourly or longer term rentals. EC2 forms the backbone of the AWS cloud stack in terms of compute infrastructure. EC2 instances can be managed directly through the AWS EC2 APIs, or through other services such as AutoScaling.</p>
<p>Storage: AWS offers multiple products in this space. Block storage is provided by Elastic Block Storage (EBS) volumes, which can be attached and detached from EC2 instances. Object storage is provided by the simple storage service (S3), which allows for binary large objects (BLOBs) to be stored and retrieved using a simple HTTP service. AWS also offers a varied suite of database services, including RDS which offers a managed SQL service, DyanmoDB, which offers a highly scalable, low-latency key-value store, and ElastiCache, an in-memory database store.</p>
<p>Networking: Amazon’s Virtual Private Cloud (VPC), Elastic Load Balancer (ELB) and Route 53 are networking services that can be used to manage the connectivity between your instances and services deployed in AWS and the outside world.</p>
<p>PaaS Products: AWS’s platforms are large and varied to cater to different application needs. AWS provides a suite of analytics platforms such as Elastic MapReduce (EMR), Amazon Kinesis and Redshift. Rapid web application development is possible through AWS Elastic Beanstalk. Amazon also offers many products to manage and control cloud deployments such as CloudFormation, OpsWorks and CodeDeploy.</p>
<p><strong>Microsoft Azure</strong></p>
<p>Microsoft Azure is one of the fastest growing cloud services in the market, with impressive growth and an increasingly expanding portfolio of cloud services. Azure also leverages Microsoft’s large data center presence worldwide, as well as CDN sites that are spread across 24 countries. Subsets of Microsoft’s Cloud Platform are available as the Windows Azure Pack, which allows an organization to build a private cloud which can seamlessly connect and interact with the Azure public cloud.</p>
<p>Compute: Microsoft offers Azure Virtual Machines, which can be configured to run Windows or various flavors of Linux. The virtual machines are managed by Azure Cloud Services, which provides a multi-language cloud management platform. A unique aspect of Azure is the staging environment and simulator, which allows developers to test out a cloud deployment before putting it into production.</p>
<p>Storage: Azure offers several storage solutions, including: Azure Blobs to store binary large objects; Azure Tables, to store NoSQL tables; and Azure Files, which offer SMB-based storage endpoints (Windows-compatible file servers) to mount and store files in the cloud. Azure also offers managed Relational Database services through the Azure SQL Database; a managed NoSQL document database service, DocumentDB; and high-performance key-value cache through Azure Redis Cache. Microsoft also offers a unique storage appliance called StorSimple, which is an SSD/HDD hybrid storage array deployed at the clients side, and also connects to Azure for backup, analytics and/or cloud deployment.</p>
<p>Networking: Microsoft also offers virtual private networking services through Azure Virtual Network. Another unique feature of Microsoft’s Azure cloud is the ability to purchase dedicated fiber connectivity to Microsoft’s data centers through ExpressRoute. Azure Traffic Manager can be used to load balance traffic to Azure Virtual Machines.</p>
<p>PaaS Products: Azure offers several PaaS products: Azure Websites is the primary PaaS platform, which enables developers to deploy scalable web applications on the Azure platform. Azure Mobile Services allow developers to create the infrastructure required to support mobile applications. In the analytics space, Azure offers several products including HDInsight, which is a managed Hadoop cluster service similar to Amazon’s EMR. Microsoft also offers managed Machine Learning and Stream Analytics services to developers.</p>
<p><strong>Google Cloud Platform</strong></p>
<p>Google’s Cloud Platform initially offered only PaaS products and APIs into Google’s most powerful products such as the Translate API. The Google Cloud Platform has now diversified into multiple services in response to the offerings of its competitors.</p>
<p>Compute: Google’s primary compute platform is the Google Compute Engine (GCE), which offers Linux virtual machines of various sizes depending on the application requirements. A unique differentiator of Google’s platform is that instances are billed by the minute, with a minimum charge of 10 minutes.</p>
<p>Storage: Google offers three primary storage services, namely Cloud Storage, which is an object storage service similar to S3 and Azure Blobs. Google’s Cloud Datastore is the managed NoSQL datastore service that allows users to store non-relational data with high scalability, but optionally supports transactions and SQL queries on your data. In addition, Google offers a traditional managed SQL database service called Cloud SQL.</p>
<p>Networking: Google offers several networking products to manage the connections between Google’s cloud services and the outside world, namely Load Balancing, Interconnect and DNS services.</p>
<p>PaaS Products: Google’s primary PaaS offering is Google App Engine (GAE), which allows developers to deploy an application using Google’s SDK. In addition, Google offers data analytics platforms such as BigQuery, which allows users to run SQL-like queries against multi-terabyte datasets. Cloud Endpoints allows developers to create RESTful services accessible from Mobile and browser clients. In addition, Google’s established products such as Prediction and Translate are available as APIs for access to developers to integrate into their own application.</p>
<p><strong>OpenStack</strong></p>
<p>All of the stacks we have looked at so far are proprietary stacks hosted by the companies on their public clouds. The OpenStack model is markedly different as it’s an open-source cloud stack that is available for both public and private clouds. OpenStack defines itself as a “cloud operating system that controls large pools of compute, storage, and networking resources throughout a datacenter, all managed through a dashboard that gives administrators control while empowering their users to provision resources through a web interface”. OpenStack can be deployed on anywhere from a bunch of machines to an entire datacenter. Public clouds that offer OpenStack include Rackspace and HP Helion.</p>
<p>Compute: OpenStack’s compute offering offers similar services as the public cloud counterparts, with the ability to manage virtualized and commodity server resources, with API-based access. A unique aspect of OpenStack’s compute system (called Nova) is support for a wide range of Hypervisors such as XenServer and KVM, as well as a wide range of hardware support, which includes ARM-based systems.</p>
<p>Storage: OpenStack offers two types of storage services: an object storage service (called Swift), as well as block storage services (called Cinder). These can be deployed and scaled according to environment and the application needs. Database systems can be deployed on top of virtual machines and storage services, if required, but OpenStack does not use or promote any particular type of database solution. Public clouds that use OpenStack, like Rackspace, offer MySQL, Percona or MariaDB deployed on top of OpenStack VMs as a service.</p>
<p>Networking: OpenStack offers a pluggable, scalable and API-driven system called Neutron to manage networks, VLANs and IP address pools for virtual machines. A novel feature of OpenStack networking is support for Software Defined Networks such as OpenFlow, which enable fine-grained configuration of networking hardware in response to provisioning or traffic requirements. More information on Software Defined Networks will be covered later.</p>
<p>PaaS Products: OpenStack itself does not have any PaaS services, but public cloud providers that are built on top of OpenStack have a few. For example, Rackspace provides several platforms for website hosting and managed Hadoop clusters.</p>
<p><strong>References</strong></p>
<ol>
<li>Li Ang, et. al. (2010). CloudCmp: Shopping for a Cloud Made Easy . Proceedings of the 2nd USENIX conference on Hot topics in cloud computing .</li>
</ol>
<h2 id="Cloud_Use_Cases"><a href="#Cloud_Use_Cases" class="headerlink" title="Cloud Use Cases"></a>Cloud Use Cases</h2><p><strong>Use Cases for the Cloud</strong></p>
<p>With the rapid evolution of cloud technologies, there are new use cases emerging every day. In this section, we discuss some of the common cloud use cases.</p>
<p><strong>Web/Mobile Applications</strong></p>
<p>A main driver for cloud computing comes from Web hosting. Websites and Web applications typically are hosted on a server with a dedicated internet connection. Older Web hosting services either provided dedicated servers to clients or gave a fraction of a larger UNIX system to multiple clients. Now, with the advent of cloud computing, Web/mobile applications can be built on top of existing IaaS/PaaS or even SaaS services.</p>
<ul>
<li>SaaS based: Using the SaaS model, organizations can deploy one-size-fits-all applications on the Web. Common examples include WebMail, social networking sites, and utility websites, such as personal organizers, calendars, and planners.</li>
<li>PaaS based: Application developers can use a range of online platforms and tools to create SaaS and mobile applications. Platforms such as Google App Engine, Parse, and AppScale are popular platforms on which Web and mobile applications can be built.</li>
<li>IaaS based: Applications that need even more customization and flexibility can adopt the IaaS model by renting out virtual machines from providers such as EC2 and Rackspace and deploy a fully customized software stack to run the Web application.</li>
</ul>
<p>Consider the following scenarios:</p>
<ul>
<li>Animoto, an online video slideshow creator, decided to deploy a Facebook application. Traffic to the service surged, which resulted in Animoto scaling up from 50 servers to 3,500 servers in 3 days. Such elastic scalability is made possible through cloud computing.</li>
<li>Online retail stores that use cloud computing, such as Amazon and Target.com, have been able to size up infrastructure for peak activity (such as the day after Thanksgiving, or Black Friday). Salesforce.com hosts customers ranging from 2 seat to more than 20,000 seat customers, all using the same Web platform.</li>
</ul>
<p><strong>Big Data Analytics</strong></p>
<p>Many organizations have to deal with large amounts of data. This data may emanate from such areas as sensors, experiments, transactional data, and Web page activity. Big data processing usually requires a lot of computational and storage resources but, depending on an organization’s needs, may be periodic or seasonal. For example, Amazon may have business intelligence and analytics jobs setup for the end of the day, which may require a few hours of time from a few hundred servers. In these scenarios, cloud computing makes sense because these resources can be acquired on demand. Many firms even have fully automated analytics pipelines that automatically collect, analyze, and store data, with resources being provisioned on demand. Examples of big data scenarios include the following:</p>
<ul>
<li>The Union Pacific Railroad mounts infrared thermometers, microphones, and ultrasound scanners alongside its tracks. These sensors scan every train as it passes and send readings to the railroad’s data centers, in which pattern-matching software identifies equipment at risk of failure.</li>
<li>Traditional retailers, such as Walmart, Sears, and Kmart, are following in the footsteps of online retailers, such as Amazon, by analyzing consumer spending habits to offer personalized marketing campaigns and offers to individual customers.</li>
<li>Companies such as Time Warner and Comcast are using big data to track media consumption habits of their subscribers and provide value-added information to advertisers and customers. The video games industry tracks the gameplay habits of millions of console owners. Companies such as Riot Games sift through 500GB of structured data and over 4TB of operational logs every day.</li>
</ul>
<p><strong>On-Demand, High-Performance Computing</strong></p>
<p>Modern science is impossible without high-performance computing (HPC). In addition to physical experimentation, computer-based simulation has become popular in fields ranging from astrophysics, quantum mechanics, and oceanography to biochemistry. Such workloads are computationally demanding and typically are run on dedicated clusters or at supercomputing facilities.</p>
<p>Scientists are now increasingly looking toward the cloud for HPC resource demands. Amazon EC2 offers extremely powerful instances with more CPU and even GPU-acceleration for HPC use. Scientists find the availability of vast amounts of computational power appealing, particularly for small projects or time-sensitive, bursty analytics, such as experimental runs before submitting research paper deadlines. Examples of HPC on the cloud include the following:</p>
<ul>
<li>A 3,809-instance EC2 cluster was set up by Cycle Computing, Inc. for a pharmaceutical company to run molecular modeling jobs. The cluster has a total of 30,472 cores, 26.7TB of RAM, and 2PB of disk storage.</li>
<li>Companies such as Pfizer, Unilever, Spiral Genetics, Integrated Proteomics Applications, and Bioproximity run bioinformatics and genomics workloads on Amazon EC2 instances.</li>
<li>NASA JPL uses high-performance Amazon EC2 instances to process high-resolution satellite images.</li>
</ul>
<p><strong>Online Storage and Archival</strong></p>
<p>One of the important resources that is available through cloud computing is storage. From personal storage solutions, such as Dropbox, to large-scale Internet storage systems, such as Amazon S3, online storage is a major cloud computing use case. The online storage options include:</p>
<ul>
<li>Web-based object storage: Services such as Amazon S3 allow users to store terabytes of data as simple objects that can be accessed over HTTP. Many websites use Amazon S3 to store static content, such as images.</li>
<li>Backup and recovery: Services such as CrashPlan and Carbonite provide online backup of customer data, which is a great option as a secure, off-site backup solution.</li>
<li>Media streaming and content distribution: Services such as Amazon CloudFront not only store large amounts of data but assist in content delivery. Requests to pull data from CloudFront are automatically routed to the nearest server, thereby decreasing latency for time-sensitive media, such as video.</li>
<li>Personal storage: Services such as Dropbox and Google Drive are popular among users to store personal documents online for anytime, anywhere access.</li>
</ul>
<p><strong>Rapid Application Development and Testing</strong></p>
<p>One of the major advantages of the cloud is the ability to rapidly deploy and test applications. An entire computing environment can be deployed in minutes and then torn down and discarded just as easily after the testing is complete. For many companies, the value is in allowing developers to quickly create enhancements and features and test them without any risk. Specialized hardware and servers do not need to be ordered and installed. Within mere minutes, a virtual server can be spun up on EC2. Applications can also be easily stress/load tested. Existing servers can be cloned to perform scalability studies as well.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Introduction to Cloud Computing Summary</p>
<ul>
<li>Cloud computing is the delivery of computing as a service over a network, whereby distributed resources are provided to the end user as a utility.</li>
<li>The idea of utility computing originated in the 1950s and 1960s, but the enabling technologies evolved decades later and have finally matured to a state in which cloud computing is a viable option for organizations to invest in.</li>
<li>The enabling technologies of cloud computing are networks, virtualization and resource management, utility computing, programming models, parallel distributed computing, and storage technologies.</li>
<li>Cloud computing consists of four building blocks: application software, development platforms, resource sharing, and infrastructure.</li>
<li>Cloud service models exist at various levels in the building blocks.</li>
<li>Software as a service (SaaS) is at the application software layer and is the delivery of SaaS over the Internet (typically through a Web browser).</li>
<li>Platform as a service (PaaS) is at the development platform layer and can be defined as a computing platform that allows for the creation of Web applications in a simplified manner without the complexity of purchasing and maintaining any of the underlying software and infrastructure.</li>
<li>In the Infrastructure as a service (IaaS) model, providers rent out compute resources in the form of instances or virtual machines, which have some form of CPU, memory, disk, and network bandwidth attached to them.</li>
<li>There are three well-known deployment models for cloud computing: public, private, and hybrid.</li>
<li>Popular cloud providers include Amazon Web Services, Microsoft Azure, Google Cloud Platform and OpenStack. Each provider typically offers a stack consisting of compute, storage and networking services, among others.</li>
<li>Some of the most popular use cases for the cloud include: web and mobile applications, big data analytics, on-demand high performance computing, online storage and archival, and rapid application development and testing.</li>
</ul>
<hr>
<h1 id="Economics_2C_Benefits_2C_Risks_2C_Challenges_and_Solutions"><a href="#Economics_2C_Benefits_2C_Risks_2C_Challenges_and_Solutions" class="headerlink" title="Economics, Benefits, Risks, Challenges and Solutions"></a>Economics, Benefits, Risks, Challenges and Solutions</h1><p>Now that we have covered the fundamental ideas behind cloud computing, it is important to explore some of the benefits, risks and ongoing challenges facing cloud computing. Since one of the main drivers of cloud adoption is reducing upfront costs and leveraging the economies of scale, we will then look at the economics behind the cloud from two perspectives- that of cloud users and service providers.</p>
<p>Since computing has migrated from a product to a service, there should be methods to define relationships between the users and service providers. The cloud has adopted traditional contractual guarantees like Service Level Agreements (SLAs) that meet a cloud user’s Service Level Objectives (SLOs). We will see how Cloud Service Providers define SLAs and SLOs to ensure a level of service to their clients. We will also briefly introduce the concept of cloud service auditing to verify conformity with SLAs.</p>
<p>Apart from contractual concerns, security and privacy are the biggest challenges that are preventing sensitive applications from moving to the cloud. We will explore some of the unique security challenges raised by the cloud and the controls required to resolve them.</p>
<p>Although the cloud has benefited from rapid adoption, there is no one-size-fits-all solution that encompasses all IT requirements. Keep in mind that the cloud is an emerging and fast-evolving paradigm with many active research areas exploring its future.</p>
<h2 id="Benefits_2C_Risks_2C_and_Challenges_of_Cloud_Computing"><a href="#Benefits_2C_Risks_2C_and_Challenges_of_Cloud_Computing" class="headerlink" title="Benefits, Risks, and Challenges of Cloud Computing"></a>Benefits, Risks, and Challenges of Cloud Computing</h2><p><strong>Benefits of the Cloud</strong></p>
<p>The popularity of cloud computing is driven by its numerous benefits, including improved economics, simplified IT management, scalability, flexibility, improved utilization, and a reduced carbon footprint. The following video (Video 1.6) discusses these benefits.</p>
<p><a href="http://youtu.be/uKpFLuqOy-o" target="_blank" rel="external">Video 1.6: The benefits of cloud computing</a></p>
<p>Although presented earlier and in the video above, let us consider the cloud benefits individually:</p>
<ul>
<li>Economic model: Organizations typically estimate their IT requirements for a period of 1 to 5 years in advance in a process called capacity planning. Capacity planning leads organizations to estimate IT investments for peak loads, which could lead to either excess capacity at times (underutilized resources) or deficient capacity when loads exceed projections (which could lead to service degradation). With the pay-as-you-go economic model, organizations pay for the resources that they need. Organizations no longer have to pay upfront cost, invest in and procure expensive computing infrastructure, nor do they have to pay recurring costs to manage their infrastructure. This is particularly important for startups because by leasing compute resources they benefit from reduced upfront cost and reduced time to market when creating and making their offerings available to the general public.</li>
<li>Simplified IT management: Users of cloud services need not allocate time and resources to setup, operate, and maintain their IT resources. The cloud provider, however, competes for clients and hence invests significant resources to manage and maintain their offerings with high reliability.</li>
<li>Scalability: In a traditional, in-house computing environment, organizations can take anywhere from a few days to several months to procure, set up, and operate IT infrastructure. Cloud service providers provision rented computing resources for their clients in a matter of hours or even minutes. Clients not only can scale up resources on demand but can scale them down during lull periods to save money. Therefore, clouds enable the important property of elasticity, wherein resources can be both provisioned and deprovisioned in a dynamic or programmatic manner to adapt to workload changes. In order to support elasticity, many cloud service providers make automated scaling solutions available to dynamically alter resource provisioning as demand fluctuates.</li>
<li>Flexibility: For certain cloud services, providers offer their users the flexibility to configure any software platform to run on any available operating system as a virtualized image on custom-provisioned, rented infrastructure. Cloud offers a shift from an organization’s inflexible IT design decisions (that are tied to specific development platforms and infrastructure) to more flexible, elastic, and modular choices.</li>
<li>Improved utilization: Resource utilization is significantly improved with cloud computing because physical resources are shared across users (multitenancy). Through virtualization, servers are now consolidated as operating system images that are sharing the same system resources. Hence, utilization is improved, which leads to overall savings in power and cooling and reduces the carbon footprint.</li>
<li>Rapid and Global Deployment: By employing the services of cloud service providers that also have a global data center presence, startups can compete with established players by rapidly rolling out applications and services across a global audience. This is particularly important of social media startups which may see viral growth trends as services become popular across multiple countries.</li>
</ul>
<p><strong>Risks of Cloud Computing</strong></p>
<p>By embracing cloud services, users and organizations can take advantage of the above benefits, however, using these services introduces several risks, such as:</p>
<ul>
<li>Vendor lock-in: Cloud computing is slowly becoming standardized. OpenStack is an open source cloud computing platform which aims to standardize the cloud-computing software stack but is not fully compatible with Amazon and is incompatible with current offerings from Microsoft, and Google. Lack of standardization can lead to the situation of vendor lock-in, such as when a client signs up for a nonstandard cloud service, develops applications, and deploys data on it. The lack of standardization makes it unlikely for the client to move to another vendor seamlessly. The client often requires a third-party cloud migration specialist or an additional service to move the application to a different platform.</li>
<li>Security risks: Because cloud computing with public clouds can result in an organization’s data being shipped beyond its four walls, security becomes a primary risk and concern. For certain domains, it is simply unacceptable for users or organizations to do so, in which case they may have to resort to building a private cloud or own resources with restricted access to meet their needs. However, certain markets that have tight security requirements have niche solutions. An example is Amazon GovCloud, which meets certain US federal government requirements for data security and integrity. GovCloud is physically distinct from other cloud infrastructures that Amazon makes available to the public, thereby reducing exposure.</li>
<li>Privacy risks: The use of the cloud also raises many privacy-related concerns. Depending on the laws under which a cloud service provider operates, governments may have the power to search and seize data from the provider without the client’s explicit consent or notification. Furthermore, clients cannot be fully assured of data confidentiality when using public clouds. We discuss some of security risks associated with cloud computing later in this module.</li>
<li>Reliability risks: Clouds are also plagued with reliability issues. In December 2012, Netflix users experienced a service outage due to Amazon’s “connectivity issues and degraded performance” from their servers in Virginia. Amazon EC2’s Northern Virginia data center, one of the most popular public clouds, went down for a few days in 2011, affecting websites such as Reddit and Foursquare. Windows Azure also faced a similar problem, and their services went down for 2½ hours in Western Europe. Public clouds hence are a potential reliability risk that can affect organizations. Clients must design for failures and use features such as Amazon’s multiple availability zones, in which clients can set up failover and redundant infrastructures to take over in case of failure, which comes at a price, of course. Cloud users attempt to mitigate the cloud reliability risk by signing Service Level Agreements (SLAs) that enable compensation when exposed to such events. Since cloud services can only be accessed over the network, any disruption of connectivity will cause the application to fail, leading to a loss of reputation and/or revenue.</li>
</ul>
<p>Some of these risks are not specific only to cloud computing, but are typical for any service provider- be they banking or health services. As with any other service, cloud service providers must carefully consider the implications of these risks and design solutions to mitigate them as their credibility and reputation directly impacts their rate of adoption. Cloud adopters who offer their own services must also safeguard their reputation against these risks.</p>
<p><strong>Challenges in Cloud Computing</strong></p>
<p>Along with the benefits and risks, there are several challenges associated with the adoption of cloud computing:</p>
<ul>
<li>Application engineering and development: A cloud inherently offers the promise of on-demand, dynamically scalable infrastructure. Programming a cloud, however, is more complex than writing code for a single machine. New programming paradigms (such as MapReduce, Spark or GraphLab), coupled with provider APIs to manage infrastructure, help developers manage complexity but still present a steep learning curve. In addition, skilled developers with cloud experience are rare, and both costs and time for application development increase substantially with clouds. Finally, these new cloud programming models and APIs are continually evolving, which may add to recurring engineering and development costs.</li>
<li>Movement of data: Use of public clouds typically requires connecting to the cloud over the Internet. Because of this requirement, movement of data to and from the cloud is significantly slower than in an organization’s local area network (LAN). Although the cloud allows applications to target large amounts of data (big data), data movement can become a limiting factor for cloud adoption. For example, Amazon allows clients to upload large datasets for free or ship hard disks with data so that they can load them into the required cloud service.</li>
<li>Quality of service (QoS): As mentioned earlier, cloud infrastructure is typically shared among many users. This sharing presents a challenge for cloud providers to offer QoS assurances to their clients. This challenge could disallow certain performance-sensitive applications from being migrated to the cloud. QoS in clouds is an important area in cloud research. For example, regulating I/O bandwidth to specific virtual machines could offer predictable performance for critical applications. I/O virtualization is covered in Unit 3.</li>
</ul>
<h2 id="Business_Case_for_Cloud_Computing__28for_Users_29"><a href="#Business_Case_for_Cloud_Computing__28for_Users_29" class="headerlink" title="Business Case for Cloud Computing (for Users)"></a>Business Case for Cloud Computing (for Users)</h2><p><strong>Evolution of the IT Business Model</strong></p>
<p>An organization’s IT costs are many-fold, which include expenses for hardware and software as well as expenses for support and management. Typically, these costs fall into two categories:</p>
<ul>
<li>Capital expenses (CapEx): The initial investment for a particular IT service or solution. For example, when an organization decides to implement a software solution to address a particular need, say, enterprise resource planning (ERP), CapEx would include all physical hardware and software purchases. CapEx investments are for the lifetime of the long-lived solution. CapEx are an upfront expense, which are either paid as a lump sum or financed with extra charges.</li>
<li>Operational expenses (OpEx): The recurring costs incurred while operating a particular system. For the ERP case, that would include utility fees (such as power and cooling) to keep the infrastructure running, space leases if the facility is rented, personnel costs to support the system, and software support and license fees. OpEx are typically a monthly recurring charge.</li>
</ul>
<p>The business model for IT software has evolved over the years into the following forms:</p>
<ol>
<li>Traditional model: An organization purchases licensed software, which it then owns and maintains.</li>
<li>Open-source model: Software is essentially free, but the organization pays vendor support costs.</li>
<li>Outsourcing model: An organization hires another company, possibly overseas, to manage and maintain the software.</li>
<li>Hybrid model: A software vendor sells highly standardized software to many clients, along with software management and support, thereby amortizing costs of expertise, software management, and support over several clients.</li>
<li>Cloud computing model: Software is developed and delivered over the Internet to many clients at lower costs.</li>
</ol>
<p>The following video (Video 1.7) discusses the evolution of these models with examples: </p>
<p><a href="http://youtu.be/yOTcTwWbEpk" target="_blank" rel="external">Video 1.7: Economics of cloud computing, software service models, and costs</a></p>
<p><strong>Reducing Capital Expenditure</strong></p>
<p>Organizations choose to reduce their capital expenditures so that they limit the commitment of large investments for long-lived IT resources. Shifting expenses away from capital expenditures into operational expenditure enables organizations to stretch their IT budgets and limit upfront costs. Specifically, organizations opt to make investments that have a bigger return on investment in the short term rather than investing in long-lived, depreciating IT resources. Operating expenses are pay-as-you-go, meaning organizations pay by the month and get value every month. With cloud computing, they can simply rent the resources and incur little to no capital expenditures.</p>
<p>The Cloud Computing paradigm offers a transition of the IT Business Model from CapEx to OpEx. CapEx in IT systems is a long-term investment that freezes a large sum of money into a single investment. OpEx, on the other hand, is a recurring expense which could enable the company the agility to utilize the funds in other profit yielding investments.</p>
<h2 id="Economics_of_Cloud_Computing__28for_Providers_29"><a href="#Economics_of_Cloud_Computing__28for_Providers_29" class="headerlink" title="Economics of Cloud Computing (for Providers)"></a>Economics of Cloud Computing (for Providers)</h2><p><strong>Cloud Service Provider Economics</strong></p>
<p>Since the cloud user has no longer needs to invest in Capital Expenditures, from a cloud service provider’s perspective, long-term CapEx as well as recurring OpEx costs are unavoidable. An important challenge for cloud service providers is to satisfy the demands of their clients while achieving high-average utilization in order to make a profit, which depends on their ability to build data centers with high efficiency and reliability at manageable costs. Cloud service providers amortize their costs over a large number of users.</p>
<p>Cloud service providers build large and reliable data centers in order to attract a large number of users in order to improve their profitability. Just like other utility providers, cloud service providers can then procure and maintain hardware and software at significant savings per unit.</p>
<p><strong>Economies of Scale</strong></p>
<p>Cloud service providers organize their infrastructure into large data centers, which typically leverage three main areas:</p>
<ul>
<li>Supply-side savings: Large-scale data centers lower costs per server.</li>
<li>Demand-side aggregation: Aggregating demand for computing allows server utilization rates to increase.</li>
<li>Multitenancy efficiency: When changing to a multitenant application model, increasing the number of tenants (i.e., customers, or users) lowers the application management and server cost per tenant.</li>
</ul>
<p>Cloud service providers undertake the difficult task of building and maintaining data centers for users. For this model to be feasible, cloud service providers will have to leverage economies of scale and bring in many users. Providers benefit from economies of scale in the following areas:</p>
<ul>
<li>Cost of power: Electricity is rapidly becoming the largest element of total cost of ownership (TCO) in a data center, contributing to approximately 15% to 20% of total costs. Large cloud service providers can place their data centers in locations with lower cost of power and sign bulk purchase agreements with electric providers to reduce electric costs significantly.</li>
<li>Infrastructure labor costs: Cloud computing enables repetitive management tasks to be automated. In addition, in larger facilities, a single system administrator can service thousands of servers with the use of advanced management software.</li>
<li>Buying power: Cloud service providers can purchase equipment in bulk from manufacturers, which can lead to major discounts over smaller buyers. In addition, cloud providers standardize their servers and equipment, which helps in lowering purchase and support costs compared to smaller IT departments.</li>
</ul>
<p>Technologies in data centers and their design considerations are covered in detail in Unit 2.</p>
<h2 id="Service_Level_Agreements_and_Objectives"><a href="#Service_Level_Agreements_and_Objectives" class="headerlink" title="Service Level Agreements and Objectives"></a>Service Level Agreements and Objectives</h2><p>In this course so far, we have talked about the fundamental ideas behind cloud computing and some of the service models that have emerged under the cloud computing paradigm. Assuming an organization wants to move their infrastructure and services to a cloud provider, several questions arise. For example, how does an organization:</p>
<ul>
<li>define its requirements in terms of the services that they require from the cloud service provider?</li>
<li>identify the type and quantity of the services that it requires?</li>
<li>negotiate the level of service and support that it expects from a cloud provider?</li>
<li>monitor and validate the type and quality of service that was guaranteed by the cloud service provider?</li>
</ul>
<p>When an organization needs to formally state its service requirements in business and legal terms, it defines these requirements in terms of service level objectives.</p>
<blockquote>
<p>Service Level Objective(definition)<br>A service level objective is defined as a key element that defines some aspect of the service which is expected from the service provider.</p>
</blockquote>
<p>A common service level objective with cloud service providers, for example, is an uptime guarantee, where-in a service is guaranteed to be available and running within normal operational parameters for a certain percentage of the time.</p>
<p>Service level objectives are typically defined and negotiated between the client and a service provider in a larger contract known as the service level agreement.</p>
<p><strong>Service Level Agreements</strong></p>
<blockquote>
<p>Service-Level-Agreement(definition)<br>A service level agreement (SLA) is a contract between a service provider (either internal or external) and the client that defines the level of service expected from the service provider.</p>
</blockquote>
<p>Service level agreements exist in many industries in a supplier-customer relationship exists for a service that is provided by the supplier to the customer periodically. Service level agreements in information technology, in their current form, have been used since late 1980s by fixed line telecom operators as part of their contracts with corporate customers.</p>
<p>A typical SLA may consist of the following segments:</p>
<ul>
<li>a definition of services to be provided by the service provider to the client,</li>
<li>methods to measure performance,</li>
<li>protocols to manage problems,</li>
<li>a list of customer duties,</li>
<li>warranties that need to be honored by the service provider,</li>
<li>procedures involved for disaster recovery, and</li>
<li>process and policies regarding the termination of the agreement.</li>
</ul>
<p><strong>SLAs in Cloud Computing</strong></p>
<p>SLAs have evolved over the years to cater to different types of IT services. The evolution of shared infrastructure services such as clouds have necessitated the use of strong service level agreements. SLAs by definition can define any level of service, but a well-structured and negotiated SLA between a cloud service provider and a client will ideally [1] :</p>
<ul>
<li>Codify the specific parameters and minimum levels required for each element of the service, as well as remedies for failure to meet those requirements.</li>
<li>Affirm the client’s ownership of its data stored on the service provider’s system, and specifies the client’s rights to get it back.</li>
<li>Detail the system infrastructure and security standards to be maintained by the service provider, along with the client’s rights to audit their compliance.</li>
<li>Specify the client’s rights and cost to continue and discontinue using the cloud service provider’s service.</li>
</ul>
<p><strong>Auditing in Cloud Computing</strong></p>
<p>Although cloud computing provides numerous advantages, one of its main challenges continues to be the reliability of cloud services. A fast evolving approach to address reliability is cloud auditing. Let’s assume that a client has employed one or more cloud services from a cloud service provider. The cloud computing business model abstracts away many aspects of the infrastructure from the client which now become the responsibility of the cloud service provider. The cloud services are managed by the cloud service provider to implement the services agreed upon in the SLA.</p>
<p>Auditing evaluates whether the cloud services comply with the SLA through monitoring. A third party auditor is requested and trusted by the client to assess the cloud service(s). Hence, public auditability of cloud services is necessary to allow clients to resort to an external auditor to check the integrity of the cloud services. The cloud service provider makes available resource usage and performance monitoring and takes measures to ensure the security of its services to its clients through providing physical security, isolation, authentication, firewalls and APIs. A third party auditor should be able to efficiently audit the cloud services without overloading the client and without adding vulnerabilities to the client’s services.</p>
<p>Given the nature of cloud services, near real time auditability is becoming necessary. This requires real time monitoring and evaluation in order to trigger a rapid response to safeguard the client’s service and reputation. In public clouds, this has to be achieved while preventing the exposure of client data to other cloud clients. Near real-time auditing is rapidly evolving and becoming a requirement for reliable cloud computing services which will require audit trails and monitoring of service, performance and security metrics among others.</p>
<p><strong>References</strong></p>
<ol>
<li>Thomas Trappler If It’s in the Cloud, Get It on Paper: Cloud Computing Contract Issues . <a href="http://www.educause.edu/ero/article/if-its-cloud-get-it-paper-cloud-computing-contract-issues" target="_blank" rel="external">http://www.educause.edu/ero/article/if-its-cloud-get-it-paper-cloud-computing-contract-issues</a>.</li>
</ol>
<h2 id="Cloud_Security_-_Threats"><a href="#Cloud_Security_-_Threats" class="headerlink" title="Cloud Security - Threats"></a>Cloud Security - Threats</h2><p>Now that we understand how the agreement between client and provider is met, let’s take a look at a major concern for cloud service providers and users alike: security.</p>
<p>As cloud service providers compete for market dominance, their security features have become a key service differentiator.</p>
<p>At one level, cloud service providers can leverage the economies of scale. By implementing security measures at a large scale, they are able to provide more affordable defensive mechanisms at a lower cost. Typically, this includes network monitoring and filters, patch management, hardening, incident response &amp; forensics, and various types of threat management.</p>
<p>They also generally provide an accessible interface to modify security settings, allowing secure key rotation, timely updates and patches. Additionally, since all actions are virtualized, these can be regularly snapshotted and analyzed forensically for exploitation using vulnerabilities that are yet unknown (also known as zero-day vulnerabilities).</p>
<p>Let us look at the cloud from the point of view of a traditional enterprise which used in-house IT infrastructure. Enterprises find that they lose control as a function of asset ownership as they move away from traditional servers towards private clouds and then up the stack from IaaS to SaaS (Figure 1.10). In all three service models, the cloud vendor has full ownership of the underlying infrastructure (networks, storage and hosts). In PaaS, the service provider may additionally claim partial ownership of the application infrastructure. Finally, in the SaaS model, the application infrastructure is fully owned by the service provider.</p>
<p><img src="/images/14529717394416.jpg" alt=""><br>Figure 1.10 - Enterprises lose control as you move up the public cloud stack</p>
<p>In all three models, however, the enterprise has full ownership over all its data. Unfortunately, it does not have full control over this data, as it is stored outside the network perimeter. This lack of control over sensitive data storage and transfer is one of the leading inhibitors to large-scale cloud adoption. Two-thirds of potential adopters have placed “data security and privacy” as the biggest risk in cloud computing (Figure 1.11).</p>
<p><img src="/images/14529717521135.jpg" alt=""><br>Figure 1.11 - Security concerns are the biggest barrier to large-scale cloud adoption</p>
<p><strong>Threats</strong></p>
<p>The biggest threats at a high-level are those caused due to vendor lock-in (since applications are not very portable between platforms), compliance risks (e.g. meeting most compliance standards is more complex on public clouds), and a loss of governance (most cloud service providers do not provide SLAs related to data security assurances).</p>
<p>At a lower level, the threats are due to shared infrastructure, lack of a hard perimeter, and limited control over physical data storage and deletion. It must be noted that attacks against hypervisors and shared hosting are significantly rarer and more difficult than attacks against OSes and networks that plague traditionally deployed applications.</p>
<p>The Cloud Security Alliance provides a taxonomy of threats, which are summarized here:</p>
<p><strong>Threat #1: Abuse and Nefarious Use of Cloud Computing</strong>(IaaS, PaaS)</p>
<p>Criminals can leverage the anonymity provided by public clouds to launch malicious attacks at low cost. Public clouds have been used for Command and Control Botnets, CAPTCHA cracking, rainbow table computation, launching dynamic attacks. Each of these is a malicious action that relies on brute force, which is provided by the data center.</p>
<p>Vendors have attempted to counter this threat by adding strict registration checks and comprehensively monitoring all network traffic. For instance, a cloud service provider may monitor metadata about all emails originating from it to find out if a customer is misusing it to send spam.</p>
<p><strong>Threat #2: Insecure Interfaces and APIs</strong>(IaaS, PaaS, SaaS)</p>
<p>As mentioned earlier, cloud vendors provide easy to use consoles, dashboards, interfaces and web services to interact with the cloud. However, this brings an additional threat to the entire network if any vulnerability exists in these interfaces. Thus, even if the entire cloud infrastructure is designed securely, a single vulnerability in the provider’s website may allow an attacker to take over a customer’s account.</p>
<p>Most vendors now use strong, multi-factor authentication, detailed logging, anomaly detection and secure defaults to counter this threat. Web interfaces are released to the public only after extremely strict checking for vulnerabilities in the code as well as in the implementation.</p>
<p><strong>Threat #3: Malicious Insiders</strong>(IaaS, PaaS, SaaS)</p>
<p>The threat of malicious insiders is expanded when using public clouds. Unlike an in-house IT deployment, Enterprise employees are now not the only ones with access to the datacenter. Since the service runs on an external machine and stores data on the provider’s resources, it is always possible that a disgruntled or motivated employee of the cloud service provider could do something that adversely impacts the provider’s service.</p>
<p>To counter this threat, cloud service providers enforce strict standards for employees and conduct detailed audits and monitoring. They also contractually define HR and breach notification policies as a part of the service contract.</p>
<p><strong>Threat #4: Shared Technology Issues</strong>(IaaS)</p>
<p>This is one of the fundamental “new” threats due to the cloud paradigm. The cloud works by providing multiple tenants (for e.g., you and your classmates) with virtualized access to shared infrastructure. Isolation between co-tenants is provided by a sandbox known as a “hypervisor”, which mediates access between virtual machines and the underlying infrastructure.</p>
<p>Although co-tenants should be unable to access their neighbors’ details, several exploits over the years have allowed tenants to break out of their sandboxes and steal data from another tenant’s memory, network etc. An example of this could be that as you write code on AWS to solve class projects, a classmate manages to log in to a VM on the same physical machine as you, and then use some properties of the shared physical machine to steal your code. Isolating all users completely is a very hard problem, even with the hypervisors of today.</p>
<p>To mitigate this, cloud service providers add strong monitoring capabilities, using SLAs to ensure timely vulnerability and patch management, and conducting regular audits. Apart from that, hypervisors must be periodically hardened against any potential new attacks.</p>
<p>For e.g. <a href="http://xenbits.xen.org/xsa/" target="_blank" rel="external">the xen security advisory page</a> shows security advisories against the Xen hypervisor. Each time an attack becomes known, AWS must patch all their servers so that an attacker cannot use these exploits.</p>
<p><strong>Threat #5: Data Loss or Leakage</strong>(IaaS, PaaS, SaaS)</p>
<p>This is another drawback of externally hosted clouds. Often, regulations mandate that an enterprise bear legal responsibility for any sensitive data that is used or stored by their applications.</p>
<p>Even if this data is encrypted and stored on the cloud, the key must also be on the cloud to decrypt this data.</p>
<p>New research in encryption technologies has led to the rise of homomorphic and split-key management. Homomorphic keys allow data processing to be carried out on encrypted data. Thus, the key itself does not need to be transferred to the cloud. Split-key solutions work by having a master key (stored securely off-cloud) and several per-application/module slave-keys. As the master is never on the cloud, the threat of data breach is reduced.</p>
<p>Unfortunately, these techniques are still limited and costly to implement. At a contractual level, it is important to define backup, data retention, data wiping, secure key management and storage processes and sufficient auditing privileges. This implies that a certain set of well-designed standards must be followed, such as the <a href="http://csrc.nist.gov/publications/nistpubs/800-144/SP800-144.pdf" target="_blank" rel="external">NIST Guidelines on Security and Privacy in Public Cloud Computing</a>.</p>
<p>It is important to also deal with all jurisdictional issues in the contract. Even though data is stored with the cloud service provider, contracts are framed so that any liability in case of a breach is due to the application owner. Hence, most cloud service providers are required to complete ISO 27001, SAS-70 and other region-relevant audits, which indicate the process maturity and the presence of security controls.</p>
<p><strong>References</strong></p>
<ol>
<li>Anthes, Gary (2010). “Security in the Cloud .” Communications of the ACM. Number 53.11. 16-18 Pages.</li>
<li>Nanavati, Mihir (2014). “Cloud Security: A Gathering Storm .” Communications of the ACM. Number 57.5. 70-79 Pages.</li>
<li>Top Threats Working Group (2013). The Notorious Nine: Cloud Computing Top Threats in 2013 .</li>
</ol>
<h2 id="Cloud_Security_-_Control__26amp_3B_Auditing"><a href="#Cloud_Security_-_Control__26amp_3B_Auditing" class="headerlink" title="Cloud Security - Control &amp; Auditing"></a>Cloud Security - Control &amp; Auditing</h2><p>When running an application on the cloud, different aspects of security must be controlled by different entities. For instance, Figure 1.12 (from AWS) shows the break up of security responsibilities between the provider and the customer.</p>
<p><img src="/images/14529720083304.jpg" alt=""><br>Figure 1.12 Security Responsibilites in AWS.</p>
<p>Many classes of applications require different infrastructure, process and security certifications. Most cloud service providers will comply with a majority of the popular certifications and audit requirements followed in the US and Europe. The following table from Putcher et. al. [3] compares the most popular providers (Figure 1.13):</p>
<p><img src="/images/14529720330687.jpg" alt=""><br>Figure 1.13 Security Responsibilites in AWS.</p>
<p>Understanding the details of these certifications is not a goal for this course, but Mather [1] provides a good reference for those interested in digging deeper into these aspects.</p>
<p>To develop an application that passes these compliance checks, both the cloud service providers as well as application developers must apply a minimal set of security controls, which we will explore below. As with the rest of this course, we look at controls from a predominantly IaaS perspective. Obviously, as we move up the stack, the cloud service provider has to ensure the security of the resources it is responsible for.</p>
<p>For an IaaS cloud, the following table gives an overview of the security controls to be implemented by both parties:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Domain</th>
<th style="text-align:left">Cloud Service Provider’s Responsibility</th>
<th style="text-align:left">Customer Responsibility</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Identity and Access Management</td>
<td style="text-align:left">A cloud service provider must provide information to customers about who is using the service. This requires that they: (1)Deliver and maintain an authentication service (so that users cannot access resources without explicit privileges) (2)Create a service that allows account management policy configuration (this means that customers can add/remove users, roles) (3)Adopt insider misuse protections (monitor employees, restrict access to sensitive server locations)</td>
<td style="text-align:left">Using the authentication and access control service provided by the cloud provider, customers must: (1)Define roles, groups and permissions (2)Create and disseminate credentials (3)Use access control logging (this means that the customer will have a log of all sensitive user events) (4)Use Multifactor Authentication where appropriate</td>
</tr>
<tr>
<td style="text-align:left">Availability and Fault-tolerance</td>
<td style="text-align:left">To ensure that the cloud is resilient to failure, cloud service providers must have: (1)Tape backups and redundancy of storage, compute systems (2)Geo-distributed datacenters</td>
<td style="text-align:left">The redundancy provided by the cloud service provider has to be leveraged by the customer, who should: (1)Add redundant options for connectivity to all endpoints (2)Use application-layer backups and snapshots of instances, storage state (a snapshot of a VM instance, or a database, stores its state at a fixed moment in time, allowing a recovery to be performed from that point)</td>
</tr>
<tr>
<td style="text-align:left">Patching &amp; Configuration Management</td>
<td style="text-align:left">(1)Ensure sandboxing of tenants using hypervisors, overlay networks (this will be explained later) (2)Regular vulnerability assessments and penetration testing (when an internal or external team of “hackers” systematically attempts to break into a system) of bare metal, hypervisor and networks</td>
<td style="text-align:left">(1)Patch OS, machine images with latest security updates (2)Use appropriate user roles with the least privilege for each application (for e.g. when you are running a web server on the cloud, ensure that it does not have access to any infrastructure keys, or even to local “root”. This way, if your website is breached, the rest of your application is isolated) (3)Restrict traffic to instances using firewalls, Virtual Private Clouds, and segment network into zones (block all network traffic from untrusted sources)</td>
</tr>
<tr>
<td style="text-align:left">Monitoring &amp; Detection</td>
<td style="text-align:left">Verify that customer resources are not being used for nefarious activities (either intentionally or unintentionally), and take appropriate actions</td>
<td style="text-align:left">(1)Install host-based Intrusion Detection and Anti-Malware systems (these detect any misuse of your cloud network or hosts) (2)Define alerts and response strategies for incidents and breaches (be prepared for attacks and automate a recovery and logging protocol)</td>
</tr>
<tr>
<td style="text-align:left">Data Security</td>
<td style="text-align:left">(1)Cross-tenant data access controls and privacy safeguards (as described on the previous page, ensure that customers on the same physical infrastructure are isolated) (2)Data integrity verification and repair from redundant data stores (when storing data in several replicas, ensure their consistency and accuracy)</td>
<td style="text-align:left">(1)Use secure protocols (like SSL/TLS, IPSec) for data in transit (these ensure that your network traffic cannot be read) (2)Encrypt data-at-rest (encrypt all the data you store on the cloud, such that even a rogue employee of the cloud service provider cannot disclose this information)</td>
</tr>
<tr>
<td style="text-align:left">Cryptographic Object Security</td>
<td style="text-align:left">(1)Support data encryption in all provided storage/file systems/DBs (for e.g. Windows environments could allow Bitlocker implementations) (2)Securely manage customer’s account and access credentials</td>
<td style="text-align:left">(1)Create and distribute access keys (for cloud service provider APIs) as well as remote connectivity (like SSH, VNC, RDP) (2)Do not store keys on cloud where possible, so that key will not be in the same place as the data.</td>
</tr>
</tbody>
</table>
<p>Most providers will often build in services simplifying the process for customers to implement security controls. For e.g. AWS provides Security Groups, which is simply an external network firewalls.</p>
<p>The process of verifying the presence of these controls is known as a security audit. These can be done internally (by hiring a technical consultant) or externally (by a certifying agency). To host sensitive information on the cloud, both the provider and the customer must pass these audits. However, the lack of demarcation of responsibilities in case of a breach have meant that in most cases, applications using sensitive information like bank records or medical data cannot be hosted “on-the-cloud”.</p>
<p><strong>References</strong></p>
<ol>
<li>Mather, Tim et. al. (2009). Cloud security and privacy: an enterprise perspective on risks and compliance.</li>
<li>Winkler, Vic J.R. (2011). Securing the Cloud: Cloud Computer Security Techniques and Tactics . Elsevier.</li>
<li>Pucher, Alex et. al. (2012). A Survey on Cloud Provider Security Measures .</li>
</ol>
<h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p><strong>Cloud Building Blocks Summary</strong></p>
<ul>
<li>Cloud computing offers many benefits, including a pay-as-you-go economic model, simplified IT management for users, scalability, flexibility, improved utilization, and a decreased carbon footprint.</li>
<li>Cloud computing also has many risks and challenges, including vendor lock-in, security risks, privacy-related concerns, developer training and reengineering, evolving tools, and movement of data.</li>
<li>Cloud computing offers a compelling economic model for businesses through the pay-as-you-go model and can significantly lower management and overall costs of IT.</li>
<li>Cloud service providers leverage economies of scale to provide services at low costs. They require large data centers and many clients in order to amortize the costs over the entire user base.</li>
<li>Service-level objectives (SLOs) allow an organization to formally state its service requirements to a service provider</li>
<li>A Service-level agreement (SLA) is a contract that is negotiated between a service provider and a client that defines the level of service expected from the service provider</li>
<li>Auditing evaluates whether the cloud services comply with the SLA through monitoring. As with auditing in other fields, a mutually trusted third party is involved in the process to ensure compliance and fairness.</li>
<li>The shared and public nature of cloud computing introduces new risks in terms of information security. The primary threats are abuse of clouds, insecure interfaces and APIs, malicious insiders, shared technology issues and data loss or leakage.</li>
<li>The responsibility of securing applications on the cloud is split between the cloud service provider and the client. The provider must make available controls and systems that can be used to secure applications, resources and data on the cloud. The client must make sure that they make full use of the provided controls and systems and follow best security practices when using clouds.</li>
</ul>
<hr>
<h1 id="Data_Center_Trends"><a href="#Data_Center_Trends" class="headerlink" title="Data Center Trends"></a>Data Center Trends</h1><p>In this unit we will present and discuss the data center which is a collection of physical computing resources that are provisioned and shared within the cloud computing paradigm. Innovations in data center efficiency and management are important enablers of the economics behind the cloud computing model. Recall in the previous unit, we discussed some of the benefits of cloud computing as well as the economics behind it. Major advances in data center design play a fundamental role in being able to make this possible. Large-scale data centers can house and run infrastructure more efficiently at scale than what most organizations can manage at small scale with their own infrastructure. Understanding data centers will enable you to be more informed, in terms of performance, cost and potential sources of failure, as you attempt to deploy robust applications and services on the cloud.</p>
<p>This module will serve as a starting step in understanding a few data center fundamentals. We will start with the definition and origins of a data center, followed by a discussion of the current trends in data center technologies: namely the increase in power and resource densities of data centers, as well as the focus on power and efficiency in data centers.</p>
<h2 id="Introduction_to_Data_Centers"><a href="#Introduction_to_Data_Centers" class="headerlink" title="Introduction to Data Centers"></a>Introduction to Data Centers</h2><p><strong>Data Centers</strong></p>
<p>Data centers include a room or building, IT equipment, and facilities to securely house, power, and cool that equipment. Over the years, data centers have evolved from a location of concentrated IT equipment to modular, agile, and highly virtualized compute centers. With growing use of Web-based services, explosion of mobile devices, and ever-increasing rates of data generation (and consumption), the demand for new data centers continues to grow. One of the main contributors to this growth has been the advent of the cloud computing paradigm, in which cost effectiveness is directly linked to economies of scale and the efficiencies gained with new data center design. All layers of cloud software and services run on top of physical resources, largely servers, storage, and networking equipment, and all of these require power. This equipment also generates heat and so requires cooling. A small data center might fit in one specialized room, while a large installation might be a dedicated, warehouse-sized facility (see Video 2.1).</p>
<p><a href="http://youtu.be/ouhskMuknoM" target="_blank" rel="external">Video 2.1: Data centers.</a></p>
<p>Data center design requirements depend on its size and use. Cloud-centric data centers could come in two varieties. An infrastructure as a service (IaaS) cloud provider offers a variety of machine types, and the customers pick and choose to build their own applications. Software as a service (SaaS) and Platform as a service (PaaS) providers typically use large-scale (many thousands) homogenous compute nodes with custom applications that are presented to end users directly. Other types of data centers include enterprise/traditional IT, which houses computers to support functions for day-to-day business operations, and high-performance computing (HPC) data centers, which house large clusters for scientific applications.</p>
<p>In the last 5 years, specific attention has been paid to the efficiency of data centers, dramatically decreasing their operational costs and carbon footprint. This increase in efficiency has led to a fast evolution of data center design, and these trends are likely to continue.</p>
<p>Effective use of cloud resources and development of large-scale, dynamic applications for the cloud require an understanding of the physical resources that make up the cloud. In this unit, we start with trends in data centers, present components that make up a data center, and discuss data center design considerations and requirements.</p>
<p><strong>Why Study Data Centers?</strong></p>
<p>If you think of the cloud as a massive computer, [1] you can still break it down into its constituent parts—processors, memory, and switch. [2] When you are programming for the cloud, you are writing programs that solve a problem or provide a service but with the ability to scale.</p>
<p>A few decades ago, to be a computer user meant to be a programmer. Early programmers knew the instruction set architecture (ISA) well and, because hardware resources were scarce, had to optimize in assembly language. With the advent of high-level languages, such as C/C++, Java, and Python, why do students still have to learn computer organization, caching, and assembly language? When you know what the compiler is doing for you, it makes you a better programmer because you understand what is going on behind the scenes. Similarly, you become better at debugging.</p>
<p>Fast forward to today, when you are developing applications on the cloud, there does not yet exist a compiler that allocates massive virtualized resources automatically to solve your specific problem. It is up to you, the cloud programmer, to do the management and to make your applications cost efficient at scale. Analogous to understanding the components within a single computer, knowing the underlying components of a data center will improve your abilities to program and debug your cloud-based applications.</p>
<p>Most of you will never go on to design and build your own large-scale data center, but understanding what goes into implementing the underlying infrastructure will help you appreciate all of things that cloud providers are doing for you.</p>
<p><strong>References</strong></p>
<ol>
<li>Barroso, Luiz André, and Urs Hölzle (2009). “The datacenter as a computer: An introduction to the design of warehouse-scale machines.” Synthesis Lectures on Computer Architecture.</li>
<li>Gordon Bell and Allen Newell (1970). “The PMS and ISP descriptive systems for computer structures.” Joint Computer Conference.</li>
</ol>
<h2 id="Definition_and_Origins"><a href="#Definition_and_Origins" class="headerlink" title="Definition and Origins"></a>Definition and Origins</h2><p><strong>Defining Data centers</strong></p>
<p>Formally, a data center can be defined as follows:</p>
<blockquote>
<p>Data center(definition)<br>Infrastructure dedicated to housing computer and networking equipment, including power, cooling, and networking.</p>
</blockquote>
<p>The term data center became popular in 1990s, referring to large rooms dedicated to housing computer and networking equipment, though computer rooms themselves date back much further. Early computers (mainframes) were massive—the size of many refrigerators. They also generated a lot of heat and required clean air filtration to increase reliability. For these reasons, early computers could not be placed into a regular office, so custom rooms were built. A lot of these same ideas go into server rooms today. The only difference is that instead of housing one computer, they hold from dozens to hundreds to even tens of thousands of servers in a single facility.</p>
<p>Modularity is important for data centers because it allows an organization to expand as needed. One of the enablers of a modular data center has been standardized racks onto which IT equipment is mounted. Historically, server racks have evolved from early relay racks found in railroad signaling. It is unclear why the railroad companies chose the original 19-inch, post-to-post width, but the same form factor made its way into early telecommunications and then audiovisual equipment at radio and television stations. Figure 2.1 shows early equipment racks in a radio operators room.</p>
<p>Did you know?</p>
<p>The width of early railroad relays dictated the width of a modern 19-in. rack. But the standard gauge of modern railroad tracks (4 ft, 8½ in.) dates back to ancient Greek stone pathways, which the Romans adopted and brought to Europe during the age of the Roman Empire (Wikipedia, 2014).</p>
<p><img src="/images/14536650557456.jpg" alt="Figure 2.1: 1940s radio operators room showing early equipment racks (Inland Marine Radio History Archive, 2012)."></p>
<p>A 1933 U.S. patent, F.C. Lavarack, 1,919,166, is an example of a standardized equipment rack for relays. In Figure 2.2, you can see some of the original drawings in the patent.</p>
<p>Some of the advantages of Lavarack’s design over common predecessors include:</p>
<ul>
<li>Fire safety: Rack posts and other fittings were cast out of iron (and later steel), this became superior to earlier wooden enclosures that could possibly ignite and damage all equipment inside.</li>
<li>Field assembly: Racks could be assembled using common hand tools and low-skill workers.</li>
<li>Regularly spaced holes: Support a wide variety of equipment.</li>
<li>Vertical mounting surface: Easier installation, maintenance, and wiring.</li>
</ul>
<p>Many of today’s standard, 19-inch equipment racks have evolved from Lavarack’s design.</p>
<p><img src="/images/14536650902905.jpg" alt="Figure 2.2: Relay rack patent drawings (figure from Relay Rack patent)."></p>
<h2 id="Size_2C_Density_and_Efficiency_Growth"><a href="#Size_2C_Density_and_Efficiency_Growth" class="headerlink" title="Size, Density and Efficiency Growth"></a>Size, Density and Efficiency Growth</h2><p><strong>Growth of Data Centers</strong></p>
<p>Over the past few decades, data centers have grown both in size (in terms of the number of racks and cabinets) and in density. Figure 2.3 is a view of a data center that is owned by Google.</p>
<p><img src="/images/14536652172800.jpg" alt="Figure 2.3: A view of one of Google&#39;s data centers (Source)."></p>
<p>Greater density has become possible because of advances in CPUs, integrated circuits (ICs), and printed circuit board (PCB) design. This leads to faster and more powerful computers within the same area.</p>
<p>The minimum size of an element on an integrated circuit (called the feature size) has become smaller by orders of magnitude over the last four decades. Individual transistors have reduced in size from about 10 microns in 1971 to about 0.022 microns in 2014. As individual transistors get smaller, more can fit on the same silicon, so each transistor consumes less power.</p>
<p>Within the same thermal constraints, CPUs have gone from single core to 16core, with transistor counts going from millions to billions. Additionally, accelerators/coprocessors have emerged that provide hundreds of additional floating point units (FPUs) each, with increasing floating point operations per second (FLOPs), per Watt. Also, recent storage arrays have gone from supporting three or four mechanical hard drives per rack unit to 15 to 21 drives per rack unit.</p>
<p>Hence, although the power efficiency of individual components has improved over the years, computers themselves have become more dense, packing in more processing cores, memory, and storage per square foot. This density caused the overall power consumption per rack and per square foot to rise dramatically over the last few years (Figure 2.4). This trend means that both power and cooling requirements also increase per data center.</p>
<p>A large data center now consumes several megawatts of power, which is roughly the same power requirements of a small town.</p>
<p><img src="/images/14536652393106.jpg" alt="Figure 2.4: Trends in power density."></p>
<p><strong>Data Center Efficiency</strong></p>
<p>Information and communications technology (ICT) now accounts for approximately 2% of the global carbon footprint. [1] Within that amount, data centers currently account for approximately 15% (or 0.3% of total global emissions). With the proliferation of ICT worldwide, as more people gain access, the energy footprint of ICT is set to grow considerably for the foreseeable future.</p>
<p>Over the past decade or so, there has been an increased focus on “green” IT, or power-efficient computing. Later in this module, we discuss various methods available in the industry to reduce power consumption and carbon footprint.</p>
<p>Efforts in improving power efficiency in IT exist across many parts of the data center:</p>
<ul>
<li>Servers: Entire servers attempt to reduce power consumption by going into idle states, in which they temporarily power down or reduce the power consumption of components when the system is underutilized. For example, a typical server that typically consumes 650W when busy can scale down to about 200W when idle. In addition, virtualization enables better management of IT resources and allows organizations to consolidate individual servers onto fewer physical servers.</li>
<li>Server components: Within individual servers, CPUs and other integrated circuits have gained in performance while maintaining or reducing power consumption. Efforts have also been made to reduce idle power consumption (time periods of low CPU utilization). In addition, dynamic clocking techniques for multicore CPUs enable these CPUs to lower the clock rate of individual cores based on usage and can significantly reduce idle power consumption.</li>
<li>Power: Systems that distribute and manage electrical power, as well as those providing backup supplies, have recently become targets in the drive for efficiency. Where individual data center racks were previously fed from 110 to 220 volts alternating current (VAC), high voltage (277 to 480VAC) is now becoming more popular because that configuration requires fewer step-down transformers.<ul>
<li>The efficiency of a power supply is the ratio of its output power divided by its input power. For instance, a fully loaded, 800W power supply at 80% efficiency would consume 1000W of power, with the remaining 200W lost as heat. These power supplies have gained efficiency recently, with some of them reaching 95%. An alternate design feeds DC to servers directly, instead of converting AC to DC on each rack. Delivering high-voltage, direct current (HVDC) to each server allows using more efficient, DC/DC supplies within the rack.</li>
<li>Large, centralized, uninterruptable power supply (UPS) systems also incur AC/DC conversion losses. To minimize these losses, Google has adopted a decentralized UPS plan, and for several years, its custom-built servers have each had dedicated battery backup. Some manufacturers now offer this server-based UPS configuration for data centers.</li>
</ul>
</li>
<li>Cooling: Significant advances have also been made in the area of cooling. Eventually, all of the electricity used to power the IT equipment turns into heat (and some noise). This heat has to be dissipated away from the equipment. Traditional server room cooling uses the raised floor plenum with computer room air conditioners (CRACs), but this has limited cooling density. Newer approaches, which improve efficiency and capacity, include hot-aisle containment, in-row cooling, liquid cooling to the rack, and even completely submerging the equipment in mineral oil. Evaporative cooling techniques, as part of the facility, are more energy efficient than chillers and compressors. In colder climates, many server rooms are designed to mix colder outside air as well as reclaim the heat generated by servers for use elsewhere in the building.<ul>
<li>The leading driver of increasing power efficiency in the data center is to decrease operating cost. Any power that is drawn from the electric company is billed, but only what is getting to the IT equipment is considered useful, while the rest of the losses eventually become heat. Similarly, the more efficient your cooling system is, the lower your monthly costs.</li>
</ul>
</li>
</ul>
<p>Techniques to calculate and improve power efficiency in data centers are covered in a later module in this Unit.</p>
<p><img src="/images/14536653383069.jpg" alt="Did You Know? A study by Google quantified the power consumption of each hardware subsystem in their servers."></p>
<p>Did you know? Each time you convert from AC to DC, or vice-versa, energy is lost. Similarly, conversions between voltage levels are never 100% efficient.</p>
<p><strong>References</strong></p>
<ol>
<li>GeSI (2008). “SMART 2020: Enabling the low-carbon economy in the information age.” Global e-Sustainability Initiative Report.</li>
</ol>
<h2 id="Challenges_in_Cloud_Data_Centers"><a href="#Challenges_in_Cloud_Data_Centers" class="headerlink" title="Challenges in Cloud Data Centers"></a>Challenges in Cloud Data Centers</h2><p><strong>Challenges and Requirements for Cloud Data Center Design</strong></p>
<p>With the advent of cloud computing, it becomes critical for data center designers to address the cloud’s current and evolving needs. In this paradigm, data centers physically host all the cloud services that are delivered to users. In turn, the cloud services are abstracted from the underlying physical resources on varying scales (over a private IP network [i.e., a private cloud] or over the Internet [i.e., a public cloud]), on demand, and for potentially millions of subscribers. Data center design requirements vary according to use, size, and desired functionalities. The cloud model redefines the way data center assets are designed and consumed. Cloud-based services and scale impose new requirements on data centers whereby traffic flows vary, I/O bandwidth and performance demands are significantly increased, and new security concerns are induced. We describe some of the challenges that cloud computing puts on data centers and identify associated requirements for designing cloud-centric data centers.</p>
<p><strong>Scalability</strong></p>
<p>With cloud computing, there is an ever-growing need for expansion and high capacity. For that sake, cloud data centers are typically designed around virtual machines (or instances), which are the units of computing in the cloud paradigm. In contrast to enterprise data centers, cloud data centers offer services to potentially millions of users. To address increasing user demands for services on the cloud, virtualization techniques are usually adopted. With virtualization, data center operators can automatically provision and deprovision virtual machines (VMs) as required, without adding or reconfiguring physical devices. Today. it is not uncommon to provision 20 or more VMs per rack-mount or blade server. Clearly, this load can greatly stress server’s resources (e.g., CPU, RAM, and network cards). In addition, this can dramatically increase the number of logical servers that operate over the physical data center network. For instance, with a rack of 64 servers and 20 VMs per a server, a cloud provider would require as many as 1200 IP subnets and VLANs (in networking, a LAN can be segmented into different broadcast domains, each referred to as a VLAN). Furthermore, with only 10 racks, 12,000 IP subnets and VLANs will be needed. This demand creates a major problem because it exceeds the limit (i.e., 4094) of usable VLANs specified by IEEE 802.1Q standard, let alone straining physical switches/routers. Cloud data center providers need to find solutions for such problems.</p>
<p>On the other hand, even with maximal use of virtualization techniques, at a point in time it will be necessary to add physical capacity to support growth. As such, cloud data centers need to be based on modular designs in order to support the easy addition of physical capacity without disrupting applications and services. Designers should specify chassis capacity to support long-term growth so that data center operators can include additional components to the chassis as necessary.</p>
<p><strong>Network Topologies</strong></p>
<p><img src="/images/14536654393746.jpg" alt="Figure 2.5: Traditional hierarchical, tree-style data center network topology."></p>
<p>Most of today’s data center networks are based on hierarchical, tree-style designs consisting of three main tiers: an access tier, an aggregation tier, and a core tier. Figure 2.5 shows a sample of a traditional tree-style network topology. First, the access tier is made up of cost-effective Ethernet switches connecting rack servers and IP-based storage devices (typically 10/100Mbps or 1GbE connections). Second, multiple access switches are connected via Ethernet (typically 1/10GbE connections) to a single aggregation switch. Third, a set of aggregation switches are connected to a layer of core switches. Because layer 2 VLANs do not involve IP routing, they are typically implemented across access and aggregation tiers. Conversely, layer 3 routing is implemented at core switches that forward traffic between aggregation switches, to an intranet, and to the Internet. A salient point is that the bandwidth between two servers is dependent on their relative locations in the network topology. For instance, nodes that are on the same rack have higher bandwidth between them as opposed to nodes that are off rack.</p>
<p>Indeed, the network is a key component in cloud data centers. Hierarchical topologies as depicted in Figure 2.5 do not truly suit clouds because they enforce inter-server traffic to traverse multiple switch layers, each adding to latency. Latency in this context refers to the delays incurred by the number of switches traversed, required processing and buffering. Minimal delays in clouds can result in poor user performance perception and loss of productivity. Hence, flatter network topologies with fewer layers to accommodate delay- and volume-intensive traffic are typically required for cloud data centers.</p>
<p><strong>Greater Utilization and Resiliency</strong></p>
<p>Usually, contemporary tree-style data center networks rely on some variant of the Spanning Tree Protocol (STP) for resiliency. STP is a data link management protocol that ensures a loop-free topology when switches/bridges are interconnected via multiple paths. STP allows only one active path across two switches, with the rest being set inactive (assuming many paths are available). On an active path failure, STP automatically selects another available inactive path instead of the failed one. This selection might take STP several seconds, which could turn unsuitable for delay-sensitive cloud applications (e.g., Web conferencing). Furthermore, setting idle backup paths is not the best choice for cloud data centers, especially with the exponential increase in user demands. Cloud data centers require more streamlined and resilient network designs that make full use of network resources and recover from failures in milliseconds to meet demands speedily and utilize resources efficiently.</p>
<p><strong>Secure Multitenant Environment</strong></p>
<p><img src="/images/14536654782240.jpg" alt="Figure 2.6: Workload-to-workload communications in a virtualized environment."></p>
<p>In modern data centers, workloads (e.g., databases, user applications, Web hosting) are typically deployed on distinct physical servers, with workload-to-workload communications occurring over physical connections. Accordingly, securing users can be achieved by conventional network-based intrusion detection/prevention systems. On the other hand, in cloud data centers, multiple VMs can be provisioned on a single rack server, with each belonging to a different user. Thus, workload-to-workload communications can occur within the same server over virtual connections in a manner completely transparent to existing security systems (see Fig. 2.6). Therefore, cloud data centers need to isolate users, protect virtual resources, and secure intra-server communications.</p>
<p><strong>Virtual Machine Mobility</strong></p>
<p>Cloud data centers can host user VMs at servers in one rack, across racks but in the same data center, or at servers across data centers. A cloud can encompass multiple data centers (e.g., Amazon allows users to provision virtualized instances across many data centers). For executing routine maintenance, balancing loads, and tolerating faults, clouds need to periodically and seamlessly migrate VMs between physical servers without impacting user services and applications. This migration does not only require expanding the layer three domain (the domain in which IP routing is required [e.g., WAN]) to move VMs across data centers, but it also requires extending the layer two domain (the domain in which no routing occurs and only broadcasting is employed [e.g., LAN]) in order to span multiple data centers.</p>
<p><strong>Fast and Highly Available Storage Transport</strong></p>
<p>Storage in a cloud data center must support VM mobility and be continually available. VMs that are migrated need to maintain communication with their storage systems. One way to get around this is to move VMs with pertaining storage/data. Clearly, this would require highly available, low-latency, and bandwidth-intensive cloud data center connectivity. Multiple storage models (e.g., Storage over IP [SoIP], Fiber Channel over Ethernet (FCoE), traditional Fiber Channel) are in use today and would further require high-performance and highly available cloud networks. We discuss cloud storage, its challenges, and protocols in the unit on cloud storage.</p>
<p>In summary, data centers tailored for clouds would require the following:</p>
<ul>
<li>Modular designs to support exponential growth and the easy addition of physical capacity without any service disruption.</li>
<li>Flatter network topologies with fewer layers and less equipment and cabling to accommodate delay- and volume-intensive traffic.</li>
<li>More efficient and resilient network designs that make full use of network resources and recover from failures in milliseconds.</li>
<li>Capability to fully isolate cloud users, protect virtual resources, and secure intra-server communications.</li>
<li>VM mobility to execute routine maintenance, achieve load balancing, and tolerate faults seamlessly and speedily.</li>
<li>Twenty-four seven, 365 days a year service availability and ability to keep migratory VMs connected to their storage systems.</li>
</ul>
<p><strong>Addressing Requirements for Cloud Data Centers</strong></p>
<p>Data center designers can address the previously discussed requirements at the infrastructure layer, the virtualization layer, or both. For instance, the scalability requirement needs to be addressed at both layers, whereas secure multitenancy can be mainly addressed at the virtualization layer. We study virtualization in detail in the unit “Resource Sharing and Virtualization.” In this unit, we are concerned with the infrastructure layer. Accordingly, we present only some of the IT devices (e.g., switches, routers), platforms, and protocols that can contribute to satisfying the requirements for cloud data centers. Because this is a cloud computing course, we do not discuss how the devices, protocols, and platforms work but focus on the benefits they bring to cloud data centers.</p>
<p>To start, data center planners can consider Multiprotocol Label Switching (MPLS) to address cloud infrastructure requirements. MPLS is a highly scalable mechanism that directs data from one server to another based on short path labels rather than long network addresses. Specifically, MPLS labels data packets and enables packet forwarding without examining the contents of packets, which makes packet forwarding quite fast because it avoids routing table lookups and solely depends on packet labels. Consequently, MPLS allows creating end-to-end circuits across any type of transport medium, a key requirement for server-to-server communication across cloud data centers.</p>
<p>As previously discussed, in cloud computing, different types of traffic are imposed, and variant bandwidth requirements are induced. Without considering MPLS, separate layer two networks might be necessary to build, which clearly is not a scalable solution in cloud data centers and can greatly increase both capital and operational expenses. By contrast, with MPLS, networks can be shared via creating virtual network connections called label switched paths (LSPs). Furthermore, quality of traffic flows over the LSPs can be flexibly controlled. Such traffic control facilitates end-to-end quality of service (QoS) and provides fast network convergence (approximately 50ms) in case of link failures, a remarkable improvement over STP. As a result, the transparency of network failures can be highly improved, and service disruptions can be reduced, which are other key requirements for greater resiliency on cloud data centers.</p>
<p>MPLS also allows enabling virtual private LAN service (VPLS) to extend layer two connectivity across multiple data centers. VPLS is a virtual private network (VPN) technology. VPN is typically utilized to implement secure connections between LANs located at different sites (i.e., data centers) using public Internet links. VPLS allows different LANs at different data centers to communicate as if they are one LAN, a key requirement for streamlining VM mobility. VM mobility can also benefit from the aforementioned traffic controlling capability offered by MPLS.</p>
<p><strong>Summary</strong></p>
<p>Several vendors are now marketing cloud-centric networking and compute products to address several of the design goals stated earlier. Most of these products are variations on what you would find in a traditional data center, with a shifted focus on density and concurrent users. A data center designer who wants to support private or public clouds has a growing catalog of products from which to choose.</p>
<p>Data center design has evolved rapidly over the last 5 years; this trend is not slowing down. The data centers 5 or 10 years from now will most likely look very different than the data centers of today. Data center designers are addressing new requirements while improving efficiency and the TCO to deliver a cloud service. With the advent of pre-engineered modular data centers, soon the facilities will become interchangeable components, much like the IT equipment itself. To meet the growing consumer demand for new cloud-based services, as well as address IT infrastructure needs of existing enterprises, we will see more and more data centers, with each generation more efficient than its predecessor.</p>
<h2 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>A data center is a term that refers to infrastructure dedicated to housing computer and networking equipment, including power, cooling, and networking.</li>
<li>Data centers have evolved from simple rooms housing mainframe computers to large warehouses that store thousands of individual servers.</li>
<li>As an organization’s IT needs continually increase, modularity in data center components allows an organization to expand its infrastructure as needed.</li>
<li>Servers are getting more and more dense, packing more CPU, RAM, and storage into each square foot. This led to greater power consumption per rack. It is common for existing data centers to run out of power or cooling before running out of floor space.</li>
<li>Industry aims to increase its focus on “green” IT, or power-efficient computing, both inside and outside the server. This efficiency reduces power consumption and carbon footprint, decreasing operating expenses and increasing profit.</li>
<li>Data centers are becoming more efficient in the last few years through advances in virtualization, power-aware server hardware, efficient power distribution, and advanced cooling techniques, among others.</li>
</ul>
<hr>
<h1 id="Data_Center_Components"><a href="#Data_Center_Components" class="headerlink" title="Data Center Components"></a>Data Center Components</h1><p>In this module, we will discuss the various components (equipment and facilities) of a data center. We will start with a discussion on IT equipment, and will clarify many server-specific terminologies that you may have encountered. Most hardware vendors provide servers that are configurable in terms of compute, storage and networking. We will present in detail these server components. While these topics can be information-dense, the choices made while configuring servers have a major impact on the cost and performance of applications that are run on them. Therefore, an exposure to the spectrum of servers and related technologies will be useful, especially if you pursue a career that utilizes or manages cloud infrastructure.</p>
<p>Also, in this module, we will discuss data center facilities. Specifically, we will present some of the recent advances in power and cooling techniques which provide additional efficiency to today’s data centers. We also cover a few additional facilities-specific aspects regarding fire and safety in data centers.</p>
<h2 id="IT_Equipment"><a href="#IT_Equipment" class="headerlink" title="IT Equipment"></a>IT Equipment</h2><p>In this section, we define IT equipment as anything that is mounted in a rack (called rack-mounted equipment). This equipment typically includes servers, dedicated storage arrays, network switches and routers, power distribution, and remote management devices. We are specifically referring to a four-post rack inside a cabinet enclosure. Additional types of racks are described on the next page of this module (“Facilities”).</p>
<p><strong>Servers</strong></p>
<p>A rack-mounted server is similar to a tower PC, except turned horizontally and made to fit into a thinner, deeper chassis (Figure 2.7). The heights are measured in multiples of rack units, where 1U = 1.75 inches (4.45 cm). A 1U server can be CPU and RAM dense but leaves little room for I/O expansion cards (usually two). A server that is 2U or 3U can have six to eight I/O card slots. Smaller chassis must also have smaller fans and therefore make considerable noise compared to your average desktop computer (this is acceptable because most server rooms are not occupied by humans). Systems that are 4U, 5U, or larger chassis usually have a specialized function: one example is an 80-core Intel server, which has CPU sockets and RAM on vertically mounted daughter cards; another is a quad-GPU accelerator server; a third is a server chassis with a 24, 36, or 48 internal hard drives.</p>
<p><img src="/images/14536663372743.jpg" alt="Figure 2.7: (a) Servers and other equipment mounted in a standard 19-inch rack . (b) A 1U server with its top cover removed to reveal internal components "></p>
<p>Rack-mounted servers have their own fans, power supply units (PSUs), network, and I/O, but blade servers share all of these across many nodes within the same blade enclosure (called blade chassis). There is no common blade standard, so each vendor’s blades work only with its enclosures. Blades are thin, vertical metal enclosures and slide into the front of a blade chassis and attach to a common backplane. Each blade has its own motherboard, CPU, RAM, and disk. The shared PSUs are typically more efficient than dedicated rack-mounted versions because they can power up or down incrementally, adjusting PSU capacity to load demand. For example, instead of 10 servers with 2 <em> 750W redundant PSU, a blade enclosure can power the equivalent of 10 servers with 4 </em> 2500W PSU, with one being redundant. Blades are denser than their horizontally mounted counterparts, allow for easier maintenance, and require fewer cables. The disadvantage is higher upfront cost if you only need a few servers, plus you are locked-in to a specific vendor.</p>
<p>Last, there are a few servers that look similar to a standard rack-mounted server, except they have two motherboards horizontally per 1U, each mounted on its own tray. They also share a PSU but, unlike blades, have their own fans and I/O. There are variations on this theme, such as four nodes per 2U or two nodes with multiple GPUs.</p>
<p>Did you know? A 1U rack-mounted server is sometimes referred to as a “pizza box.”</p>
<p>The following video (Video 2.2) describes various server form factors:</p>
<p><a href="http://youtu.be/0EM6jPYafys" target="_blank" rel="external">Video 2.2: Server Form Factors.</a></p>
<p>An important feature found in most rack-mounted servers is hot-swap capability. Components such as PSU, fans, and hard drives can be removed and replaced while the server stays running. This feature increases uptime/reliability on small- and medium-scale deployments. Large-scale application deployments require more sophisticated resiliency to be built into software layers, which is discussed in the next unit. These large-scale systems do not use hot-swap or redundant components for individual servers but instead consider the entire server to be failed (and replaced) as a unit.</p>
<p>The electrical components (e.g., capacitors, voltage regulators) of a server are typically more expensive and longer lasting than those parts used on desktop systems because the servers are designed to run 24/7 for many years on end. A workstation is similar to a server class computer, with similar CPU, high RAM capacity, and added reliability. The difference is that a workstation sits at the user’s desk, so it requires quieter fans. To add to the confusion, there are also rack-mount workstations, which are just like a server, but have remote viewing capability, so the end user sits at a thin client.</p>
<p>Newer servers are now being designed to run reliably at higher ambient room temperatures (up to 95°F, or 35°C), which decreases cooling requirements and therefore lowers operational expense.</p>
<p><strong>On the Motherboard/Mainboard</strong></p>
<p>CPU and memory: A typical server motherboard has more CPU sockets than a desktop system, and each of those sockets can control more DIMMs (dual inline memory modules). Another primary difference of server-class CPUs versus desktop class (Intel Xeon versus i3/i5/i7; AMD Opteron versus FX/A/Phenom) is more onboard cache, support for registered DIMMs, and support for error-correcting code (ECC) RAM.</p>
<p>Server-class CPUs also have dedicated circuitry that allows them to communicate with each other through dedicated channels in the motherboard. For Intel, this is known as QuickPath Interconnect (QPI), and for AMD, it is known as HyperTransport (HT). These follow a nonuniform memory access (NUMA) model, in which processes running on other CPUs (sockets) can access large amounts of RAM by going through QPI or HT to the RAM attached to another CPU. Combining high-density, registered DIMMs (16GB or 32GB), more DIMM slots per server (9 or 16), and multiple sockets with onboard interconnect, a single server can have 512GB or even 1TB of RAM available to a single process (although you get higher performance when you have multiple processes with multiple threads each, but that is a topic for another course).</p>
<p>Onboard management: Although many desktop motherboards now have onboard gigabit Ethernet (GigE) networking, this trend started with servers, and on modern servers, you will find two to four GigE ports. Other onboard devices that a server includes are a serial port for console redirection and an embedded management controller, which allows remote management even if the system is powered off (but still plugged in) or if the OS is not responding (i.e., kernel panic). Many server motherboards have onboard hard drive controllers, but these are also common in the form of an expansion card, which is discussed below.</p>
<p><strong>Expansion cards</strong></p>
<p>PCI Express: Often, a server requires additional I/O devices, depending on the applications intended to run on it. PCs and mainframes have always had some notion of expandability, and expansion buses have evolved from ISA to PCI to PCI-X to what is the current standard—PCI Express (PCIe). The biggest difference between PCI and PCIe is that PCIe is based on point-to-point, high-speed serial links, rather than an actual bus, which has multiple devices attached. Each of these high-speed links constitutes a lane, and multiple lanes work in parallel. So a PCIe device that is x8 has eight of these high speed lanes. Each generation of PCIe, from 1.0 to 3.0, effectively doubles the bandwidth of the previous generation.</p>
<p><img src="/images/14536664462775.jpg" alt="Table 2.1: PCIe generations throughput per x1, x4, x8, and x16 lane slots."></p>
<p>RAID: RAID (redundant array of inexpensive disks) adapters allow multiple hard drives to act as a single logical unit, with increased performance and redundancy or a mixture of both.</p>
<table>
<thead>
<tr>
<th>RAID Level</th>
<th>Name</th>
<th>Advantages</th>
<th>Disadvantage</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Called block-level striping</td>
<td>Improved performance</td>
<td>No fault tolerance, and a single drive failure destroys the entire array</td>
</tr>
<tr>
<td>1</td>
<td>Called mirroring because data is replicated entirely.</td>
<td>Can withstand drive failure. It has faster reads (sometimes), and this level maintains performance on failure.</td>
<td>One-half the capacity versus two independent drives. This level has slower writes.</td>
</tr>
<tr>
<td>10</td>
<td>Called RAID 1+0 or ten (mirroring+striping). It combines the speed of RAID 0 and redundancy of RAID 1.</td>
<td>Provides fault tolerance. It has improved performance. This level maintains performance on failure</td>
<td>One-half the capacity versus two independent RAID 0 arrays.</td>
</tr>
<tr>
<td>5</td>
<td>Block-level striping with distributed parity.</td>
<td>This level can withstands single drive failure, and it has more capacity versus RAID 1, 10, and 6.</td>
<td>Slower writes than RAID 0, 1, and 10, degraded performance on failure, and slow rebuild process.</td>
</tr>
<tr>
<td>6</td>
<td>Block-level striping with double-distributed parity.</td>
<td>Withstands two drive failures and has more capacity versus RAID 1 and 10 (with more than four drives).</td>
<td>Slower writes than RAID 5, degraded performance on failure, and slow rebuild process.</td>
</tr>
</tbody>
</table>
<p>Table 2.2: Different RAID levels with major advantages and disadvantages.</p>
<p>RAID controllers come with different number of ports (connected drives), support different interfaces (SATA, SAS), vary in amount of built-in cache (e.g., 512MB, 1GB), and have varying performance levels (MB/s throughput, I/O per second). Each of those factors can change the price of the adapter (from a few hundred to over $1000).</p>
<p>A host bus adapter (HBA) is similar to a RAID controller, except that it connects to external storage (discussed in the next section) and does not typically include any RAID features on the card itself. The card will encapsulate all communication, so the remote disk device will look like a local disk to the OS and is bootable.</p>
<p>Networking support: Ethernet network adapters can include single or multiport cards, with speeds ranging from 100Mbps (obsolete) to 1Gbps (common, inexpensive), then came 10Gbps (becoming more prevalent but still expensive), and now 40Gbps (available but not common yet).</p>
<p>A host channel adapter (HCA) refers to a high-speed, low-latency interconnect adapter, such as Infiniband. These are typically used on high-performance computing (HPC) clusters.</p>
<p>Solid-state storage cards are now available, such as those from Fusion-IO. They provide much faster performance than RAID controllers, especially on random I/O. They have a very high cost per gigabyte compared to hard disk drives (HDDs) or even solid-state drives (SSDs) (see the following “Storage” section).</p>
<p>Accelerators: Video cards, started in desktop computers, use GPUs for fast 3D rendering. These GPUs can also be used for scientific computation, so they made their way into HPC server markets, and now nVidia Tesla series accelerators can be found in more than 50 of the top 500 supercomputers list. Recently, Intel released an accelerator product, the Xeon Phi, which is based on Many Integrated Cores (MIC) architecture and is similar in performance to Tesla product (&gt;1 teraFLOPs). These cost approximately $3000 but can increase performance 2.5x to 10x (often) to 100x (atypical) for math-intensive applications. Note, however, that these will not fit into your average server case due to the large size of the card and additional PSU requirements.</p>
<p><img src="/images/14536667410289.jpg" alt=""></p>
<p>Server Configuration</p>
<p>Company X wants to expand its private cloud for hosting a new Web application composed of many Linux virtual machines running in a VMware ESXi 5 environment. They do not want to use more than one rack of equipment, so they like the density that blade servers provide. They have benchmarked their application and found that a blade server with 2 x Intel Xeon E5-2660 performs 40% faster (i.e., 1.4x) than a blade server that has 2 x AMD Opteron 6276 CPUs. They use a fast QDR Infiniband network and fast enterprise-class SSD drives in each blade server, so the only bottleneck for the applications is the CPU.</p>
<p>Each blade enclosure/chassis is 7U tall and can house 20 blade servers (each blade is a two-socket blade). The enclosure itself costs $12,200 and includes power supplies, fans, gigabit Ethernet, and Infiniband network switch modules. These particular blades must be bought in pairs because of the way they stack in the enclosure.</p>
<p>The price of the Opteron 6276 processor is <code>$850</code> (per socket). The price of the Xeon E5-2660 processor is <code>$1370</code> (per socket).</p>
<p><img src="/images/14536669237218.jpg" alt=""></p>
<p>Configured to Company X’s requirements, the AMD blade servers cost $4875 each, and the Intel blade servers cost $6700 each. The company intends to configure the enclosures with the exact number of blade servers as determined in the scalability testing mentioned above.</p>
<p><img src="/images/14536670010535.jpg" alt=""></p>
<p><strong>Storage</strong></p>
<p>When most people hear the word storage, it is likely that they will envision an HDD (Figure 2.8(a)). Inside an HDD, the rotating platters and moving read/write head have been similar for decades. What has evolved are higher areal density, new recording techniques, faster interfaces, and overall lower power. The two most common form factors of HDDs are 3.5 inches and 2.5 inches. Rotational speeds can vary, but common RPM values are 5400, 5900, 7200, 10,000, and 15,000. The higher the RPM, the lower the latency between random seeks.</p>
<p><img src="/images/14536670221645.jpg" alt="Figure 2.8: (a) Internal view of a traditional hard disk showing the rotating platters and read/write head. (b) SSD with the circuitry exposed."></p>
<p>Did you know? The maximum physical capacity of a hard drive is governed by what is called the superparamagnetic limit.</p>
<p>SSDs (Figure 2.8(b)) are becoming more affordable but still have a much higher cost per gigabyte than their mechanical counterparts. SSDs are completely electronic; there are no moving parts, which gives lower seek times, higher performance, and higher reliability. SSDs do have a limited number of writes, but one would have to continuously write to the drive 24 hours a day for several years to reach that limit. The terms SLC, MLC, and TLC refer to how many voltage levels there are in each cell: S is for single (1), M is for multi (2), and T is for triple (3). The fewer the levels, the more reliable it is (SLC &gt; MLC &gt; TLC). The more levels, the higher the density (TLC &gt; MLC &gt; SLC). SLC is generally reserved for high-throughput transactional servers. MLC is the most common replacement for desktop and laptop drives, plus servers that need higher I/O performance than a fast (10k or 15k rpm) HDD can provide. TLC is emerging as a lower cost alternative to MLC, but somewhat compromises performance and reliability to achieve a lower price.</p>
<p>The hard drives that are found in data centers are nearly identical to those found in desktop PCs. The primary difference between an enterprise and a regular SATA drive is added antivibration protection, which claims to increase the lifespan of drives that are rack mounted with many other drives in the same chassis. They also come with longer warranties—5 years instead of 3.</p>
<p>SATA (serial ATA) and SAS (serial attached SCSI) are the two predominant interfaces for all modern HDDs and most SSDs. For servers, SAS provides the ability for a single drive to “talk” to two drive controllers (in case one controller fails). SAS also requires lower CPU usage and provides the ability to daisy chain a large number of drives via an SAS expander. Most RAID controllers that support SAS also support SATA, but the inverse is not true.</p>
<p>Direct attached storage (DAS) refers to a rack mount chassis that holds additional drives, along with PSUs and fans, but minimal control logic. All of the work is done by the controller (on the server) that these units attach to. A typical connection is made through external SAS cable (high-speed serial, multilane, copper).</p>
<p>A storage area network (SAN) is a collection of hard drives in a chassis that can be connected to many servers. A system admisinstrator will carve the array into LUNs (storage logical unit numbers), where each LUN can be used for a different purpose. Many servers connect to the same SAN simultaneously, but only one server accesses a given LUN at a time (unless a coordinated sharing software is installed, such as VMware). Servers connect to the SAN through a network, typically Fibre-Channel or iSCSI over Ethernet (1Gb or 10Gb). Each LUN gets a unique World Wide Name (WWN), which can be used for zoning and masking, providing security so that servers can only read the LUNs that belong to them. Access can also be restricted by IP and subnet.</p>
<p>Most commercial SAN products span multiple rack units or even multiple racks, and have multiple redundant controllers and network paths to increase uptime. SANs have embedded RAID controllers, so disk failures are transparent to the attached servers. SAN products vary greatly and, depending on features, performance, and capacity, can range from $5000 to several million US dollars.</p>
<p>Network attached storage (NAS) is a device or server that shares a file system to multiple users over the network. Any Linux server can become a NAS, and many consumer dedicated NAS devices run Linux underneath. The two most common protocols used by a NAS are called CIFS (Common Internet File System, also called SMB [Server Message Block]) and NFS (Network File System). CIFS comes from Microsoft Windows’ heritage and is a per-user connection in which NFS is primarily used on UNIX/Linux and is a per-host connection.</p>
<p>Last, there are also many Distributed Parallel File Systems, which are typically used in compute clusters and implement a many-to-many scheme. These provide a much higher combined throughput than any single SAN or NAS and can scale in both performance and capacity.</p>
<p>The choice on which type of storage to use is largely dependent on the application, and most organizations will use a combination of the above. For example, a large storage array uses a parallel distributed file system on its internal back-end network but presents both SAN-style LUNs and provides NAS capability for other clients. A Windows file server will look like a NAS to a client mapping a network drive, but it is likely that the storage device in which files actually reside are in a SAN, which is outside of the Windows server itself. When you learn about resource sharing in the next unit, some of those features (live migration, fault tolerance) require a SAN storage device.</p>
<p><img src="/images/14536670862076.jpg" alt=""></p>
<p><img src="/images/14536671077200.jpg" alt=""></p>
<p><img src="/images/14536671199042.jpg" alt=""></p>
<p><strong>Networking</strong></p>
<p>In addition to servers, a data center houses all of the network equipment that is used to connect those servers to each other and to the outside world.</p>
<p>One popular design is the multitier topology (Figure 2.9), in which servers connect directly to switches, and those switches link to aggregation switches, which, in turn, connect to the rest of the organization’s network (i.e., core switches) and eventually to the upstream ISP.</p>
<p><img src="/images/14536671423623.jpg" alt="Figure 2.9: A stack of network switches mounted in a rack"></p>
<p>A fatter (higher bandwidth) connection typically links the bottom-tier and aggregation switches. For example, with 48x1Gb ports connecting the rack servers and the access tier, the access to the aggregation tier might have 2 * 10Gb uplinks.</p>
<p>Top of rack (TOR) refers to a configuration in which the lowest tier of network switches are found in each rack. This design can decrease cabling cost and complexity because the wires are shorter and can be copper even at 10Gb speeds. This is especially useful if most of the network traffic is to other servers in the same rack. Apache Hadoop, for example, can take advantage of this topology for choosing which nodes to store data (on rack and off rack) and which nodes from which to retrieve data (on rack). One disadvantage includes limited scalability. Ethernet switches are blocking, which means that if all the ports are busy at the same time, the backplane cannot handle all the traffic, so some requests are postponed. Also, the uplinks that are available are typically less than the full possible bandwidth of the Ethernet switch (e.g., you cannot have a 48Gbps uplink on a 48x1Gb switch). In most real-world scenarios, however, those uplinks are never saturated because not every server is using the full capacity simultaneously. Another disadvantage is port utilization. Switches come with a specific number of ports, typically 24 or 48, so if you have a number of servers in between that, you will have some ports in each rack that are never used.</p>
<p>End of row (EOR) refers to a configuration in which you have a row of adjacent racks, or a row of back-to-back racks,sometimes called a bay, and there is one additional rack dedicated for network switches, and all of the hosts in that row (or bay) are wired to that dedicated rack. Wiring is either done above the racks in a wiring tray or below the racks, underneath a raised floor (more on that in the next section). The switches in this dedicated rack can be a larger (10U+) modular chassis, in which each module communicates to other modules through a fast backplane. The advantage is that all servers in the same row (or bay) can communicate with each other at the same speed (and lower latency), so there are fewer bottlenecks. The disadvantage is higher upfront cost and more complex cabling (harder to debug if there is a problem). Gigabit links can still be copper at this distance, but anything 10Gbps or above should be fiber.</p>
<p>Ethernet is by far the most popular type of network in data centers (and elsewhere in organizations). Although its rise in popularity started in the 1980s with 10Mbps, both that and 100Mbps products are obsolete because of the low price and ubiquity of 1Gbps products. The primary driver for Ethernet popularity is the low cost and ease of installation of twisted pair wiring. The most common type of Ethernet wiring is unshielded twisted pair (UTP) (Figure 2.10(a)), which has four pairs of thin (28awg) copper wires and comes in different categories: category 5 (Cat5) is good for 10/100Mbps, Cat5e and Cat6 are good for 1Gbps, and Cat7 can go up to 10Gbps. Each generation has slight design changes over the previous, which allow for greater bandwidth and/or longer runs. Alternatively, shielded twisted pair (STP) has an extra metal shielding on the outside for use in electrically noisy environments (such as a factory or near high power lines). The shielding reduces interference but is generally not needed for most in-rack installations.</p>
<p>Ethernet can also travel over fiber (sometimes fibre) optic cable (Figure 2.10(b)). Fiber is unidirectional, so in a cable, you will see two strands, one send and one receive. The electrical signals are converted to photons, which stay inside the fiber due to the differences in indexes of refraction between the core (thin center) and cladding (thicker outside layer). The cladding is covered by a layer of fireproofing material. Fiber cables are thinner and lighter than copper cables, but care has to be taken in routing to maintain a minimum bend radius, otherwise some photons escape causing loss. Multimode fiber is most commonly used inside data centers because the cables and transceivers are much less expensive than single-mode fiber. Single-mode fiber is generally reserved for long-distance (&gt;100m to 40km) connections. Higher speed Ethernet of 40Gbps is actually 4x10Gbps links that work in unison (similar to PCIe lanes); similarly, 100Gbps is 10x10Gbps (copper, fiber) or 4x25Gbps (fiber only).</p>
<p><img src="/images/14536671655060.jpg" alt="Figure 2.10: (a) A cutout of an unshielded twisted pair (UTP) cable, showing four pairs of cables. (b) Fiber-optic network cables with LC (top) and ST (bottom) connectors."></p>
<p>Fibre Channel (FC) is a protocol designed to support SANs and remote block-level storage devices. Even though it typically runs over fiber, it can also run over copper. It can also be encapsulated in Ethernet in the form of Fibre Channel over Ethernet (FCoE) but is not routable like iSCSI. Fibre Channel HCA and switch generations are named by their speed (in Gbps)—#GFC, in which # is 1, 2, 4, 8, 10, or 16. The most common now is 8GFC, which supports 8Gbps links (approximately 800MB/s in each direction).</p>
<p>iSCSI, or internet SCSI (pronounced “scuzzy”), is a protocol which packetizes SCSI (Small Computer Systems Interface, an old hard drive standard) commands to be transported over LANs or WANs. Many SANs support iSCSI, and it is popular because you can use your existing Ethernet network instead of deploying a dedicated FC network.</p>
<p>Infiniband (IB) has been popular in HPC clusters for almost a decade and is also gaining traction in other areas in the data center. The main advantage of Infiniband is much lower end-to-end latency compared to Ethernet (1.7 microseconds versus 12.5 microseconds for 10GigE versus 30 to 100 microseconds for GigE). It also scales better with the number of nodes in a cluster. The other key factor in IB performance is that the switches (Figure 2.11) are fully nonblocking, which means that the backplane supports running every single port at full speed, in both directions. Infiniband speeds are summarized by the chart below:</p>
<p><img src="/images/14536671966270.jpg" alt="Table 2.3: Summary of Infiniband speeds."></p>
<p>Infiniband and 40Gbit Ethernet are also being deployed as a converged network, or virtual fabric, which, along with virtualization of the OS, allows many virtual adapters (Ethernet, FC-HBAs) to run over fewer high-speed links. The connections can be configured dynamically through software and allow for fewer cables and easier maintenance and management.</p>
<p><img src="/images/14536672169882.jpg" alt="Figure 2.11: An infiniband switch"></p>
<p>The choice of network technology and topology to deploy in a data center also depends on your applications but will typically include multiple tiers, with the connection between the tiers being high-speed fiber. Copper UTP cabling is prevalent within the rack and will continue to function well for low-speed management networks.</p>
<p><img src="/images/14536673271675.jpg" alt=""></p>
<p><img src="/images/14536673906970.jpg" alt=""></p>
<p><img src="/images/14536674222579.jpg" alt=""></p>
<h2 id="Infrastructure_and_Facilities"><a href="#Infrastructure_and_Facilities" class="headerlink" title="Infrastructure and Facilities"></a>Infrastructure and Facilities</h2><p><strong>Data Center Facilities</strong></p>
<p>A data center’s functional units (servers, storage, networking) all rely on a facility’s infrastructure, which includes physical space, power, cooling, and safety. In assembling each of the latter component systems, designers prioritize redundancy issues. Redundant power sources, for example, minimize the risk of service outages should the building lose main power. Redundant cooling avoids physical damage to IT equipment during an unplanned outage and enables planned outages for HVAC equipment maintenance.</p>
<p><strong>Server Room</strong></p>
<p>A server room can vary in size from a single rack in a closet to several hundred square feet to a warehouse the size of a football field. Some authors use the phrases server room and data center interchangeably. For the purpose of the course, we define a server room as the actual room that houses all of the racks full of IT equipment, and a data center as the server room plus all of the power and HVAC equipment that may be located outside of that room.</p>
<p><img src="/images/14536674954350.jpg" alt="Figure 2.12: A 42U, four-post rack/cabinet, with sides and doors removed."></p>
<p>You were already introduced to the concept of a rack and a rack unit (U). Figure 2.12 shows a rack inside an open cabinet. Sometimes racks are also called cabinets. There are several variations of racks, but the most common are 19 inches wide (measured from the center of each hole on the same U). Some IBM equipment racks measure 24 inches post to post, inherited from older mainframes. Usually, network equipment is designed to mount only on two posts of the rack because wiring closets often have only two posts permanently mounted to the floor and/or wall. Servers, however, are designed to mount on four-post racks. The depth of the rear two posts is not standardized, and in most racks you can adjust the depth. Different rack-mount equipment has different depths, and each server or storage array will come with two mounting rails that connect to the front and rear posts on the left and right sides. There are two types of holes in vertical posts, square and round. Some mounting rails hook directly into square holes and have tool-less installation (fast). For round-hole racks, equipment is directly screwed in (these are more popular for telecom and A/V racks). If you need to mount round-hole style equipment or rails into a square-hole rack, you use cage nuts and bolts.</p>
<p>The most common height rack is 42U, and that is simply to fit through a normal doorway. The overall height, width, and depth of racks by different manufacturers are not exactly the same; the only guarantee is the post widths. Some racks have extended depth, which is useful for larger servers and/or routing cables and mounting zero-U (vertical mount outside of the 42U space) equipment. “Wide” racks have extra space to the left and right of the posts, which is useful for end-of-row networking racks because of the added space to run many cables. Racks also have casters that allow them to be rolled into place or moved if needed. However, these wheels are not meant to permanently support the full weight of a filled rack, which is why you have to screw down the four stabilization feet at each corner. Regions that are prone to earthquakes also have safety regulations that require racks to be bolted into concrete through metal plates on the front and rear of the rack.</p>
<p>Many server rooms have a raised floor, although it is not a strict requirement. This provides a plenum for cold air to be distributed throughout the room (tiles in front of racks have vent holes). Raised floors also provide space to run electrical or network cable or chilled water pipes for in-row cooling. Last, they provide more flexibility for future layout/configuration changes. Figure 2.13 shows what a raised floor looks like.</p>
<p><img src="/images/14536675592221.jpg" alt="Figure 2.13: An example of a raised floor."></p>
<p>The floor consists of an array of metal support pedestals, which are mounted to the subfloor; metal stringers that are placed horizontally between pedestals; and strong floor tiles that rest atop the stringers at each corner at a pedestal. When a tile is removed, the holes are large enough for a human to fit through, and the tiles are strong enough to withstand the weight of a filled rack. Tiles do have a rollover rating, however, and for safety reasons, any tile that has been rolled over (moving a filled rack on top of it) more times than it is rated for should be replaced. Also for safety reasons, on a stringerless floor, no more than two or three consecutive tiles should ever be lifted from the floor simultaneously. If there is a problem with one of the pedestals, weight can shift laterally, cascading to a catastrophic floor failure.</p>
<p>Above the racks are cable trays that run horizontally between racks. There are options to hang these from the ceiling, or some racks have optional trays that mount on top. For electrical safety, these are required to be properly earth grounded, even if they are carrying only network cable. When there are a many wires to run between two racks, hook-and-loop (Velcro) fasteners are most often used to bundle the wires together.</p>
<p>Most data centers implement strict physical security procedures—for good reason. If someone with bad intentions had physical access to a server, they could, for example, gain administrative privileges, steal data, eavesdrop network connections, and install viruses/Trojans. Common practice includes keycard/pin access and/or biometric scanner, full time security guard, cameras, and break-in detectors. In shared data centers with multiple tenants, one technique is to have a perimeter chain-link fence with a padlock around each customer’s set of racks. Watch Google’s security practices <a href="http://youtu.be/cLory3qLoY8" target="_blank" rel="external">here</a>.</p>
<p><img src="/images/14536676574968.jpg" alt=""></p>
<p><strong>Power</strong></p>
<p>The following video (Video 2.3) discusses various power distribution methods in data centers:</p>
<p><a href="http://youtu.be/Xl4VjEqitSk" target="_blank" rel="external">Video 2.3: Data center power distribution methods.</a></p>
<p>Reliability/uptime is often the number-one design consideration for a data center. Unfortunately, the power feeding the data center is not 100% reliable because of events such as bad weather conditions and downed power lines. In some locations, it is possible to get feeds from multiple electrical utility suppliers, but often this is not available. To keep the IT equipment powered on during a power outage, a generator can be installed. Backup generators come in two varieties, powered either by diesel fuel or natural gas. They could power the data center indefinitely as long as fuel is available, but both fuel sources are significantly more expensive than electricity from the grid. Generators are typically mounted outdoors due to fumes, noise, weight, and vibration. An automatic or universal transfer switch is a device that can choose a working power source (utility 1, utility 2, or generator) and connect it to the main power input to the data center.</p>
<p>Generators have a 15- to 60-second start-up time, so this is where an uninterruptable power supply (UPS) can provide power to the IT equipment until the utility power is restored or the generator is running. UPS have many lead-acid batteries (like in a car) strung in series. For example, a 480-volt UPS would have a string of forty (40) 12-volt batteries. A UPS also act as a line conditioner and will switch to a DC battery source if it detects poor AC conditions, such as surges, sags, overvoltage, undervoltage (brown out), and variations in wave shape or frequency.</p>
<p><img src="/images/14536677081141.jpg" alt="Figure 2.14: Diagrams of the (a)C13 power connector and (b)C19 power connector "></p>
<p>Between the UPS and the IT equipment, there are power distribution units (PDUs). Think of PDUs as similar to power strips you use at home but designed for higher voltages and amps, with more outlets and built-in circuit breakers. They often include monitoring features, so you can remotely see the power draw per branch (group of outlets connected to a single breaker). Some also include per-outlet power sensing (POPS) as well as remote ON/OFF switching for each outlet. The outlets for PDUs do not look like the electrical outlets in your house; instead, they are IEC 60320 C13 (said “C thirteen”) (Figure 2.14(a)) for 10- to 12-amp rating and C19 for 16 to 20 amps (Figure 2.14(b)).</p>
<p>For AC, higher voltage (400V and 480V) is more efficient for distributing power throughout a data center than 240V or 208V but still has to be stepped down before going to the actual server. Most server power supplies are universal and will accept input AC voltages ranging from 110V to 240V. The benefits to running at 208 to 240V versus running at 110 to 125V are slightly higher efficiency (5% to 10%) as well as getting the full rated power output (as labeled on the PSU). Most server room/data center installations will run at 200+ VAC for the efficiency, as well as lower pricing for electrical wiring (smaller gauge copper). In order to boost efficiency, some server PSUs also support 277V directly. Instead of traditional wire, some server rooms employ bus bars that mount over head (above the racks) and have circuit breaker whips that can attach at any horizontal location (these are like track lighting, only larger).</p>
<p>Figure 2.14: Diagrams of the (a)C13 power connector and (b)C19 power connector (Source).<br>Some vendors offer DC distribution, in which the AC-to-DC conversion is done per rack, per row, or per bay, rather than converting AC to DC within every server power supply. These systems have been measured to be more efficient than their AC counterparts, but only 2% to 4% for average loads. [1] Because DC power supplies are not a commodity, these are only suited for large-scale deployments with custom components.</p>
<p><img src="/images/14536677510030.jpg" alt=""></p>
<p><img src="/images/14536677649442.jpg" alt=""></p>
<p><img src="/images/14536678084746.jpg" alt=""></p>
<p><strong>Cooling</strong></p>
<p>Many of the advances in data center efficiency over the last 10 years have come from new designs and methods for cooling.</p>
<p><img src="/images/14536678293190.jpg" alt="Figure 2.15: Typical data center cooling technique. Figure shows the use of a CRAC and raised floor."></p>
<p>Commonly found in traditional server rooms are computer room air conditioners (CRAC or CAC). These continuously take in hot air and output cold air into the space under a raised floor or into ducts. The difference between a CRAC and a regular air conditioner is that CRACs also provide humidity control. Keeping a relative humidity around 40% is recommended. If the air is too wet, you get condensation (bad for electronics and anything metal), or if it is too dry, you get a higher risk of ESD, or electrostatic discharge (also harmful to electronics). The units’ fans have to be sized large enough to create positive pressure and airflow for the volume of the room and have sufficient cooling capacity to maintain the desired “cold-aisle” air temperature (more on hot and cold aisles below and in the next module). The CRACs remove heat through the use of a condenser (similar to what is in your refrigerator at home) or through a heat exchanger that uses chilled water supplied by chillers elsewhere on site.</p>
<p>Measurement of energy for electronics is usually in kW, but most HVAC equipment is measured in tons or BTU/h, so here are some conversions:</p>
<ul>
<li>1 kW = 3412 Btu/h</li>
<li>1 ton = 12,000 Btu/h</li>
</ul>
<p>Did you know? A BTU, or British thermal unit, is the amount of energy needed to heat 1 pound of water to 1°F, and a ton is the heat absorbed by melting 1 ton of ice in 24 hours.</p>
<p>Using vapor compression, chillers remove heat from water in a closed-loop, high-pressure system, usually outputting water that is approximately 42°F (5.5°C). Chillers themselves need to dissipate the heat they remove from the water, which can be through air cooling (fans) or water cooling (needs another water source and/or cooling tower). Chillers are sized based on water temperatures (entering and leaving) and flow rate (gallons per minute). The main sources of energy consumption in a chiller are the electric motors in the compressor and pump(s).</p>
<p>To reduce the load on these chillers, evaporative cooling techniques are now frequently deployed for large data center installations. When hot dry air passes over water, some of the water evaporates, absorbing energy and cooling the air. If you want to use evaporative cooling, it is a good idea to locate a new data center near an abundant water source.</p>
<p>The overall system can be made more efficient if it does not have to cool as much air. Air-side economization is a method of using or mixing outside air when it is cooler than the recirculated air. This method is cost effective in cold climates but not as useful in hot and humid regions.</p>
<p>When rack densities increase to 10kW or higher, it is helpful to move the cooling equipment closer to the rack. A product category that allows this is in-row cooling. This way the cold air can flow directly into the front of the IT equipment, and the hot exhaust air from the rear of the rack goes directly into the adjacent air conditioner. This method puts a focus back on cooling the racks, instead of cooling the room. Similar to in-row cooling, there are top-of-rack cooling systems. These systems are bolted to the top of each rack and provide localized cooling on a per-rack basis. The advantage of top of rack is you are not taking up floor space in the server room, while the disadvantage is higher difficulty for installation and maintenance. Smaller capacity, in-row systems use a compressor, whereas higher capacity models use chilled-water or external gas refrigerant. Top-of-rack systems typically use external refrigerant. The advantage of using a gas refrigerant is that there is no chance of water leaking near the IT equipment, but the disadvantage is the added cost of additional equipment in the server room, which removes the heat from the refrigerant loop (using chilled water). Both in-row and top-of-aisle systems offer a modular cooling approach. As long as the facility’s chilled-water plant has enough capacity, you can add coolers only when you add new IT equipment, thus staggering your capital expenditures.</p>
<p>Hot-aisle containment refers to a method of placing your racks into rows/bays such that adjacent rows face away from each other (i.e., cold-hot-cold) and then completely enclosing the hot aisle. This arrangement prevents hot and cold air mixing before it recirculates back through the air conditioner, which greatly improves efficiency. Some server room designs employ the opposite, cold-aisle containment, in which the room itself is hot, but the separate cold air is fed into the contained fronts of racks.</p>
<p>Although it has been mainstream amongst overclocking enthusiasts for years, water-cooling products (or glycol or other liquid) are also becoming available from several vendors. There are two approaches: one is to have specialized rack doors that are in essence huge heatsinks, with cold water fed in and hot water returned; the second is to have cold-in and hot-out water hoses going to every server in the rack, and inside each server are specialized heatsinks for the CPU (and GPU) that the water circulates through. In both cases, the servers still require fans to cool the other components (e.g., RAM, hard drives).</p>
<p>One vendor even offers an extreme liquid cooling technique. The (sealed) rack is turned sideways and filled with mineral oil (nonconductive dielectric), and the servers are fully submersed vertically. The fans are removed, and the hard drives have to be sealed (or use SSD). It is best to use servers with front-facing I/O ports.</p>
<p>Many modern buildings’ HVAC systems are designed to reclaim heat that is produced in the server room and use it elsewhere, such as hot water or heating (in cold climates), thus reducing overall energy costs.</p>
<p><img src="/images/14536678975205.jpg" alt=""></p>
<p><img src="/images/14536679199064.jpg" alt=""></p>
<p><strong>Safety</strong></p>
<p>In addition to the safety notes mentioned earlier, there are some features of a data center that are safety specific.</p>
<p><strong>Fire Suppression</strong></p>
<p>The preferred system for putting out fires in a server room is to use a clean agent. These agents are stored and transported under high pressure so that it is a liquid and takes up less space. When activated, they are a gas that comes out of misting nozzles in the ceiling. The “clean” term means they do not leave residue or require cleanup as do handheld fire extinguishers (dry chemical) or water sprinkler systems.</p>
<p>Halon was the most popular fire suppressant in this category, but it is a CFC (greenhouse gas), and manufacturing of Halons was banned in 1994. The old systems still exist (must use recycled Halon) but cannot be used in new installations.</p>
<p>A popular clean agent of today is DuPont’s FM-200 (CF3CHFCF3), which is nontoxic, and with a properly designed system, the gas will fill the room and extinguish all fires within 10 seconds (hint: do not leave the doors open). It is safe for humans to breath but can create fumes when it reacts with fire. Standard practice is to leave the room (sealed) for 10 minutes to ensure all fire is out.</p>
<p>Another method of fire suppression is the use of inert gases, such as CO2. These gases work by reducing the ratio of oxygen in the air. The problem with these systems is that they are dangerous to humans and also not as effective (depending on type of fire).</p>
<p>Traditional sprinkler systems use a large amount of water to decrease combustibility of everything in the room. They are not as effective for electrical fires, damage electronics, and require extensive cleanup. Sometimes they are required to be in every room by the municipality, so you might still find sprinklers alongside an FM-200 system. With a wet-pipe system, water is already in the sprinkler pipes, and heat from the fire melts the caps and releases the water. A more appropriate dry-pipe system has normally empty pipes, and smoke detectors will electronically trigger a preaction valve to fill the pipes (but the caps still have to melt before water comes out). The main purpose of sprinklers is to protect the building from collapse, not to protect the electronics in the room.</p>
<p>No matter what system is in place, modern facilities have electronic sensors throughout for monitoring and alerting building engineers, security, the fire department, and other pertinent parties in an automated fashion.</p>
<p><strong>OSHA Compliance</strong></p>
<p>Occupational Safety and Health Administration (OSHA) is a governmental entity (under the U.S. Department of Labor) that is tasked with providing regulations to maintain a safe workplace environment. Some of the rules you might find in a data center are not as common elsewhere.</p>
<p>Noise is becoming more of a problem, not just from fans on the IT equipment, but also on the HVAC systems. To maintain safe volumes, ear plugs or ear muffs are recommended for all personnel while inside the server room.</p>
<p>Procedures for removing floor tiles on raised floors should include cones or temporary barriers so that someone does not accidentally fall through an open hole. If the subfloor is deep or is superficial, workers should be harnessed, and the tiles should be tethered before removal.</p>
<p>Electrical safety is relevant because of the large amount of high-voltage circuits found in modern data centers. Any electrical maintenance or installations should be done by certified electricians. Large UPS cabinets have a potential for lethal electrical arcs, so arc-flash (“bunny”) suits must be worn during maintenance. All racks/cabinets, PDUs, and other electrical equipment must be properly grounded. An emergency power cutoff (aka “big red button”) should be installed, which when pressed will cut all power to the room (or module or bay) if someone is getting shocked.</p>
<p>For server rooms in older buildings, all locations of asbestos should be clearly marked, or technicians should be trained for awareness and the use appropriate safety precautions (respiratory masks) when running network cable (typically to/from other areas of the building).</p>
<p>Servers can be heavy, even 75 lb (35 kg) for a 4U server. Even some large network switches cannot be lifted and must be delivered by forklift. To reduce back strain and risk of injury, you should use teamwork to mount servers. There are also server lifts that can align a server to the appropriate height for installation and removal.</p>
<p>There should be an adequate number of well-marked emergency exits. This seems obvious but is more difficult in large, containerized data centers and where there a multiple floors in the same facility.</p>
<p><strong>References</strong></p>
<ol>
<li>The Green Grid (2008). Quantitative Efficiency Analysis of Power Distribution Configuration for Data Centers. www.thegreengrid.org/~/media/WhitePapers/White_Paper<em>16</em>-_Quantitative_Efficiency_Analysis_30DEC08.pdf?lang=en.</li>
</ol>
<h2 id="Geographic_Location_Criteria"><a href="#Geographic_Location_Criteria" class="headerlink" title="Geographic Location Criteria"></a>Geographic Location Criteria</h2><p><strong>Geographic Location Requirements</strong></p>
<p>For a large multinational corporation, there is a good chance that it already has a number of servers at each of its branch offices and would likely build traditional data centers in the countries in which it is already established. The reason for having equipment closer to the users is for better application responsiveness, which (hopefully) yields higher productivity.</p>
<p>A company whose product is Web based would need to build multiple data centers around the world to provide better service to customers in a wide variety of geographic locations. A common example of this is Web search—Google, Microsoft, and Yahoo! all have large data centers at many sites scattered around the globe. This variety in location provides faster search results to the user, which is a competitive advantage (as long as accuracy is maintained).</p>
<p>A third category of companies that require large data centers are Web hosting providers. They offer server(s) to host the website for companies of various sizes. Small companies might need to have their site hosted in only one location to serve local customers. Large companies might have several different regional sites but would prefer to deal with one hosting company rather than a different one in each region.</p>
<p>An organization in any of the above categories would have similar goals when it comes to picking the location of a new data center. In order to satisfy the facility’s needs, it is necessary to locate a data center where there are adequate electrical power suppliers. For the “green” conscious, one might look for an electricity supplier that uses hydroelectric, solar, wind, geothermal, or other renewable energy sources. Regardless of the source of the power, it is desirable to work with a utility company that is willing to negotiate bulk-purchasing agreements, which lock in a specific discounted price per kilowatt hour for several years.</p>
<p>In addition to power, if you want to take advantage of evaporative cooling techniques mentioned earlier in this unit, your data center should be located near an adequate water supply. There are also instances in which you can locate near a lake/reservoir that is naturally cold, use that water as your cold supply, and return it after tempering (lower the temperature by mixing again with cold water from the same source).</p>
<p>For a smaller scale data center, another factor to consider is the cost of real estate and taxes. For a large deployment, however, the cost of the IT and facilities equipment are much higher than the cost of land, and monthly energy costs far outweigh any taxes.</p>
<p>There are also basic logistics that are desirable to have available to support the data center and its life cycle. A supply chain is needed to procure the goods and services, not just servers but all equipment and building materials. There also should be adequate ports/rails/highways to deliver standard shipping containers (40’ × 8’ × 8’6”; or 12.19m × 2.43m x 2.59m) to your site. Small and medium sized data centers will typically buy servers from companies such as IBM, Dell, or HP, whereas big companies have custom servers, but none of these are manufactured on site and so have to be purchased and delivered. Containerized/modular solutions do more of the manufacturing off site so that installation is faster at the data center. Recycling of raw materials should occur at the end of the life span of the hardware or for failed components, such as hard drives, so recycling availability should also be considered.</p>
<p><strong>Weather</strong></p>
<p>Also mentioned in the previous module was economization, or mixing of colder outside air with the hot air expelled from the IT equipment. In Figure 2.16, you will notice different colored bands. In regions of yellow or orange, you could use outside cooling during the colder seasons, and in regions of green and blue, you could likely use outside cooling year round. Regions marked as red could still use outside cooling but for a few months of the year. For example, because of its cold average climate and renewable energy sources, Iceland has a growing data center industry.</p>
<p><img src="/images/14536687448435.jpg" alt="Figure 2.16: Global average temperature map (Wikipedia, 2014)."></p>
<p>As you will see in the power usage effectiveness (PUE) section later in this module, the energy utilization will be higher during hot months and lower during cold months. This is due to the efficiency gains from “free cooling” (either using air economizers or naturally cold water.)</p>
<p>The average amount of annual rainfall might be a factor if you are considering using rainwater storage/filtration as a water source for cooling your data center. However, the number of sunny days per year in a particular region might convince you to try solar.</p>
<p>Part of a risk assessment for a particular location would include the frequency of natural disasters in the region, such as floods, hurricanes, tornadoes/cyclones, tsunamis, and earthquakes. As you can see in Figure 2.17, different regions have varying levels of susceptibility to a potentially damaging event.</p>
<p><img src="/images/14536687773973.jpg" alt="(a) Earthquake risk map"></p>
<p><img src="/images/14536687993821.jpg" alt="(b) Flood risk map"></p>
<p><img src="/images/14536688116773.jpg" alt="(c) Hurricane risk map"></p>
<p><img src="/images/14536688215654.jpg" alt="(d) Lightening risk map"></p>
<p><img src="/images/14536688339101.jpg" alt="(e) Tornado risk map"></p>
<p><img src="/images/14536688504783.jpg" alt="(f) Thunderstorm risk map"></p>
<p><img src="/images/14536688631189.jpg" alt="(g) Volcano risk map"></p>
<p><img src="/images/14536688743833.jpg" alt="(h) Wildfire risk map"></p>
<p>Figure 2.17: Natural disaster threat maps (<a href="/Global Datavault">Global Datavault</a>, 2013). (Click on each figure for an enlarged view.)</p>
<p>As a cloud user, or cloud provider, it is beneficial to have two (or more) geographically distinct data centers for your services to mitigate risks of natural disaster, excluding large asteroid impacts, of course.</p>
<p><img src="/images/14536689245659.jpg" alt=""></p>
<p><strong>Connectivity</strong></p>
<p>As broadband adoption among consumers continues to grow, the effectiveness of cloud computing will increase. You can find trending graphs and current broadband adoption at Akamai.</p>
<p>The Internet relies on fiber optics to send and receive data over long distances. Figure 2.18 shows the relationship between multiple tiers of the Internet, in which tier 1 providers own the actual fiber cables, the network equipment, and the buildings they are housed in; tier 2 providers own large networks as well and peer with (have connections to) other tier 2 providers but also have to lease some connections from tier 1 providers in order to reach the whole Internet; and tier 3 providers are only resellers, which provide connections to end users. A single corporate entity, such as Verizon in the United States, may provide services at all tiers.</p>
<p><img src="/images/14536689450310.jpg" alt="Figure 2.18: Internet connectivity (Wikipedia, 2014)."></p>
<p>In order to support a large amount of users, a cloud provider should choose a data center location that is in a city/region that has a tier 2 or tier 1 provider. This will also decrease the latency to global users due to fewer hops (each router is a hop) between the client and the server. A data center’s requirement for uplink to the Internet ranges from a few megabits per second to several hundred gigabits per second, and that much bandwidth simply is not yet available everywhere.</p>
<h2 id="Costs"><a href="#Costs" class="headerlink" title="Costs"></a>Costs</h2><p><strong>Data Center Costs</strong></p>
<p>As you may recall from Unit 1, organizations have to deal with capital expenses and operating expenses for their IT projects. For new companies with an anticipated need for many servers, or for existing companies that have outgrown their existing infrastructure, a decision must be made to build a new data center, expand existing facilities, or migrate some (or all) of their IT services to a cloud provider. With rising energy costs, companies with existing (but outdated) data centers would also consider building a new data center or retrofitting the existing one with new power/cooling. Many scenarios, such a retrofit, might cause unacceptable downtime, so they would instead choose to go to a new location and migrate servers/services from the old location to a new data center. In any of the above scenarios, a detailed cost analysis would be helpful in making the decision (and likely required before any budgetary approvals).</p>
<p>Prices fluctuate often. What was valid in 2011 might not be valid in 2014. The intent of this section is not to give you the tools to do an entire cost analysis on your own but rather to help you understand the types of expenses that go into building a data center.</p>
<p><strong>Capital Expenditures (CapEx)</strong></p>
<p>Capital expenses (which occur only once) for data centers include upfront planning, cost of property and/or construction, facilities equipment (power, HVAC, safety, and security), as well as IT equipment (servers, network switches, initial network connectivity). A more detailed explanation of each follows:</p>
<p>Upfront planning: Upfront planning design costs account for a significant portion of CapEx. This typically includes feasibility and impact studies, architectural design, engineering design, project management costs, and contingency costs. A recent Forrester research study [1] estimates these costs to range from 20% to 25% of the total cost for a typical data center.</p>
<p>Property costs: A principal expense incurred in building data centers is the cost of property. This, of course, varies significantly depending on location. Organizations typically have the option of either constructing a building from scratch or to repurpose an existing building to be a data center.</p>
<p>When building from scratch, many costs, such as the real estate transaction, consultant, brokerage, and building permits, apply in addition to the actual cost of building the data center “shell,” which could include excavation, grading, roadways, utility connections, and physical security. Reed Construction Data has estimates for the United States and are typically between <code>$200</code> to <code>$300</code> per square foot, depending on the type of building and labor.</p>
<p>Many organizations opt to repurpose existing buildings, particularly those that have been abandoned, such as factories or mills, among others. Google’s Hamina data center in Finland was repurposed from an old paper mill. Barcelona Supercomputing Center is an example of an old chapel repurposed as a data center site.</p>
<p>Leasing a building or subleasing space in a building can also be done, shifting this capital expense to an operating expense.</p>
<p>Facilities equipment: Some heavy equipment is often installed during the construction phase, and once the building shell is complete, data center–specific facilities can be built, and equipment needs to be procured and commissioned. This includes most of the equipment outlined in the “Facilities” page in the “Data Center Components” module, such as cooling (CRACs, chillers, condensers, water tanks), electrical equipment (power distribution, generators, transformers, UPSs, automated transfer switch), and fire detection and suppression systems (FM-200).</p>
<p>Estimates for the cost of HVAC and electrical equipment are approximately $7,000 to $20,000 per kilowatt of IT load. [1] In addition, fire suppression systems costs can range from <code>$20,000</code> to <code>$60,000</code> for a typical data center spanning several thousand square feet.</p>
<p>IT equipment and connectivity: Once the data center facility is ready, equipment can be moved in and installed. Typically, this involves purchasing rack-mountable servers and networking and rack power distribution equipment. In larger data center environments, it is becoming increasingly popular to roll out fully containerized servers with integrated power, cooling, and network management. IT equipment costs can vary widely depending on the size and configuration of the hardware.</p>
<p>In addition to IT equipment, the data center needs to have a dedicated network connection in order for it to be accessible to the organization that is using it. Network service providers typically charge an upfront average of $10,000/mile to install and commission fiber to a data center. [1]</p>
<p><strong>Operating Expenditures</strong></p>
<p>Operational expenditures (periodically recurring) for data centers typically include electrical power, staffing, cooling/HVAC, Internet uplink, maintenance, taxes, and/or leasing.</p>
<p>One of the biggest operational expenses in running a data center is the power, and it can account for approximately 70% to 80% of the overall cost of running a data center. [1] Electrical utility charges to industrial sectors (which differs from residential pricing) can vary significantly from state to state and country to country. In the United States, the Energy Information Administration (EIA), an entity within the U.S. Department of Energy, publishes regular reports on power costs broken down by state and sector. The International Energy Agency (IEA) publishes energy data for other countries as well. It is also important to note that the cost of power has been rising over the past few years. [2] Because power forms a large percentage of operational expenses and total cost of ownership (TCO), large data centers tend to be placed in regions with cheaper sources of power, whenever possible.</p>
<p>Staffing: Even though data centers can house thousands of servers, they are relatively efficient in terms of number of personnel per server. However, data centers still need a number of operational staff such, as twenty-four seven security and an on-site engineering staff. In countries in which labor is expensive, this cost will end up being the second-largest recurring expense after power. These costs can run into hundreds of thousands of dollars per year, depending on the location and staffing levels.</p>
<p>Cooling: Cooling costs also amount to a significant portion of data center expenses. According to a study by the American Society of Heating, Refrigerating, and Air Conditioning Engineers (ASHRAE), data center cooling consumes 42% of the total power drawn by traditional, inefficient data centers. [3] Many organizations are now paying attention to new techniques to reduce the amount of power required to keep a data center cool. ASHRAE has also published thermal guidelines for data center designers to plan and correctly configure cooling systems to maximize efficiency.</p>
<p>Connectivity: In addition to upfront installation and commissioning costs, network connectivity is usually charged monthly or quarterly from a service provider. The fee is typically in the range of several hundred or thousand dollars per month, based on the bandwidth and reliability guarantees of the connection. [1]</p>
<p>Maintenance: Purpose-built data centers also incur significant maintenance overheads in order to keep the facility running smoothly and to minimize downtime. This cost could be 3% to 5% of the initial construction costs. [1] In addition, facilities equipment may need periodic replacement or repair. For example, UPS batteries are replaced once in 5 years, on average.</p>
<p>Taxes and/or leasing: Building permits are required when building new data center shells or if an existing building’s structure is altered significantly. Organizations also need to pay some form of annual property tax on the property owned. This cost, like others, can vary considerably from region to region. Average taxes are estimated to be $70 per square foot in the United States. [1] If you are leasing a property, many of these taxes are included in the monthly price of your lease.</p>
<p><strong>Reference</strong></p>
<ol>
<li>Rachel Dines, et al. (2011). “Build or Buy? The Economics of Data Center Facilities.” Forrester Research.</li>
<li>U.S. Energy Information Administration. (May 7, 2014). Annual Energy Outlook 2014. <a href="http://www.eia.gov/electricity/" target="_blank" rel="external">http://www.eia.gov/electricity/</a>.</li>
<li>Barroso, Luiz André, and Hölzle, Urs. (2009). The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines. Morgan &amp; Claypool Publishers. <a href="http://bnrg.eecs.berkeley.edu/~randy/Courses/CS294.F09/wharehousesizedcomputers.pdf" target="_blank" rel="external">http://bnrg.eecs.berkeley.edu/~randy/Courses/CS294.F09/wharehousesizedcomputers.pdf</a>.</li>
</ol>
<h2 id="Power_and_Efficiency"><a href="#Power_and_Efficiency" class="headerlink" title="Power and Efficiency"></a>Power and Efficiency</h2><p><strong>Server Utilization</strong></p>
<p>As you recall from the introduction unit, multiple servers are virtualized and consolidated onto fewer physical hosts to increase utilization and decrease energy costs. This improvement is helpful, but the demand for IT-related services in organizations continues to grow, as does the number of Web-based startup companies, which have a higher proportional demand for IT equipment.</p>
<p><a href="http://youtu.be/bkjdGO0jz3E" target="_blank" rel="external">Video 2.4: Data center efficency.</a></p>
<p>To see why server utilization is important, let us discuss power consumption when a server is idle (the CPUs are not doing anything, but the HDDs are spinning, and RAM and I/O devices still consume power) versus when the server is at maximum load (when all CPUs are at 100% utilization).</p>
<p>To estimate the power consumption (P) at a specific utilization (n%) use the following formula:</p>
<p><img src="/images/14536691108382.jpg" alt=""></p>
<p>Through empirical measurements, this approximation is accurate to within ±5% across utilization rates.</p>
<p><img src="/images/14536691310594.jpg" alt=""></p>
<p><img src="/images/14536691438298.jpg" alt=""></p>
<p>If an organization wants to decrease its overall monthly operating expenses, it has to do so both inside the rack and outside the rack, and to minimize the latter, you must first understand power usage effectiveness (PUE).</p>
<p><strong>Power Usage Effectiveness (PUE)</strong></p>
<p>As we have read earlier, a data center draws a significant amount of power, from kilowatts to several megawatts. The cost of power is a significant element of the operating expenses of a data center and hence contributes to the total cost of ownership (TCO). As companies are offering more Web-based services, which are housed at a data center, the cost of power becomes an important element in the cost of offering services on the Web. Furthermore, some projections claim that data center related emissions will triple by 2020. These economic and environmental factors have accelerated the interest in measuring and improving the energy efficiency of data centers.</p>
<p>The power drawn by a data center is shared between the IT equipment and the support equipment, such as the power distribution and cooling facilities. Data center energy efficiency can be thought of as the ratio of the energy delivered to the IT equipment to the total energy delivered to the data center. Clear and standardized efficiency metrics are needed to help data centers to understand their energy efficiency, identify areas for improvement, and perform comparisons over time.</p>
<p>The Green Grid consortium has developed the PUE and its inverse, the data center infrastructure efficiency (DCIE). The PUE is simply the ratio of the total power entering the data center divided by the power used by the IT equipment. PUE measures how efficiently the power is delivered to the IT equipment in the data center.</p>
<p><img src="/images/14536691932629.jpg" alt=""></p>
<p>If a data center’s PUE is 3.0, then the data center facilities (e.g., power distribution, cooling) utilizes 2 units of energy for every unit delivered to the IT equipment. The lower the PUE, the more efficient the data center facilities. An ideal PUE is 1.0, which would indicate 100% efficiency, meaning that all the power drawn by the data center was delivered to the IT equipment.</p>
<p><img src="/images/14536692227041.jpg" alt=""></p>
<p><img src="/images/14536692443849.jpg" alt=""></p>
<p>In 2007, the Lawrence Berkeley National Labs (LBNL) ran an energy study for 25 data centers (see Figure 2.19). The best PUE, 1.14, resulted in about 87% of the site energy reaching the IT equipment, while in the worst case (PUE 3.0), only 33% makes it to the IT equipment.</p>
<p><img src="/images/14536692548006.jpg" alt="Figure 2.19: PUE of 25 data centers studied by LBNL (Lawrence Berkeley National Labs, 2007)."></p>
<p>The PUE allows companies to identify areas for improvement, address these areas, and monitor the progress in PUE over time. Google publishes quarterly PUEs for their actual data centers, as shown in Figure 2.20. Because Google’s data centers are mostly in the northern hemisphere, the average PUE typically rises in the summer because they require increased use of the cooling equipment.</p>
<p><img src="/images/14536692713460.jpg" alt="Figure 2.20: PUE data for all large-scale Google data centers. "></p>
<p>Google improves the efficiency of its data centers using five methods. [1] First, they accurately measure and record the PUE as often as possible. Second, they design good air containment to limit the mixing of hot and cold air. Third, they increase the cold air temperatures because equipment manufacturers allow their equipment to run within aisles at higher temperatures. Fourth, they utilize free cooling whenever possible. This possibility is determined by the geographical location of their data centers and the number of hours per year that allow the use of cool ambient air, evaporating power, and large thermal reservoirs. Fifth, they minimize power distribution losses by eliminating several power conversion steps. The lowest recorded PUE to date at a Google data center is 1.08, which is around 92% efficiency.</p>
<p>PUE simply measures the efficient use of power. Understanding the energy contributors to PUE is important to improve data center design practices. However, PUE is not sufficient as the only measure because it does not account for the load on the IT equipment. If PUE is low, but the IT equipment is not doing useful work, then the data center is losing money. Some recent practices include utilizing a TCO metric that accounts for the cost of the server, which is the total cost of energy that it will consume while running a specific workload over its lifespan. Using this approach, data centers will utilize application-specific optimizations, which will lead to more effective use of the data center equipment.</p>
<p><strong>PDU Branches (Optional Reading)</strong></p>
<p>In an earlier module, you were introduced to a PDU, or power distribution unit. You also learned about inefficiencies related to multiple conversions. This section shows you the advantages of using three-phase power and some common pitfalls to avoid when choosing the size and number of PDUs. This section is optional reading as it goes into specific details concerning electrical engineering</p>
<p>There are several types of power distribution: rack level, row level, and room level. All of them have an input electrical feed and provide one or more branch circuits, with each branch protected by a circuit breaker (a safety device that will “trip” if there is an overload, stopping the flow of electricity). The difference with row-level PDUs is that they typically take a higher voltage and output to a lower voltage using a transformer (transformers generate heat, so you will also find cooling fans inside a row-level PDU).</p>
<p><img src="/images/14536692981877.jpg" alt="Figure 2.21: Row-level and rack-level PDUs."></p>
<p>Three-phase power (often denoted with Greek letter phi [3Φ]) is how AC electricity is generated and transmitted, with each phase a sine wave that is 120 degrees apart. It is not common to have three-phase power in homes, but it is common in industrial buildings and a requirement in any modern data center. It is important to keep each phase as evenly loaded as possible. You should not plug all servers into one branch before going on to the next—stagger them instead. The important thing to know is that the total power that can be handled by a three-phase circuit is greater than that of a single phase (1Φ) for each copper wire (same thickness/gauge).</p>
<p><img src="/images/14536693222840.jpg" alt=""></p>
<p>For example, given V = 120 volts and I = 15 amps, then single-phase power = 1800W per three wires; and given Vline = 208 volts and Iline = 15 amps, then three-phase power = 5410W per five wires. (Recall that it is only the current/amps that determines how thick the copper has to be.)</p>
<p>Knowing the maximum power of any branch is important for choosing the number and size of PDU(s) to power your IT equipment. U.S. electric code states that the line current should not exceed 80% of the rating of the breaker (or fuse).</p>
<p><img src="/images/14536693341164.jpg" alt=""></p>
<p>It is rare for a server to actually consume the number of watts that their power supply is rated. For example, a PSU is labeled as 975W, but the server might only consume 650W at maximum capacity. For this reason, it is useful to take power measurements of each new model of server at various loads (idle, 25%, 50%, 75%, 100%) before putting it into production.</p>
<p><img src="/images/14536693525163.jpg" alt=""></p>
<p>When designing a redundant system, it is common to have multiple independent paths from the power source to the IT equipment. You can think of redundant power (and cooling) systems like hard drives in RAID. Recall that in a RAID1 mirror, you withstand a drive failing, but you also lose half your capacity, which is analogous to 2N redundancy—you cannot load a single component (e.g., UPS, generator, PDU) greater than 50% of its original capacity. While in RAID5, you still withstand a single drive failure, but you get higher utilization of individual drives, which is what N+1 redundancy is like for power and HVAC equipment. The remaining (working) units have to absorb the workload of the failed unit.</p>
<p>For example, you have a 100kW load and desire to have redundant UPS. In a 2N system, you have two UPS, each capable of powering 100kW, but their normal load would be 50kW each. If you use three UPS units instead, each could be smaller and capable of handling 50kW each but having a normal load of 33kW. In this case, the utilization is higher (66% instead of 50%), so the UPS would also run more efficiently.</p>
<p>Although blade chassis and some larger 4U/5U servers have N+1 power supplies, most servers and network equipment are designed with 2N redundant power supplies. For this reason, in a traditional data center, you would run two independent power feeds to each rack. This way, if you lose power in one feed, the other can take over. You must be cautious, however, because when one of the two feeds fail, the other one gets double the load. Incorrect assumptions often lead to overloading a branch circuit, which goes unnoticed in normal conditions. Then when there is a single failure, it has a cascading effect, overloading the remaining branches and tripping the breakers. This single failure leads to some or all servers in a rack losing power completely.</p>
<p>Power and Efficiency: Redundancy</p>
<p>You have a 100kW load and are using an N+1 design for UPS, with 3 + 1 = 4 UPS total.</p>
<p><img src="/images/14536694765030.jpg" alt=""></p>
<p>You have 12 servers, each with dual-redundant PSUs, which are measured together to consume a maximum of 500W. You also have two power feeds (A and B) going to the rack. To each feed, a three-phase 208V PDU is attached. Each PDU has three branches (one per phase), each with a 20A breaker.</p>
<p><img src="/images/14536694916072.jpg" alt=""></p>
<p><strong>References</strong></p>
<ol>
<li>GoogleEfficiency: How Others Can Do It. <a href="http://www.google.com/about/datacenters/efficiency/external/" target="_blank" rel="external">http://www.google.com/about/datacenters/efficiency/external/</a>.</li>
</ol>
<h2 id="Redundancy"><a href="#Redundancy" class="headerlink" title="Redundancy"></a>Redundancy</h2><p><strong>Data Center Redundancy</strong></p>
<p>The following video (Video 2.5) covers some important aspects of data center redundancy:</p>
<p><a href="http://youtu.be/Vh6RfwiRxNY" target="_blank" rel="external">Video 2.5: Data center redundancy.</a></p>
<p><strong>Data Center Tier Classifications</strong></p>
<p>Data centers can be classified based on reliability. In order to understand the four different tiers, as specified in TIA-942 standard, you must first know what is meant by the word reliable. Reliability is most frequently measured in uptime, or availability. A service that is 100% reliable is extremely difficult to guarantee, and, therefore, no companies will make that claim in their service-level agreement (SLA) to a customer. But if they did, it would be available to users every second of every year.</p>
<p><img src="/images/14536695528250.jpg" alt=""></p>
<p>Tier 1 has non-redundant components, such as power, cooling, and network connections, which can lead to downtime for maintenance in addition to single points of failure (SPOFs) (service is disrupted if a single part of the overall system has a problem). Tier 2 still has a single path for power and cooling but adds redundancy, such as UPS and a generator. Tier 3 adds multiple paths for power (multiple UPS and PDUs from the source to the rack) and cooling (e.g., several CRACs feeding raised floors). Tier 3 allows for no interruptions from planned maintenance. Tier 4 is similar to tier 3, but all paths must be redundant and can continue operations at full capacity with at least one unplanned outage (e.g., losing main power or network provider, UPS failure, AC outage).</p>
<p>Even a tier 4 data center could incur downtime due to multiple simultaneous outages, as occurred for Amazon during a large thunderstorm in Virginia. [1]</p>
<p><img src="/images/14536695671871.jpg" alt=""></p>
<p>The percentage of availability can be measured in hours, minutes, or seconds. Downtime is calculated as:</p>
<p><img src="/images/14536695753393.jpg" alt=""></p>
<p>For example, the calculation for the downtime that a tier 4 data center should achieve is as follows:</p>
<p><img src="/images/14536695864344.jpg" alt=""></p>
<p><img src="/images/14536696069897.jpg" alt=""></p>
<p><strong>References</strong></p>
<ol>
<li>Malik, Om. (2012). Severe Storms Cause Amazon Web Services Outage. <a href="http://gigaom.com/2012/06/29/some-of-amazon-web-services-are-down-again/" target="_blank" rel="external">http://gigaom.com/2012/06/29/some-of-amazon-web-services-are-down-again/</a>.</li>
</ol>
<h2 id="Reliability_Metrics"><a href="#Reliability_Metrics" class="headerlink" title="Reliability Metrics"></a>Reliability Metrics</h2><p><strong>Mean Time Between Failures (MTBF)</strong></p>
<p>Sometimes, when you read material referencing availability and reliability, you will see the term nines used. Five nines or nine nines refers to the number of nines in a percentage of availability. Two nines is 99%, three nines is 99.9%, four is 99.99%, and so on.</p>
<p>You will also see the phrases mean time between failure (MTBF) and mean time to failure (MTTF) in the specifications for many individual components (e.g., hard drives, motherboards, power supplies). These are defined as the average number of hours that component is expected to last and are usually determined by the manufacturer using a sample of parts in more extreme conditions. However, reported failure rates in the field often are higher. For example, hard drives are rated 1 million hours or more, but they have been found to be 2 to 10 times higher, [1] and Google found drive failures rates to be 50% higher on average in their study. [2] The failure rate is 1/MTBF. For example, if the MTBF of a device is 100 hours, then the chances of that device failing in 1 hour is 1/100, 0.01, or 1%.</p>
<p>It is important to note that when determining the overall MTBF of a system that has non-redundant components, the MTBFs of each individual component add as a reciprocal. Formally,</p>
<p><img src="/images/14536696484378.jpg" alt=""></p>
<p>On the other hand, when a system consists of redundant components, required in both components simultaneously to have an overall system failure. The overall MTBF of the system is thus the product of the MTBFs of each individual redundant component of the system. Formally,</p>
<p><img src="/images/14536696577801.jpg" alt=""></p>
<p>Availability and Reliability</p>
<p>Assume you have 20,000 independent hard drives of a particular model in your data center, each with a manufacturer specified MTBF of 1 million hours. Assume you do not trust the manufacturer-specified MTBF, so divide by two to get 500,000 hours.</p>
<p><img src="/images/14536698176330.jpg" alt=""></p>
<p><img src="/images/14536698337164.jpg" alt=""></p>
<p>One factor that is often overlooked when considering uptime is human error. No matter how much redundancy is designed into the system, even if it is properly implemented and maintained, there is some likelihood of a mistake being made by a person. The result of which eventually leads to a service being unavailable (downtime). Some mistakes can be prevented through policy, specifying standard configurations, good documentation, and change management.</p>
<p>When it comes to large cloud deployments, there is little focus on the hardware resiliency of an individual server. When 10,000 or more servers are working together as part of a single application, the application itself builds in the fault tolerance (more on that in the next unit, “Resource Sharing”). In this situation, a single server failure, or even several, will not disrupt the application/service. Small and medium sized businesses, or even a large enterprise that has legacy applications, cannot afford to author these cloud-style, fully customized applications, so they rely on third-party software, most of which does not respond well to hardware failures. Instead, cloud providers will focus on server hardware that is inexpensive and as energy efficient as possible, removing unneeded parts.</p>
<p><strong>References</strong></p>
<ol>
<li>Schroeder, Bianca, and Gibson, Garth A. (2007). Disk Failures in the Real World: What Does an MTTF of 1,000,000 Hours Mean to You?. In Proceedings of the 5th USENIX Conference on File and Storage Technologies. 1–16 Pages.</li>
<li>Eduardo Pinheiro, Weber, Wolf-Dietrich, and Barroso, Luiz André. (2007). Failure Trends in a Large Disk Drive Population. In Proceedings of the 5th USENIX Conference on File and Storage Technologies.</li>
</ol>
<h2 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>Servers mount into racks, come in incremental heights, called rack units (e.g., 1U, 2U, 3U), and have various depths. Larger cases allow for more expansion and/or redundant components.</li>
<li>Servers support more CPU sockets and DIMM slots than a desktop or laptop.</li>
<li>There are many types of PCI express cards available to expand servers, including RAID controllers, network cards (1, 10, or 40 gigabit Ethernet), Infiniband HCAs, storage HBAs, and coprocessors/accelerators, such as GPUs.</li>
<li>HDDs and SSDs can attach to a host through SATA or SAS and are used as building blocks for larger storage arrays, often employing RAID technology. These arrays can be a directly attached DAS, a remote block device SAN (using FC or iSCSI), or a remote file server NAS (using CIFS or NFS). Distributed file systems use many servers or arrays in parallel to increase performance and resiliency.</li>
<li>Data center networking topology is often multitier, with the lowest tier switches in the same rack (top of rack) or in the same row (end of row).</li>
<li>Ethernet remains the de-facto standard for data center networks, is available at a variety of speeds, and works over copper or fibre-optic cables.</li>
<li>Infiniband is a high-speed interconnect, now popular with HPC, and gaining acceptance in enterprise IT data centers.</li>
<li>Data centers and server rooms contain racks, raised floors, and cable trays and are designed with strong physical security.</li>
<li>Power distribution comes from the utility company or a backup generator and typically goes through a UPS, through PDUs, to the rack, and then server level. Some systems improve efficiency by converting AC to DC fewer times and/or decentralizing the UPS.</li>
<li>Servers are most often cooled through CRACs, which push cold air under a raised floor while taking in hot air and removing the heat through a chilled water loop. That loop is attached to a chiller, which itself removes heat by venting through the air or a secondary loop to a cooling tower. Higher densities/efficiencies can be achieved using in-row cooling with hot-aisle containment. Evaporative cooling is also becoming popular for data centers.</li>
<li>Safety is important for both equipment and personnel. FM-200 provides fast, safe fire suppression. All metal surfaces should be properly grounded, and safety equipment should be used appropriately.</li>
</ul>
<h1 id="Cloud_Management"><a href="#Cloud_Management" class="headerlink" title="Cloud Management"></a>Cloud Management</h1><p>You have learned the origins of a data center, from its roots in mainframe computers to the newest trends, and about what goes inside a data center (cooling, power, servers, network, and more), as well as some of the design criteria for various components that data center.</p>
<p>When designing large data centers, it is not possible to follow the same practices as that of a small data center. To truly leverage the economies of scale, it is important that the data center have a software layer that allows resources to be governed and managed easily. The cloud software stack is a platform to run a cloud given a pool of physical resources. Since most Cloud Service Providers (CSPs) are extremely wary of revealing their techniques (since it is their Intellectual Property) we have to rely on reverse engineering, rumors and the contribution of open-source cloud software stacks like OpenStack to understand the components involved.</p>
<p>In this module, we will start by learning about how a software middleware layer enables all the benefits of the cloud. We will look at the simplest use case, that of resource provisioning, and understand that a long series of steps are involved in handling a simple resource request. Of course, cloud providers charge for every quantifiable resource that a user utilizes. We must understand the details of the billing and monitoring systems that enables CSPs to profit from their data center. Automation and orchestration are some important techniques that we will look at the enable the low staff-to-resource ratio at CSPs, and drive down their effective costs.</p>
<p>Finally, we will look at OpenStack, an increasingly popular software platform that allows anyone with physical resources to create a cloud environment.</p>
<h2 id="Cloud_Middleware"><a href="#Cloud_Middleware" class="headerlink" title="Cloud Middleware"></a>Cloud Middleware</h2><p>Middleware is a general term for software that serves to “glue together” separate, often complex and already existing, programs. The term middleware is used in many contexts. For example, in the context of a single computer, middleware exists between the operating system kernel and application programs in the form of APIs, which manage access to system resources such as hardware devices.</p>
<p><img src="/images/14545511086121.jpg" alt="Figure 2.22: Cloud Middleware Features"></p>
<p>Cloud Middleware is a software platform that controls and coordinates different cloud services and makes it possible for users to issue service requests, and cloud providers to manage their infrastructure. Cloud Middleware consists of multiple abstraction layers that hides system complexity and enables communication between various applications, services, and devices that are part of a cloud service.</p>
<p><strong>Cloud Middleware Features</strong></p>
<p>There are a number of distinct and important features that cloud middleware provides, which come with several benefits. Some of the most important responsibilities of a cloud middleware stack are as follows:</p>
<p>Interoperability: Middleware is designed to connect distinct application services with different APIs to one another. Cloud service APIs act as middleware for cloud services by taking instructions from a program (written in languages such as Java, Python etc.), and translating them into service calls that the cloud service can understand. These instructions are further passed down the middleware stack at the cloud service provider’s end to perform actions (such as create virtual machines, allocate disk space, create a database table, etc.). Thus cloud middleware is the proverbial “glue” that enables multiple distinct applications and services to connect and communicate to each other.</p>
<p>Virtualization Management: Cloud middleware is also responsible for the configuration, allocation, creation, management and destruction of virtualized resources from physical resources. As an example, when a cloud service provider gets a request from a client to provision a virtual machine, it handles that request through multiple middleware layers until it reaches a hypervisor layer, which handles the configuration and allocation of a virtual machine for the client.</p>
<p>Resource Allocation and Scheduling: As discussed above, an important aspect of cloud middleware is the management of resources. As part of this responsibility middleware must manage resource allocation and scheduling of multiple resource types in order to achieve multiple goals such as performance, isolation, utilization, etc.</p>
<p>Load Balancing and Fault Tolerance: Cloud service providers must utilize adequate load balancing mechanisms in their middleware in order to optimize the distribution of load on multiple back-end services and physical infrastructure. The middleware should also coordinate with back-end resources in order to provide end-to-end fault-tolerance so that the availability of services to the client meets required SLOs.</p>
<p>Resource Monitoring: A crucial responsibility of middleware is the monitoring of resources. Monitoring provides a source of data which is valuable for the internal middleware features such as allocation, scheduling, load balancing and fault tolerance as discussed above. In addition, data from monitoring systems can be made available to clients, which gives them an additional visibility into the state of their applications and provisioned resources.</p>
<p>User Management and Security: Cloud middleware also must provide support for access control of users, and use standard security practices for the management of various types of credentials to control access to individual resources. The user management system in the middleware should provide features that allow cloud clients to create and destroy entities such as users and groups, and configure the Access Control Lists (ACLs) that define the resources that individual users and groups have access to.</p>
<p>User Interface and APIs: Finally, cloud middleware must make available a client-facing set of APIs. It is also typical of cloud middleware to provide user-friendly interfaces (typically in the form of Web interfaces), where clients can log in and manage their provisioned resources and make service requests.</p>
<p><img src="/images/14545512278768.jpg" alt=""></p>
<p><strong>References</strong></p>
<ol>
<li>Amrani C., Filali K., Ahmed K., Diallo A., Telolahy S. (2012). “A Comparative Study of Cloud Computing Middleware.” 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing.</li>
</ol>
<h2 id="Resource_Provisioning"><a href="#Resource_Provisioning" class="headerlink" title="Resource Provisioning"></a>Resource Provisioning</h2><p>The most basic role of a Cloud Service Provider is to package and isolate virtual parts of a physical data center and provide access for cloud users to provision resources on the cloud. Provisioning is the process of mapping abstract resource requests by cloud users to physical resources on servers within a data center.</p>
<p>In IAAS, this mainly involves launching virtual machines on top of hypervisors (these are special software applications that enable isolation of virtual machines on top of a physical machine- we will read more about them soon), as well as mounting storage volumes from a storage pool and creating a private network overlay for the user’s resources.</p>
<p>The techniques behind virtualizing compute, storage or networking resources is dealt with in future modules. Here, we focus on understanding some of the high level steps that a CSP must take to create and allocate a publicly accessible virtual machine with a fixed set of resources to an end user.</p>
<p><strong>Components of a Resource Provisioning System</strong></p>
<p>A resource provisioning system on the cloud generally has the following sub-parts:</p>
<ol>
<li>Access to a physical pool of resources- generally thousands or millions of servers, interconnected by a network (generally using a fat-tree topology as discussed in a previous module) and also a large pool of disks.</li>
<li>An identity management subsystem that maintains and validates the end user’s credentials for accessing many different types of resources; it also provides for role based access control.</li>
<li>A metering and monitoring system to detect utilization of physical resources</li>
<li>A billing and charge management system to map the metered resources to physical costs and take appropriate actions based on the user’s allowed privileges.</li>
<li>A resource manager that works with a hypervisor to map physical resources to virtual abstractions.</li>
<li>Often, the provisioning system will have a web front-end or an API.</li>
</ol>
<p><strong>Cloud End-to-End Service Provisioning Flow</strong></p>
<p>Figure 2.23 below shows the typical end-to-end steps for a customer provisioning a virtual machine from a Cloud Service Provider (CSP):</p>
<p><img src="/images/14545514768762.jpg" alt="Figure 2.23: Typical End-to-End IaaS provisioning steps"></p>
<p>The steps illustrated in Figure 2.23 are explained as follows:</p>
<ol>
<li>The customer logs on to the portal and is authenticated by the identity management system.</li>
<li>Based on the customer’s entitlement, the portal extracts a subset of services that the user can order from the service catalogue and constructs a ‘request catalog’.</li>
<li>The customer selects a service, e.g. a virtual server of a particular size. Associated to this service is a set of technical requirements such as the amount of vRAM, vCPU, etc. in addition to business requirements such as high availability or SLA requirements.</li>
<li>The orchestration tool extracts the technical service information from the service catalog and decomposes the service into individual parts, such as compute resource configuration, network configuration, and so on.</li>
<li>The provisioning process is initiated.</li>
<li>The virtual machine running on the server is provisioned using the server/compute domain manager.</li>
<li>The network, including firewalls and load balancers, as well the storage is provisioned by the network, network services and storage domain managers.</li>
<li>Charging is initiated for billing/chargeback and the change management case is closed and the customer is notified accordingly.</li>
</ol>
<p><img src="/images/14545515552417.jpg" alt=""></p>
<p><strong>Metering and Monitoring Cloud Services</strong></p>
<p>Cloud service providers (CSPs) charge their customers according to usage of their services. In order to do that, providers have to monitor the usage of their services carefully and have to include the cost of running these services in their infrastructure. This is mission critical for both CSPs and their customers.</p>
<p>The revenue stream of a CSP depends on the correctness of metering and monitoring their resources. Considering AWS revenues in 2014, losing metrics for a couple of hours can cost Amazon millions of dollars. On the other, overcharging customers for a couple of hours can highly affect their credibility.</p>
<p>From the customer’s point of view, the cost of cloud resources forms an important part of their expenses. In order for cloud clients to make budget plans, they must receive consistent bills every month. This poses important challenges to cloud providers for metering and monitoring.</p>
<p><strong>Challenges in Monitoring and Metering</strong></p>
<p>There are various costs included in cloud resources. Although fixed costs such as facilities, staff, servers are easy to calculate, variable costs require constant metering and monitoring. The advantage of using CSPs comes from paying only for the resources which are used. For example, provisioning an EC2 instance includes the cost of instance usage per hour, storage per GB-month for each storage type, and data transfer per GB-month. Even for this one resource, AWS has to keep track of these metrics for every instance and attached volume. In Figure 2.24, a possible break down of various cost for services can be found. If we imagine doing this for more than 1 million AWS customers for tens of different types of services, this will require the metering and monitoring of gigabytes of logs every minute and charging customers accordingly. The most popular model which is used to define such metrics is called the chargeback model.</p>
<p><img src="/images/14545516100780.jpg" alt="Figure 2.24: Metering in Different Types of Cloud Services"></p>
<p><strong>Chargeback Model</strong></p>
<p>Basically the chargeback model is the ability of an IT organization to measure the usage of resources and chargeback their customers accordingly. Traditionally the chargeback model is easy to implement since an IT department can easily divide its budget for the business units that it serves like software licences, stand-alone servers etc. This is challenging in cloud because the CSP needs to consider the rate and time of consumption.</p>
<p><strong>Validation of Metering and Monitoring</strong></p>
<p>From the customer’s point of view, verifiable metering is an important issue. There are some costs which are easy to measure like EC2 usage (hourly usage * cost per hour) but it is hard for customers to measure other resources such as data transitions or I/O requests. In order to verify metering and monitoring, users can work with certified cloud providers. For instance, IBM’s “Resilient Cloud Validation” program allows businesses who collaborate with IBM to perform a consistent program of benchmarking and validation of cloud services.</p>
<p><strong>Case Study: Ceilometer</strong></p>
<p>Although the underlying architecture of metering and measuring is hidden by corporate CSPs, Ceilometer is designed for OpenStack metering, billing and rating. The high-level architecture of OpenStack Ceilometer can be summarized as follows (Figure 2.25):</p>
<p><img src="/images/14545516558630.jpg" alt="Figure 2.25: Ceilometer Architecture"></p>
<p>Polling Agent: A daemon which polls OpenStack services for metering.</p>
<p>Notification Agent: A daemon which listens to notifications on the message queue and converts them into samples and events.</p>
<p>Collector: Daemon designed to gather metering data created by the notification and polling agents.</p>
<p>API: Service to query the data recorded by the collector.</p>
<p>Alarming: Daemons to evaluate and trigger notifications based on predefined rules.</p>
<p>Each of these services are designed to scale horizontally. Additional workers and nodes can be added according to expected load.</p>
<p><img src="/images/14545516976128.jpg" alt=""></p>
<h2 id="Cloud_Orchestration_and_Automation"><a href="#Cloud_Orchestration_and_Automation" class="headerlink" title="Cloud Orchestration and Automation"></a>Cloud Orchestration and Automation</h2><p>Cloud Orchestration is the process by which all of the provisioning, middleware and other complex systems in a cloud can be automatically arranged and coordinated. Today, cloud orchestration supports the specification and management of the following resources including but not limited to:</p>
<ul>
<li>Compute Servers (In the form of virtual machines)</li>
<li>Auto Scaling</li>
<li>Load Balancers</li>
<li>Databases</li>
<li>Block Storage</li>
<li>DNS and Virtual Network (VLANs)</li>
<li>Software configuration and setup (typically in the form of bootstrapping scripts)</li>
</ul>
<p><strong>Benefits of Orchestration</strong></p>
<p>Cloud orchestration is a method to fully realize the dynamic potential of cloud infrastructure, by allowing users to specify and configure a complete application encompassing multiple resource types. One of the most important aspects of the cloud is the rapid service delivery which is made possible by cloud orchestration. It also saves cost by eliminating manual intervention and management of IT services. Benefits of cloud orchestration can be summarized as follows:</p>
<ul>
<li>Integration, automation and optimization of service deployment across heterogeneous environments.</li>
<li>Self-service portals for selection of cloud services, including storage and networking, from a predefined menu of offerings.</li>
<li>Reduced need for intervention to allow lower ratio of administrators to physical and virtual servers.</li>
<li>Automated high-scale provisioning and de-provisioning of resources with policy-based tools to manage virtual machine sprawl by reclaiming resources automatically.</li>
<li>Ability to integrate workflows and approval chains across technology silos to improve collaboration and reduce delays.</li>
<li>Real-time monitoring of physical and virtual cloud resources, as well as usage and accounting chargeback capabilities to track and optimize system usage.</li>
<li>Making adoption of best practices easy by prepackaging automation templates and workflows for most common resource types.</li>
</ul>
<p><strong>Orchestrator Tools</strong></p>
<p>There are variety of tools which provides orchestration, Puppet and Chef are popular examples.</p>
<p><strong>Puppet</strong></p>
<p>Puppet is a tool that can be used to issue service commands to multiple client machines from a master machine. This allows developers and systems administrators to manage client machines from a single master machine, providing commands to individual clients based on code which describes the configuration actions that are to be performed on each machine:</p>
<p><img src="/images/14545518040764.jpg" alt="Figure 2.26: Puppet"></p>
<p><strong>Chef</strong></p>
<p>Chef uses the same concepts as Puppet, but differs in deployment. Chef operates using user-specified recipes, which describe the state of the resources in the system, such as the packages (and versions) to install, start up daemons or services to execute, or an data to be downloaded/created. This ensures an identical operating environment with the same resources and configurations across all systems. Using Chef, it is possible to automate the creation of a complex distributed system, stitching together various components and workflows.</p>
<p><img src="/images/14545518601244.jpg" alt=""></p>
<h2 id="Case_Study__3A_OpenStack"><a href="#Case_Study__3A_OpenStack" class="headerlink" title="Case Study : OpenStack"></a>Case Study : OpenStack</h2><p>Earlier, we described OpenStack as a popular solution that allows developers to build both public and private clouds. At the time of writing, OpenStack has mature offerings that cover a large part of the cloud software stack spectrum. We take a very quick conceptual overview of the various pieces of the OpenStack architecture, as the complete description of the OpenStack architecture is beyond the scope of this course. The purpose is to give you an overview of the different components that make up a cloud software stack.</p>
<p>OpenStack consists of multiple layers that can be used to configure, provision, manage, monitor and deprovision various types compute, storage and networking resources.</p>
<p>A high level view of the various services involved in the OpenStack middleware suite is represented in Figure 2.27 below:</p>
<p><img src="/images/14545518887840.jpg" alt="Figure 2.27: OpenStack Service Architecture"></p>
<p><strong>User Authentication Service (Keystone)</strong></p>
<p>The primary authentication service in OpenStack is called Keystone. Keystone is an OpenStack project that provides Identity, Token, Catalog and Policy services for use specifically by individual services in the OpenStack family.</p>
<p>The identity service provides authentication credential validation as well as data about users and groups. The token service validates and manages session tokens used for authenticating requests once a user’s credentials have already been verified. The catalog service provides an endpoint registry used for endpoint discovery. The policy service provides a rule-based authorization engine and the associated rule management interface which can be used to dynamically grant or deny resource privileges to specific applications and services.</p>
<p><strong>Monitoring Service (Ceilometer)</strong></p>
<p>The Ceilometer project aims to deliver a unique point of contact for billing systems to acquire all of the measurements needed to establish customer billing, across all current OpenStack core components with work underway to support future OpenStack components. Celiometer is designed to provide efficient collection of metering data, while ensuring metering messages are digitally signed and non-repudiable. Ceilometer was introduced in “Metering and Monitoring Cloud Services”.</p>
<p><strong>Orchestration Service (Heat)</strong></p>
<p>Heat is the primary orchestration system in OpenStack. Heat provides orchestration services for higher-level systems such as Sahara, OpenStack’s cluster provisioning system that is similar to AWS EMR. Heat can orchestrate individual cloud middleware services such as the compute service (nova), block storage service (Cinder) and networking services (Neutron).</p>
<p><strong>Virtual Machine Image Service (Glance)</strong></p>
<p>Glance image services include discovering, registering, and retrieving virtual machine images. Glance has a RESTful API that allows querying of VM image metadata as well as retrieval of actual virtual machine images.</p>
<p><strong>Object Storage Service (Swift)</strong></p>
<p>Swift is a highly available, distributed, eventually consistent object store, similar to Amazon’s S3. You create, modify, and get objects and metadata by using Swift’s Object Storage API, which is implemented as a set of REST web services. We will take a closer look at OpenStack’s Swift service in future modules.</p>
<p><strong>Block Storage Volume Management Service (Cinder)</strong></p>
<p>Cinder is an OpenStack project to provide “block storage as a service”, similar to Amazon’s Elastic Block Storage (EBS) service. Cinder allows users to define block storage devices and attach them as volumes to individual virtual machines. Cinder virtualizes pools of block storage devices and provides end users with a self service API to request and consume those resources without requiring any knowledge of where their storage is actually deployed or on what type of device. Cinder is also used by the Glance service to store virtual machine images as volumes. Cinder is designed to work with a growing number of storage systems and devices including Storage-Area-Network (SAN) appliances and distributed file systems.</p>
<p><strong>Cluster Provisioning and Management Service (Sahara)</strong></p>
<p>The Sahara project aims to provide users with a simple means to provision data processing frameworks (such as Hadoop, Spark and Storm) on OpenStack, much like the Elastic MapReduce (EMR) service on AWS. Sahara can be used to specify cluster configuration parameters such as the framework version, cluster topology, node hardware details and more. Sahara uses Nova to provision individual cluster nodes using framework specific images supplied by Glance. Sahara then runs special scripts to complete the configuration of each of the individual cluster nodes so that they are ready to execute jobs.</p>
<p><strong>Compute Service (Nova)</strong></p>
<p>Nova is an OpenStack project designed to provide massively scalable, on demand, self service access to compute resources. Nova is designed to provision, manage different virtual machines deployed using different virtualization platforms including Xen, VMWare, Hyper-V and can even deploy virtual machines onto EC2, with an aim to support the notion of “cloud bursting”. The Nova service is also used by other services such as Trove (to provision virtual machines that hold databases), and Sahara (to provision virtual machines for analytics clusters).</p>
<p><strong>Database-as-a-Service (Trove)</strong></p>
<p>Trove is a Database as a Service for OpenStack. It’s designed to run entirely on OpenStack, with the goal of allowing users to quickly and easily utilize the features of a relational database without the burden of handling complex administrative tasks. Cloud users and database administrators can provision and manage multiple database instances as needed. The service focuses on providing resource isolation at high performance while automating complex administrative tasks including deployment, configuration, patching, backups, restores, and monitoring.</p>
<p><strong>User Interface (Horizon)</strong></p>
<p>Horizon is the canonical implementation of OpenStack’s Dashboard, which provides a web-based user interface to OpenStack services including Nova, Swift, Keystone, etc. Users can log into Horizon to get interactive dashboard views of the status of their resources, and issue service requests to individual OpenStack services.</p>
<p><strong>Physical Hardware Management (Ironic)</strong></p>
<p>Ironic is a bare-metal provisioning system that can be used to control physical serves. Ironic has the ability to power on and power off individual servers using industry-standard protocols such as Intelligent Platform Management Interface (IPMI). Once machines are powered on, it can coordinate the machine boot up process using the Preboot eXecution Environment (PXE) boot, using specific boot images stored in the image management service (Cinder).</p>
<p>One may wonder why bare metal provisioning makes sense in a cloud environment where virtualized resources are the norm. There are situations where bare metal provisioning could be advantageous: A client that has a higher SLO requirement for compute performance for say, a high performance computing application, may require dedicated hardware without the overheads of a hypervisor. In such a case, it would be advantageous to provision and dedicate an entire physical server without virtualization.</p>
<p>As an example of one of the scenarios that Ironic can be used in, consider the case where there happens to be a physical server capacity crunch due to a large number of virtual machines that have been provisioned. In this case, Ironic can power on additional servers that are powered off and configure them to boot with a specific hypervisor image. Once the additional capacity is made available, virtual machines can be provisioned on the new servers, or existing virtual machines can be migrated to the new servers. In case of the reverse (where the physical server capacity is not being fully utilized), virtual machines can be consolidated to a smaller number of physical servers and the servers that have been freed up can be powered off to save on electric costs.</p>
<h2 id="Cloud_Software_Stack_Summary"><a href="#Cloud_Software_Stack_Summary" class="headerlink" title="Cloud Software Stack Summary"></a>Cloud Software Stack Summary</h2><ul>
<li>The cloud software stack enables the proivisioning, monitoring and metering of virtual user “resources” on top of a Cloud Service Provider’s infrastructure.</li>
<li>Cloud Middleware is the overarching platform that coordinates all of the different cloud services and allows them to be accessed as services by users.</li>
<li>Provisioning on the cloud refers to the creation of different resources on top of the physical infrastructure. A provisioning system deals with identity and cost management, scheduling resources.</li>
<li>Metering is an extremely challenging task on clouds. It requires tracking the utilization of network, storage IOPS, disk and compute capacity, by thousands or millions of concurrent users and mapping into different costs.</li>
<li>Orchestration and Automation are the processes that enable Cloud Service Providers to operate at scale, by running workflows and creating environments and configurations without manual intervention.</li>
<li>OpenStack is an open-source cloud stack implemenations that contains services for user authentication, provisioning, management, metering, compute, storage among others.</li>
</ul>
<h1 id="Cloud_Software_Deployment_Considerations"><a href="#Cloud_Software_Deployment_Considerations" class="headerlink" title="Cloud Software Deployment Considerations"></a>Cloud Software Deployment Considerations</h1><p>Now that you have seen how a cloud data center runs, you may feel that all of the complexity is handled by the Cloud Service Providers (CSPs), and it is trivial to build a cloud application. However, to truly fulfil the promise of the cloud, developers must design and deploy their applications following a few best practices.</p>
<p>In this module, we look at how applications are to be deployed on the cloud to ensure fault tolerance and achieve high performance. The global presence of cloud data centers simplifies the process of reaching many end users, but deployment patterns must support easy scaling and fault-tolerance.</p>
<p>A cloud application must be economical, reachable with low-latency, support a large number of simultaneous users (high throughput), without any service degradation (fault tolerance and elasticity). Despite the tools that CSPs provide, building such an application requires a lot of planning.</p>
<p>In the last module of Unit 2, we will look at some common patterns around load balancing and scaling, as well as how robust applications should be built.</p>
<p>Last, we explore some additional challenges faced by responsive, interactive applications that use a large cluster of cloud computing resources and look at some solutions.</p>
<h2 id="Programming_the_Cloud"><a href="#Programming_the_Cloud" class="headerlink" title="Programming the Cloud"></a>Programming the Cloud</h2><p><strong>Cloud Programming Considerations</strong></p>
<p>Designing programs that are destined for the cloud requires special considerations. Depending on the type of application and the expected load, developers can utilize some of the features provided by cloud providers to enhance the scalability and maintainability of programs. Use of automatic scaling systems and load balancers allow developers to dynamically grow or shrink infrastructure based on the utilization of hardware or a program-computed load factor.</p>
<p>There are multiple considerations that a developer must account for when developing or migrating an application to the cloud, particularly those that concern performance and security.</p>
<p><strong>Factors that Impact Application Performance on the Cloud</strong></p>
<p>The environment in a cloud-centric data center is different from what developers might be used to when designing and deploying applications on owned infrastructure. Some developers find it hard to fine-tune or enhance the performance of their applications because they do not have access to the physical hardware layout or specifications on public clouds. We will try to enumerate some of the top concerns, with specific emphasis on factors that affect application performance on the cloud:</p>
<p><strong>Resource Bandwidth and Latency</strong></p>
<p>A primary concern for developing and deploying cloud applications is latency. Developers must plan their applications with strict latency requirements in mind. One approach is to compile the distribution of client locations. This will allow developers to find the optimal set of data center locations which can be used to optimize end-user performance and responsiveness. This is particularly true in web applications, where individual HTTP requests for static web content can represent an important fraction of the web page load times.</p>
<p>Apart from latency, applications may also have strict bandwidth requirements, particularly with those that deal with rich multimedia content such as audio and video. Many cloud providers allow cloud developers to specify performance parameters during provisioning in the form of IOPS requirements for compute and storage resources. In addition, many cloud providers allow developers to set up virtual networks. The implementation and adoption of Software Defined Networking and Storage (covered in future modules) provide additional insights into newer techniques used by data centers to manage traffic from multiple clients, while managing individual requirements as specified in the client SLOs.</p>
<p>The techniques mentioned above are primarily targeted for static content. A far more difficult problem is to optimize the latency of access to distributed data storage systems, particularly those that have to handle writes and updates. We will learn a bit more of these concerns in future modules.</p>
<p><strong>Multi-tenancy</strong></p>
<p>Applications on public data centers typically run on shared infrastructure. This aspect of cloud services raised several important issues. While modern virtualization technologies provide an isolated environment in terms of application environment and security, they typically cannot ensure performance isolation. Therefore virtualized resources on clouds cannot guarantee consistent performance at all times, the performance of a resource at any given time is a function of the total load on the resources from all tenants, also known as the interference experienced from other tenants sharing the same hardware.</p>
<p>Some cloud providers such as AWS provide clients the ability to provision certain types of resources (such as EC2 instances) on dedicated hardware. This provides protection against wide fluctuations in resource performance, delivering fairly consistent performance for the resources. However, dedicated hardware instances cost considerably more than regular on-demand instances, as AWS needs to assign a server exclusively for your resources.</p>
<p>A related aspect of multi-tenancy is the issue of provisioning variation, wherein identical requests for virtual resources on public clouds are not mapped identically onto physical resources, thereby causing a variation in performance [1] . For example, two identical requests for virtual machines (VM1 and VM2) could be routed to two different physical machines (A and B). Physical machine A might have four other tenants competing for resources on the same machine, while machine B may have only two. However, the client is charged the same for virtual machines VM1 and VM2, but can potentially experience different performance on these machines.</p>
<p><strong>Security Settings</strong></p>
<p>Public clouds are subject to increased attack vectors, as we saw in Unit 1. Developers must be extremely cautious in ensuring that they follow best practices, protocols and procedures when deploying and maintaining applications on the cloud. As a result, additional performance overheads may be experienced due to the use of security protocols mandated by public clouds.</p>
<p>Since we have already discussed these protocols in a previous module, we will not discuss it in detail again. Any code deployed on a public cloud should go through a strict process of manual and automated source code reviews and static analysis, as well as dynamic vulnerability analysis and penetration testing. Guidelines for deploying applications securely are shown on the next page.</p>
<p><strong>References</strong></p>
<ol>
<li>Rehman, M.S and Sakr, M.F (2010). “Initial Findings for Provisioning Variation in Cloud Computing.” 2010 IEEE Second International Conference on Cloud Computing Technology and Science (CloudCom).</li>
</ol>
<h2 id="Deploying_Applications_on_the_Cloud"><a href="#Deploying_Applications_on_the_Cloud" class="headerlink" title="Deploying Applications on the Cloud"></a>Deploying Applications on the Cloud</h2><p>Once a cloud application has been designed and developed, it can be moved to the deployment phase for release to clients. Deployment can be a multi-stage process, each involving a series of checks to ensure that the goals of the application are met.</p>
<p>Before deploying a cloud application into production, it is useful to have a checklist to assist in evaluating your application against a list of essential and recommended best practices. Examples include the deployment checklist from <a href="https://media.amazonwebservices.com/AWS_Operational_Checklists.pdf" target="_blank" rel="external">AWS</a> and <a href="https://msdn.microsoft.com/en-us/library/azure/hh694044.aspx" target="_blank" rel="external">Azure</a>. Many cloud providers provide a comprehensive list of tools and services that assist in deployment, such as <a href="http://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf" target="_blank" rel="external">this document</a> from AWS.</p>
<p><strong>The Deployment Process</strong></p>
<p>The deployment of a cloud application is an iterative process which starts from the end of development right through to the release of the application on the production resources (Figure 2.28):</p>
<p><img src="/images/14545526245060.jpg" alt="Figure 2.28: Code deployment process"></p>
<p>It is typical for cloud developers to maintain multiple concurrently running versions of their applications to pipeline deployment of their application to into various stages:</p>
<ol>
<li>Testing</li>
<li>Staging</li>
<li>Production</li>
</ol>
<p>Each of the three stages mentioned above should ideally have identical resources and configuration which allows developers to test and deploy the application and minimize the chances of inconsistencies stemming from a change in the environment and configuration.</p>
<p><strong>Pipelining Application Changes</strong></p>
<p>In a typical agile application development scenario (illustrated in the figure above), applications are maintained by a set of engineers and developers who work on issues and bugs using some kind of issue tracking mechanism. The changes to the code are maintained through a code repository system (say, svn, mercurial or git), where separate branches are maintained for release of code. After passing through code changes, reviews and approvals, the code can be pipelined into the testing, staging and production phases. This can be done in multiple ways:</p>
<p>Custom Scripts: Developers can use custom scripts to pull the latest version of the code and run specific commands to build the application and bring it into production state.</p>
<p>Pre-Baked Virtual Machine Images: Developers can also provision and configure a virtual machine with all the required environment and software to deploy their application. Once configured, the virtual machine can be snapshotted and exported to a virtual machine image (such as an AMI in AWS), and this image can be provided to various cloud orchestration systems to be automatically deployed and configured for a production deployment.</p>
<p>Continuous Integration Systems: In order to simplify the various tasks that are involved in deployment, Continuous integration (CI) tools can be used to automate tasks (such as retrieval of the latest version from a repository, building application binaries and running test cases) that need to be completed in the various machines that make up the production infrastructure. Examples of popular CI tools include: Jenkins, Bamboo, Travis. AWS Code Pipeline is an AWS-specific CI tool designed to work with AWS deployments.</p>
<p><strong>Managing Downtime</strong></p>
<p>Certain changes to the application may require partial or full termination of the application services to incorporate a change in the application back-end. Developers have to typically schedule a specific time of day to minimize interruptions to customers of the application. Applications that are designed for continuous integration may be able to perform these changes live on production systems with minimal or no interruption to the application’s clients.</p>
<p><strong>Redundancy and Fault Tolerance</strong></p>
<p>Best practices in application deployment typically assume that cloud infrastructure is ephemeral and may be unavailable or change at any moment. For example, virtual machines deployed in an IaaS service may be scheduled for termination at the cloud provider’s discretion, depending on the type of SLA.</p>
<p>Applications must refrain from hard-coding or assuming static endpoints for various components, such as databases and storage endpoints. Well designed applications should ideally use service APIs to query and discover resources and connect to them in a dynamic fashion.</p>
<p>Catastrophic failures in resources or connectivity can happen at a moment’s’ notice. Critical applications must be designed in anticipation of such failures and must be designed for failover redundancy.</p>
<p>Many cloud providers design their data centers into regions and zones. A region is a specific geographic site which houses a complete data center, while zones are individual sections within a data center which are isolated for fault tolerance. For example two or more zones inside a data center may have separate power, cooling and connectivity infrastructure so that a fault in one zone will not affect the infrastructure in the other. Region and Zone information is typically made available by cloud service providers to clients and developers to design and develop applications that can utilize this isolation property.</p>
<p>Developers can therefore configure their application to use resources in multiple regions or zones in order to improve the availability of their application and tolerate failures that may happen across a zone or region. They will need to configure systems that can route and balance traffic across regions and zones. DNS servers can also be configured to reply to domain lookup requests to particular IP addresses in each zone, depending on where the request originated. This provides a method of load balancing based on the geographic proximity of clients.</p>
<p><strong>Security and Hardening in Production</strong></p>
<p>Running Internet applications on a public cloud must be done with care. Since cloud IP ranges are well-known locations for high-value targets, it is important to ensure that all applications deployed on the cloud follow best practices when it comes to securing and hardening endpoints and interfaces. Some very basic principles that should be followed include:</p>
<ol>
<li>All software should be switched to production mode. Most software supports “debug-mode” for local testing and “production-mode” for actual deployments. Debug-mode applications generally leak a large deal of information to attackers who send it malformed inputs and hence provide an easy source of reconnaissance for hackers. No matter if you are using a web-framework like Django and Rails or a database like Oracle, it is important to follow the relevant guidelines for deploying production applications.</li>
<li>Access to nonpublic services should be restricted to certain internal IP addresses for admin access. Make sure that administrators cannot directly log in to a critical resource from the Internet without visiting an internal launchpad. Configure firewalls with IP address and port-based rules to allow the minimal set of required accesses, especially over SSH and other remote connectivity tools.</li>
<li>Follow the principle of least privilege. Run all services as the least privileged user that can perform the required role. Restrict the use root credentials to specific manual logins by system administrators who need to debug or configure some critical problems in the system. This also applies to access to databases and administrative panels. Accesses should generally be protected using a long, random public-private key pair, and this key pair should be stored securely in a restricted and encrypted location. All passwords should have strict strength requirements.</li>
<li>Use well-known defensive techniques and tools for intrusion detection and prevention systems (IDS/IPS), security information and event management (SIEM), application layer firewalls, and anti-malware systems.</li>
<li>Set up a patching schedule that coincides with patch releases by the vendor of the systems that you use. Often, vendors like Microsoft have a fixed release cycle for patches.</li>
</ol>
<h2 id="Build_Fault-tolerant_Cloud_Services"><a href="#Build_Fault-tolerant_Cloud_Services" class="headerlink" title="Build Fault-tolerant Cloud Services"></a>Build Fault-tolerant Cloud Services</h2><p><strong>Failures and Fault Tolerance</strong></p>
<p>A large part of data center and cloud service management involves designing and maintaining a reliable service based on unreliable parts. The slide below (Figure 2.29) is a part of Google’s training for new hires, and should provide an idea of the large number (and types) of failures that are experienced regularly at a large data center.</p>
<p><img src="/images/14545528044106.jpg" alt="Figure 2.29: Reliability Issues from a Google Presentation"></p>
<p>A failure in a system occurs as a result of an invalid state introduced within the system due to a fault. Systems typically develop faults of one of the following types:</p>
<ol>
<li>Transient faults– Temporary fault in the system that corrects itself with time.</li>
<li>Permanent faults – Faults that cannot be recovered from and generally require replacement of resources.</li>
<li>Intermittent faults – Faults that occur periodically in a system.</li>
</ol>
<p>Faults may affect the availability of the system by bringing down the services or performance of the system functionalities. A fault-tolerant system has the ability to perform its function even in the presence of failures in the system. On the cloud, a fault-tolerant system is often thought of as one that provides services in a consistent manner with lower downtime than the agreed Service Level Agreements (SLAs) allow.</p>
<p><strong>Why is it Important?</strong></p>
<p>Failures in large mission critical systems can result in significant monetary losses to all parties concerned. By the very nature of cloud computing systems having a layered architecture, a fault in one layer of the cloud resources can trigger a failure in other layers above, or hide access to the layers below.</p>
<p>For example a fault in any hardware component of the system can affect normal execution of a Software as a Service application running on a virtual machine using the faulty resources. Faults in a system at any layer have a direct relation to the Service Level Agreements between the providers at each level.</p>
<p><strong>Proactive Measures</strong></p>
<p>Service providers take several measures in order to design the system in a specific way to avoid known issues, or predictable failures.</p>
<p><strong>Profiling and Testing</strong></p>
<p>Load and stress testing cloud resources in order to understand possible causes of failure is essential to ensure the availability of services. Profiling these metrics helps in designing a system that can successfully bear the expected load without any unpredictable behavior.</p>
<p><strong>Over-provisioning</strong></p>
<p>This is the practice of deploying resources in volumes that are larger than the general projected utilisation of the resource at a given time. In situations where the exact needs of the system cannot necessarily be predicted, over-provisioning resources can be an acceptable strategy in order to handle unexpected spikes in loads.</p>
<p>Consider as an example an e-commerce platform that has average consistent load on their servers year round, but during the holiday season the expectation is that the load pattern will spike up rapidly. At these peak times, it is advisable to provision extra resources based on the historical data for peak usage. A rapid rise in traffic is typically difficult to accommodate in a short period of time. As discussed in later sections, there is a time cost associated with scaling dynamically which involves the time consuming steps of detecting a change in the load pattern and provisioning extra resources to accommodate the new load, which will require time. This time delay in adjustment can be enough to overwhelm and at worst crash the system or at best degrade the Quality of Service.</p>
<p>Over-provisioning is also a tactic used to defend against DoS (Denial of Service) or DDoS (Distributed DoS) attacks, which is when attackers generate requests designed to overwhelm a system by throwing large volumes of traffic at them as an attempt to make the system fail. In any attack, it always takes some time for the system detect and take corrective measures. While such analysis of request patterns is being made, the system is already under attack and needs to be able to accommodate the increased traffic until a mitigation strategy can be implemented.</p>
<p><strong>Replication</strong></p>
<p>Critical systems components can be duplicated by using additional hardware and software components to silently handle failures in parts of the system without the entire system failing. Replication has two basic strategies:</p>
<ul>
<li>Active Replication, where all replicated resources are alive concurrently and respond to and process all requests. This means that for any client request, all resources receive the same request, all resources respond to the same request, and ensure that that the order of the requests is maintained to maintain state across all resources.</li>
<li>Passive Replication, where only the primary unit processes requests and secondary units merely maintain state and take over once the primary unit fails. The client is only in contact with the primary resource, which relays the state change to all secondary resources. The disadvantage of passive replication is that there may be either dropped requests or degraded QoS when switching from the primary to the secondary instance.</li>
</ul>
<p>There is also a hybrid strategy that is used, called semi-active, which is very similar to the active strategy with the difference that only the output of the primary resource is exposed to the client. The outputs of the secondary resources are suppressed and logged, and are ready to switch over as soon as a failure of the primary resource occurs. Figure 2.30 illustrates the differences between the replication strategies.</p>
<p><img src="/images/14545529419696.jpg" alt="Figure 2.30 : Replication Strategies"></p>
<p>An important factor to consider in replication is the number secondary resources to use. Although this differs from application to application based on the criticality of the system- there are 3 formal levels of replication:</p>
<ul>
<li>N+1 - This basically means that for an application that needs N nodes to function properly, one extra resource is provisioned as a fail-safe.</li>
<li>2N - At this level one extra node for each node required for normal function is provisioned as fail-safe.</li>
<li>2N+1 - At this level one extra node for each node required for normal function and one additional node overall is provisioned as a fail-safe.</li>
</ul>
<p><strong>Reactive Measures</strong></p>
<p>In addition to predictive measures, systems can take reactive measures and deal with failures as and when they happen:</p>
<p><strong>Checking and Monitoring</strong></p>
<p>All resources are constantly monitored in order to check for unpredictable behavior or loss of resources. Based on the monitoring information, recovery or reconfiguration strategies are designed in order to restart resources or bring up new resources. Monitoring can help in the identification of faults in the systems. Faults that cause a service to be unavailable are called crash faults and those that induce an irregular/incorrect behavior in the system are called byzantine faults.</p>
<p>There are several monitoring tactics that are used to check crash faults within a system. Two of them are:</p>
<ol>
<li>Ping-Echo, where the monitoring service asks each resource for its state and is given a time window to respond</li>
<li>Heartbeat, where each instance sends status to the monitoring service at regular intervals, without any trigger.</li>
</ol>
<p>Monitoring byzantine faults usually depends on the properties of the service being provided. Monitoring systems can check basic metrics like latency, CPU utilization, memory utilization, etc, and check against the expected values to see if the Quality of Service is being degraded. In addition application specific supervision logs are usually kept at each important service execution point and analysed periodically to see that the service is functioning properly at all times or whether there are injected failures in the system.</p>
<p>Checkpoint and Restart<br>Several programming models in the cloud implement checkpoint strategies, whereby state is saved at several stages of execution in order to enable recovery to a last saved checkpoint. In data analytics applications, there are often long running parallel distributed tasks that run on Terabytes of data sets to extract information. Since these tasks are executed in several small execution chunks, each step in the execution of the program can save the overall state of execution as a checkpoint. At points of failures where individual nodes are unable to complete their work, the execution can be restarted from a previous checkpoint. The biggest challenge while identifying valid checkpoints to roll back to is when parallel processes are sharing information. A failure in one of the processes may cause a cascading rollback in another process, as the checkpoints made in that process can be a result of fault in the data shared by the failing process. You will learn more about fault tolerance for programming models in future modules.</p>
<p><strong>Case Studies in Resiliency Testing</strong></p>
<p>Cloud services need to be built with redundancy and fault-tolerance in mind, as no single component of a large distributed system can guarantee 100% availability or uptime.</p>
<p>All failures including failures of dependencies in the same node, rack, data-center or regionally redundant deployments need to be handled gracefully without affecting the entirety of the system. Testing the ability of the system to handle catastrophic failures is important as sometimes even a few seconds of downtime or service degradation can cause hundreds of thousands, if not millions of dollars in revenue loss.</p>
<p>Testing for failures with real traffic needs to be done regularly so that the system is hardened and can cope when an unplanned outage occurs. There are various systems built to test resiliency, one such testing suite is Simian Army built by Netflix.</p>
<p>Simian Army consists of services (Monkeys) in the cloud for generating various kinds of failures, detecting abnormal conditions, and testing ability to survive them. The goal is to keep the cloud safe, secure, and highly available. Some of the Monkeys found in the Simian Army are:</p>
<ol>
<li>Chaos Monkey: A tool that randomly picks a production instance and disables it to make sure the cloud survives common types of failure without any customer impact. Netflix describes Chaos Monkey as “The idea of unleashing a wild monkey with a weapon in your data center (or cloud region) to randomly shoot down instances and chew through cables – all the while we continue serving our customers without interruption”. This kind of testing with detailed monitoring can expose various forms of weaknesses in the system and automatic recovery strategies can be built based on the results.</li>
<li>Latency Monkey: A service that induces delays in between RESTful communication of different clients and servers, simulating service degradation and downtime.</li>
<li>Doctor Monkey: A service that finds instances that are exhibiting unhealthy behaviors (eg: CPU load) and removes them from service. It allows the service owners some time to figure out the reason for the problem and eventually terminates the instance.</li>
<li>Chaos Gorilla: A service that can simulate the loss of an entire AWS availability zone. This is used to test that the services automatically rebalance the functionality among the remaining zones without user-visible impact or manual intervention.</li>
</ol>
<p><img src="/images/14545530896366.jpg" alt=""></p>
<h2 id="Load_Balancing"><a href="#Load_Balancing" class="headerlink" title="Load Balancing"></a>Load Balancing</h2><p>The need for load balancing in computing stems from two basic requirements, a system employs replication to provide high availability and improves performance through parallel processing. High availability is the property of a service that is available for near 100% of the time when any client tries to access the service. The Quality of Service of a particular service generally includes several considerations such as throughput, latency requirements among others.</p>
<p><strong>What is Load Balancing?</strong></p>
<p>The most well-known form of load balancing is “Round robin DNS” employed by many large web services to load balance requests among a number of servers. Specifically, multiple front-end web servers, each with a unique IP address share a DNS name. To balance the number of requests on each of these web servers, large companies like Google maintain and curate a pool of IP addresses associated with a single DNS entry. When a client makes a request (for e.g. to the domain www.google.com), Google’s DNS would select one of the available addresses from the pool and sends it to the client. The simplest strategy employed to dispatch IP addresses is to use a simple round-robin queue, where after each DNS response, the list of addresses are permuted.</p>
<p>Before the advent of the cloud, DNS load balancing was a simple way to tackle the latency of long-distance connections. The dispatcher at the DNS server was programmed to respond with the IP address of the server that was geographically nearest to the client. The simplest schemes to do this tried to respond with the IP address from the pool that was numerically the closest to the IP address of the client. This method, of course, was unreliable, as IP addresses are not distributed in a global hierarchy. Current techniques are more sophisticated and rely on a software mapping of IP addresses to locations based on physical maps of Internet Service Providers (ISPs). Since this is implemented as a costly software lookup, this method yields more accurate results, but is expensive to compute. However, the cost of a slow lookup is amortized since the DNS lookup occurs only when the first connection to a server is made by the client. All subsequent communications happen directly between the client and the server that owns the dispatched IP address. An example of a DNS load balancing scheme is shown in the figure below.</p>
<p><img src="/images/14545531403688.jpg" alt="Figure 2.31: Load Balancing in a Cloud Hosting Environment"></p>
<p>The downside of this method is in the case of a server failure, the switch over to a different IP address is dependent on the configuration of the Time-To-Live(TTL) of the DNS cache. DNS entries are known to be long-living and updates are known to take over a week to propagate over the Internet. Hence, it is difficult to quickly “hide” a server failure from the client. This can be improved by reducing the validity (TTL) of an IP address in the cache, but this occurs at the cost of performance and increasing the number of lookups.</p>
<p>Modern load balancing often refers to the use a dedicated instance (or a pair of instances) that directs incoming traffic to the backend servers. For each incoming request on a specified port, the load balancer redirects the traffic to one of the backend servers based on a distribution strategy. In doing so the load balancer maintains the request metadata including information like Application protocol headers (such as HTTP headers). In this situation, there is no problem of stale information as every request passes through the load balancer.</p>
<p>Though all types of network load balancers will simply forward the user’s information along with any context to the backend servers, when it comes to serving the response back to the client they may employ one of two basic strategies [1] :</p>
<ol>
<li>Proxying - In this approach the load balancer receives the response from the backend and relays it back to the client. The load balancer behaves as a standard web proxy and is involved in both halves of a network transaction, namely forwarding the request to the client and sending back the response.</li>
<li>TCP Handoff - In this approach the TCP connection with the client is handed off to the backend server and therefore the server sends the response directly to the client, without going through the load balancer.</li>
</ol>
<p><img src="/images/14545532403522.jpg" alt="Figure 2.32: TCP Handoff mechanism from the dispatcher to the backend server."></p>
<p><strong>How Does This Help With Availability and Performance?</strong></p>
<p>Load balancing is an important strategy to mask failures in a system. As long as the client of the system is exposed to a single endpoint that is balancing load across several resources, failures in individual resources can be masked from the client by simply servicing the request via a different resource. However it is important to note that the now the Load Balancer is the single point of failure of the service because if it fails for any reason, even if all backend servers are still functioning, no client request will be able to be served. Hence in order to achieve high availability, load balancers are often implemented in pairs.</p>
<p>Load balancing allows a service to distribute workloads across several compute resources in the cloud. Having a single compute instance in the cloud has several limitations. We have discussed earlier the physical limitation on performance, where more resources are required for increasing workloads. By using load balancing, larger volume of workloads can be distributed across multiple resources such that each resource can fulfil its requests independently and in parallel, thereby improving the throughput of the application. This also improves average service times since there are more servers to serve the workload.</p>
<p>Checking and monitoring services are key in enabling the success of load balancing strategies. A load balancer needs to ensure that every request is fulfilled by ensuring each resource node is available, if not then traffic is not directed that specific node. Ping-echo monitoring is one of the most popular tactic in order to check the health of a specific resource node. In addition to health of a node, some load balancing strategies require additional information such as throughput, latency, CPU utilization, etc., in order to evaluate the most appropriate resource to direct traffic.</p>
<p>Load Balancers must often guarantee high availability. The simplest way to do this is to create multiple load balancing instances (each with a unique IP address) and link it to a single DNS address. Whenever a load balancer instance fails for any reason, it is replaced with a new one, and all traffic is passed on to the failover instance with a small performance impact. Simultaneously, a new load balancer instance can be configured to replace the failed one, and the DNS records should be immediately updated.</p>
<p><strong>Strategies for Load Balancing</strong></p>
<p>There are several load balancing strategies in the cloud:</p>
<p><strong>Equitable Dispatching</strong></p>
<p>This is a static approach to load balancing where a simple round-robin algorithm is used to divide the traffic between all nodes evenly and does not take into consideration the utilisation of any individual resource node in the system or the execution time of any request. This approach tries to keep every node in the system busy and is one of the simplest approaches to implement. An important drawback to this approach is that heavy client requests may aggregate and hit the same data centers, causing a few nodes to get overwhelmed while others remain underutilised. However, this requires a very specific load pattern and has low probability of occurring in practice on a large number of clients and servers with fairly uniform connection distribution and capacity. This strategy also makes it difficult to implement caching strategies on the data center that take into account considerations like spatial locality (where you prefetch and cache data near the data that was currently fetched), since the next request made by the same client may end up on a different server.</p>
<p>AWS uses this approach in their ELB (Elastic Load Balancer) offering. AWS ELB provisions load balancers that balance the traffic across the attached EC2 instances. Load balancers are essentially EC2 instances themselves with a service to specifically route traffic. As the resources behind the load balancer are scaled out, the IP addresses of the new resources are updated on the DNS record of the load balancer. This process takes several minutes to complete as it requires both monitoring and provisioning time. This period of scaling in the load balancer to be able to reach a point where it can handle a higher load is referred to as “warming up” the load balancer.</p>
<p>AWS’ ELB load balancers also monitor each resource attached for workload distribution in order to maintain a health check. A ping-echo mechanism is used to ensure all resources are healthy. ELB users can configure the parameters of the health check by specifying the delays and the number of retries.</p>
<p>AWS also supports “sticky sessions”, whereby metadata is added to maintain a shared context between the client’s browser and the load balancer so that all requests from that connection session are passed to the same backend server. This allows a web server to maintain session state transparently. Without sticky sessions, traditional web applications would struggle to function since a login that was completed on one of the web servers would not be valid if the user’s next request would be sent to a different web server.</p>
<p><strong>Hash-based Distribution</strong></p>
<p>This approach tries to ensure that at any point the requests made by a client through the same connection always ends up on the same server. In addition, in order to balance the traffic distribution of requests, it is done in a random order (it is important to note that this is different from round-robin). This has several advantages over the round-robin approach as it helps in session-aware applications where state persistence and caching strategies can be much simpler. It is also less susceptible to traffic patterns that would result in clogging on a single server since the distribution is random, but the risk still exists. In addition since every request needs to be evaluated for connection metadata in order to route to a relevant server, it introduce a small amount of latency to every request.</p>
<p>Azure Load Balancing Services uses such a hash-based distribution mechanism in order to distribute load. This mechanism creates a hash for every request based on – Source IP, Source Port, Destination IP, Destination Port and Protocol Type, in order to ensure that every packet from the same connection always ends up on the same server. The hash function is chosen such that the distribution of connections to servers is fairly random.</p>
<p>Azure provides health-checks via three types of probes - Guest Agent probe (on PaaS VMs), HTTP custom probes and TCP custom probes. All three probes provide health-check for the resource nodes via a ping-echo mechanism.</p>
<p><strong>Other Popular Strategies</strong></p>
<p>There are several other strategies used to balance load across multiple resources. Each of them uses different metrics to gauge the most appropriate resource node for a particular request:</p>
<ol>
<li>Request execution time based strategies - These strategies use a priority scheduling algorithm, whereby request execution times are used in order to judge the most appropriate order of load distribution. The main challenge in using this approach is to predict the execution time of a particular request.</li>
<li>Resource utilization based strategies – These strategies use the CPU utilization on each resource node to balance the utilization across each node. The load balancers maintain an ordered list of resources based on their utilization and thus direct requests to the least loaded node.</li>
</ol>
<p><strong>Other Benefits of Employing a Load Balancer</strong></p>
<p>Having a centralized load balancer lends itself to several strategies that can used to increase the performance of the service. However it is important to note that this strategy only works as long as the load balancer is not under insurmountable load, otherwise the load balancer itself becomes the bottleneck. Some of these are listed below:</p>
<ul>
<li>SSL Offload - Network transactions via SSL have a an extra cost associated with them since they need to have processing for encryption and authentication. Instead of serving all requests via SSL, the client connection to the load balancer can be made via SSL, while redirect requests to each individual server can be made via HTTP. This reduces the load on the servers considerably. Additionally security is maintained as long as the redirect requests are not made over an open network.</li>
<li>TCP Buffering - This is a strategy to offload clients with slow connections on to the load balancer in order to relieve servers that are serving responses to these clients.</li>
<li>Caching - In certain scenarios, the load balancer can maintain a cache for the most popular requests (or request that can be handled without going to the servers, like static content) so that it reduces the load on the servers.</li>
<li>Traffic Shaping - For some applications a load balancer can be used to delay/reprioritize the flow of packets such that traffic can molded to suit the server configuration. This does affect the QoS for some requests, but makes sure that the incoming load can be served.</li>
</ul>
<p><img src="/images/14545534121194.jpg" alt=""></p>
<p><strong>References</strong></p>
<ol>
<li>Aron, Mohit and Sanders, Darren and Druschel, Peter and Zwaenepoel, Willy (2000). “Scalable content-aware request distribution in cluster-based network servers.” Proceedings of the 2000 Annual USENIX technical Conference.</li>
</ol>
<h2 id="Scaling_Resources"><a href="#Scaling_Resources" class="headerlink" title="Scaling Resources"></a>Scaling Resources</h2><p>One of the important advantages of the cloud is the ability scale resources into a system on-demand. Scaling up (provisioning larger resources) or scaling out (provisioning extra resources) can help in reducing the load on a single resources by decreasing utilization as a result of increased capacity or broader distribution of the workload.</p>
<p>Scaling can help in improving performance by increasing the throughput, since a larger number of requests can now be served. This also helps in improving latency since a reduced number of requests are queued during peak loads on a single resource. In addition this can also help in improving the reliability of the system by reducing the resource utilization to be farther away from the breaking point of the resource.</p>
<p>It is important to note that although the cloud enables us to easily provision newer or better resources, cost is always an opposing factor that needs to be considered. Therefore even though it is beneficial to scale up/out, it is also important to recognize when to scale in/down in order to save costs. In an n-tier application it is also essential to pinpoint where the bottlenecks are and which tier to scale, whether it is the data tier or server tier.</p>
<p>Scaling resources is facilitated by load balancing (we discussed this earlier), which helps in masking the scaling aspect of a system by hiding it behind a consistent endpoint.</p>
<p><strong>Scaling Strategies</strong></p>
<p><strong>Horizontal Scaling (Scale Out/In)</strong></p>
<p>Horizontal scaling is a strategy where additional resources can be added into the system or extraneous resources can be removed from the system. This type of scaling is beneficial for the server tier, when the load on the system is unpredictable and fluctuates inconsistently. The nature of the fluctuating load makes it essential to efficiently provision the correct amount of resources to handle the load at all times.</p>
<p>A few considerations that make this a challenging task is the spin up time of an instance, the pricing model of the cloud service provider and the potential loss in revenue from degrading Quality of Service by not scaling out in time. As an example let’s consider the following load pattern (Figure 2.33):</p>
<p><img src="/images/14545536485618.jpg" alt="Figure 2.33: Sample Request Load Pattern"></p>
<p>Let us imagine we are using Amazon’s Web Services, let us also imagine that each unit of time is equivalent to 3 hours of actual time and that we require one server to serve 5k requests. If you consider the load during the time units 16 to 22, there is an enormous fluctuation in the load. We can detect a fall in demand at right around time unit 16 and start to reduce the number of allocated resources. Since we are going from roughly 50k requests to almost 0 requests in the space of 3 hours, academically we can save the cost of 10 instances that would have been up at time 16.</p>
<p>Now let us imagine instead that each time unit is equal to 20 mins of actual time. In that case spinning down the all the resources at time unit 16 only to spin up new resources after 20 mins will actually increase the cost instead of saving, since AWS bills each compute instance on an hourly basis.</p>
<p>In addition to the above two considerations, a service provider will also need to evaluate the losses they will incur by providing degraded QoS during time unit 20, if they only have capacity for 90k requests instead of 100k requests.</p>
<p>The scaling depends on the characteristics of the traffic and its ensuing load generated at a web service. If the traffic follows a predictable pattern, for example based on human behavior such as streaming movies from a web service in the evening, then the scaling can be predictive in order to maintain the QoS. However, in many instances, the traffic cannot be predicted and the scaling systems need to be reactive based on different criteria as the examples above showed.</p>
<p><strong>Vertical Scaling (Scale Up/Down)</strong></p>
<p>There are certain kinds of loads for service providers that are more predictable than others. For example if you know from historical patterns that the number of requests will always be between 10k-15k, then you can comfortably assume that one server that can serve 20k requests will be good enough for the service provider’s purposes. These loads may increase in the future, but as long they increase in a consistent manner, the service can be moved to larger instance, that can serve more requests. This is suitable for small applications that experience a low amount of traffic.</p>
<p>The challenge in vertical scaling is that there is always some switch-over time that can be considered as down time. This is because in order to move all operations from the smaller instance to a larger instance, even if the switch-over time is mere minutes, the Quality of Service does degrade during that interval.</p>
<p>In addition, most cloud providers provide compute resources in increasing compute power by doubling the compute power of a resource. Therefore the granularity in scaling up is not as high as it is in horizontal scaling. So even if the load is predictable and steadily increasing as the popularity of the service increases, many service providers choose to do horizontal scaling instead of vertical scaling.</p>
<p><strong>Considerations for Scaling</strong></p>
<p><strong>Monitoring</strong></p>
<p>Monitoring is one of the most crucial element in scaling resources as it enables you to have metrics that can be used to interpret what parts of the system to scale and when to scale it. Monitoring enables analyzing traffic patterns or resource utilization in order to make an educated assessment to scale resources in order to maximize QoS along with profit.</p>
<p>There are several aspects of resources that are monitored in order to trigger scaling of resources. The most common metric is resource utilization. For example a monitoring service can track the CPU utilization of each resource node and scale resources if the usage is excessive or too low. If for instance the usage for each resource is higher than 95% then it is probably a good idea to add more resources since the system is under a heavy load. Service providers usually decide these trigger points by analyzing the breaking point of resource nodes, when they will start to fail, and mapping out their behavior under various levels of load. Though, for cost reasons, it is important to make maximum utilization out of each resource, it is advisable to leave some room for the Operating System to allow for overhead activities. Similarly if the utilization is significantly below say 50%, then it is possible that not all the resource nodes are required and can be un-provisioned.</p>
<p>In practice, service providers usually monitor a combination of several different metrics of a resource node to evaluate when to scale resources. Some of these include CPU utilization, Memory Consumptions, Throughput, Latency, etc. AWS provides CloudWatch as an additional service that can monitor any AWS resource and provide such metrics. There are other solutions available in the market as well like Nagios.</p>
<p><strong>Statelessness</strong></p>
<p>A stateless service design lends itself to a scalable architecture. A stateless service essentially means that the client request contains all the information necessary to serve a request by the server. The server does not store any client related information on the instance and does store any session related information on the server instance.</p>
<p>Having a stateless service helps in switching resources at will, without any configuration required to maintain the context (state) of the client connection for subsequent requests. If the service is stateful, then the scaling of resources needs to implement a strategy to transfer the context from the existing node configuration to the new node configuration. However there are techniques to implement stateful services like maintaining a network cache such as Memcached where the context is shared across the servers.</p>
<p><strong>Deciding What to Scale</strong></p>
<p>Depending on the nature of the service, different resources need to be scaled depending on the requirement. For the server tier, as the workloads increase, depending on the type of application, it may increase the resource contention for either CPU, memory, network bandwidth, or all of the above. Monitoring the traffic allows us to identify which resource is getting constrained and appropriately scale that specific resource. Cloud Service Providers do not necessarily provide scalability granularity to only scale compute or memory, but they do provide different types of compute instances that specifically cater to compute heavy or memory heavy loads. So for example for an application that has memory intensive workloads, it would be more advisable to scale up the resources to memory optimized instances, or for applications that need to serve a large number of requests that are not necessarily compute heavy or memory heavy, scaling out multiple standard compute instances might be a better strategy.</p>
<p>Increasing hardware resources may not always be the best solution for increasing the performance of a service. Increasing the efficiency of the algorithms used within the service can also help in reducing resource contention and improve utilization, removing the need to scale physical resources.</p>
<p><strong>Scaling the Data Tier</strong></p>
<p>In data-oriented applications, where there is a high number of reads and writes (or both) to a database or storage system, the round trip time for each request is often limited by the hard disk I/O read and write times. Larger instances allow for higher I/O performance for reads and writes which can improve seek times on the hard disk can in turn result in a large improvement in the latency of the service. Whereas having multiple data instances in the data tier can improve the reliability and availability of the application by providing failover redundancies. Replicating data across multiple instances has additional advantages in reducing network latency if the client is served by a data center physically closer to it. Sharding or partitioning of the data across multiple resources is another horizontal data scaling strategy where instead of simply replicating the data across multiple instances, data is partitioned into several partitions and stored across multiple data servers.</p>
<p>The additional challenge when it comes to scaling the data tier is that of maintaining all the three facets of Consistency (a read operation on all replicas is the same), Availability (reads and writes always succeed) and Partition Tolerance (guaranteed properties in the system are maintained when failures prevent communication across nodes) in the system. This is often referred to as the CAP theorem which in short states that within a distributed database system, it is very difficult to obtain all three properties completely and thus may at most exhibit a combination of two of the properties. You will learn more about database scaling strategies and the CAP theorem in future modules.</p>
<p><img src="/images/14545537883625.jpg" alt=""></p>
<h2 id="Dealing_with_Tail_Latency"><a href="#Dealing_with_Tail_Latency" class="headerlink" title="Dealing with Tail Latency"></a>Dealing with Tail Latency</h2><p>We have already discussed several optimization techniques used on the cloud to reduce latency. Some of the measures we studied include scaling resources horizontally or vertically and using a load balancer to route requests to the nearest available resources. This page delves more deeply into why, in a large data center or cloud application, it is important to minimize latency for all requests, and not just optimize the general case. We will study how even a few high-latency outliers can significantly degrade the observed performance of a large system. This page also covers various techniques to create services that provide predictable low-latency responses even if the individual components do not guarantee this. This is a problem that is especially significant for interactive applications where the desired latency for an interaction is below 100ms.</p>
<p><strong>What is Tail Latency?</strong></p>
<p>Most cloud applications are large, distributed systems often rely on parallelization as a source of reducing latency. A common technique is to fan-out a request received at a root node (for e.g. a front-end web server) to many leaf nodes (back-end compute servers). The performance improvement is driven both by the parallelism of the distributed computation, and also by the fact that extremely expensive data-moving costs are avoided. We simply move the computation to where the data is stored. Of course, each leaf node concurrently operates on hundreds or even thousands of parallel requests.</p>
<p><img src="/images/14545538640150.jpg" alt="Figure 2.34: Latency due to scale out"></p>
<p>Consider the example of searching for a movie on Netflix. As a user begins to type in the search box, this will generate several parallel events from the root web server, which include at least the following requests:</p>
<ol>
<li>to the autocomplete engine to actually predict the search being made based on past trends and the user’s profile,</li>
<li>to the correction engine which finds errors in the typed query based on a constantly adapting language model,</li>
<li>individual search results for each of the component words of a multi-word query, which must be combined together based on the rank and relevance of the movies,</li>
<li>additional post-processing and filtering of results to meet the user’s “safe-search” preferences</li>
</ol>
<p>Such examples are extremely common. A single Facebook request is known to contact thousands of memcached servers, whereas a single Bing search often contacts over ten thousand index servers.</p>
<p>Clearly, the need for scale has led to a large fan-out at the back-end for each individual request serviced by the front-end. For services that are expected to be “responsive” to retain their user base, heuristics show that responses are expected within 100 ms. As the number of servers required to resolve a query increases, the overall time often depends on the worst performing response from a leaf node to a root node. Assuming that all leaf nodes must complete executing before a result can be returned, the overall latency must always be greater than the latency of the single slowest component.</p>
<p>Like most stochastic processes, the response time of a single leaf node can be expressed as a distribution. Decades of experience have shown that in the general case, most (&gt;99%) requests of a well-configured cloud system will execute extremely quickly. But often, there are very few outliers on a system that execute extremely slowly.</p>
<p><img src="/images/14545540355724.jpg" alt="Figure 2.35: Tail Latency Example 5"></p>
<p>Consider a system where all leaf nodes have an average response time of 1 ms, but there is probability of a 1% that the response time is greater than 1000 ms (one second). If each query is handled by only a single leaf node, the probability of the query taking longer than one second is also 1%. However, as we increase the number of nodes to 100, the probability that the query will complete within one second drops to 36.6%, which means that there is a 63.4% chance that the query duration will be determined by the tail (lowest 1%) of the latency distribution.</p>
<p><img src="/images/14545540699473.jpg" alt=""></p>
<p>If we simulate this for a variety of cases, we see that as the number of servers increase, the impact of a single slow query is more pronounced (notice that the graph below is monotonically increasing). Also, as the probability of these outliers decreases from 1% to 0.01%, the system is substantially lower.</p>
<p><img src="/images/14545540831998.jpg" alt="Figure 2.36: Response time probability and the 50%ile, 95%ile and 99%ile latency of requests in a recent study."></p>
<p>Just like we designed our applications to be fault-tolerant to deal with resource reliability problems, it should be clear now why it is important for applications to be “tail-tolerant”. To be able to do this, we must understand the sources of these long performance variabilities and identify mitigations where possible and workarounds where not.</p>
<p><strong>Variability in the Cloud: Sources and Mitigations</strong></p>
<p>To resolve the response time variability that leads to this tail latency problem, we must understand the sources of performance variability [1] .</p>
<ol>
<li>The use of shared resources: Many different VMs (and applications within those VMs) contend for a shared pool of compute resources and in rare cases it is possible that this contention leads to low-latency for some requests. For critical tasks, it may make sense to use dedicated instances and periodically run benchmarks when idle, to ensure that it behaves correctly.</li>
<li>Background daemons and maintenance: We have already spoken about the need for background processes to create checkpoints, backups, update logs, collect garbage and handle resource clean up. However, these can degrade the performance of the system while executing. To mitigate this, it is important to synchronize disruptions due to maintenance threads to minimize the impact on the flow of traffic. This will cause all variation to occur in a short, well-known window rather than randomly over the lifetime of the application.</li>
<li>Queueing: Another common source of variability is the burstiness of traffic arrival patterns [1] . This variability is exacerbated if the OS uses a scheduling algorithm other than FIFO. Linux systems often schedule threads out-of-order to optimize the overall throughput and maximize utilization of the server. However, studies have found that using FIFO scheduling in the OS reduces tail latency but may also reduce the overall throughput of the system.</li>
<li>All-to-all incast: The pattern shown in Fig 2.34 above is known as all-to-all communication. Since most network communication is over TCP, this leads to thousands of simultaneous requests and responses between the front-end web server and all the back-end processing nodes. This is an extremely bursty pattern of communication and often leads to a special kind of congestion failure known TCP incast collapse [1] [2] . The intense sudden response from thousands of servers leads to many packet drops and retransmissions, eventually causing a network avalanche of traffic for packets of data that are very small. Large data centers and cloud applications would often need to use custom network drivers to dynamically adjust the TCP receiving window and the retransmission timer. Routers may also be configured to drop traffic that exceeds a specific rate and reduce the size of the sending.</li>
<li>Power and temperature management: Finally, variability is a byproduct of other cost reduction techniques like using idle states or CPU frequency down-scaling. A processor may often spend a non-trivial amount of time scaling up from an idle state. Turning off such cost optimizations lead to higher energy usage and costs, but lower variability. This is less of a problem in the public cloud as pricing models rarely consider internal utilization metrics of customers’ resources.</li>
</ol>
<p>Experiments conducted on AWS EC2 instances have found that the variability of such systems are much worse on the public cloud [3] , typically due to imperfect performance isolation of virtual resources and the shared processor. This is exacerbated if many latency-sensitive jobs are executed on the same physical node as CPU-intensive jobs.</p>
<p><strong>Living with Variability: Engineering Solutions</strong></p>
<p>Many of the sources of variability above have no fool-proof solution. Hence, instead of trying to resolve all of the sources that inflate the latency tail, cloud applications must be designed to be tail-tolerant. This, of course, is similar to the way that we design applications to be fault-tolerant since we cannot possibly hope to fix all possible faults. Some of the common techniques to deal with this variability are:</p>
<ol>
<li>“Good enough” results: Often, when waiting to receive results from 1000s of nodes, the importance of any single result may be assumed to be quite low. Hence, many applications may choose to simply respond to the users with results that arrive within a particular, short latency window and discard the rest.</li>
<li>Canaries: Another alternative that is often used for rare code paths is to test a request on a small subset of leaf nodes in order to test if it causes a crash or failure that can impact the entire system. The full fan-out query is only generated if the canary does not cause a failure. This is akin to sending a canary (bird) into a coal mine to test if it is safe for humans.</li>
<li>Latency-induced probation and health checks: Of course, a bulk of the requests to a system are too common to test using a canary. Such requests are more likely to have a long-tail if one of the leaf nodes is performing poorly. To counter this, the system must periodically monitor the health and latency of each leaf node and not route requests to nodes that demonstrate low performance (due to maintenance or failures).</li>
<li>Differential QoS: Separate service classes can be created for interactive requests, allowing them to take priority in any queue. Latency-insensitive applications can tolerate longer waiting time for their operations.</li>
<li>Request Hedging: This is a simple solution to reduce the impact of variability by forwarding the same request to multiple replicas and using the response that arrives first. Of course, this can double or triple the amount of resources required. To reduce the number of hedged requests, the second request may only be sent if the first response has been pending for greater than the 95th-percentile of the expected latency for that request. This causes the extra load to be only about 5%, but reduces the latency tail significantly (in the typical case shown in Fig 2.35, where the 95th-percentile latency is much lower than the 99th-percentile latency).</li>
<li>Speculative execution and selective replication: Tasks on nodes that are particularly busy can be speculatively launched on other underutilized leaf nodes. This is especially effective if a failure in a particular node causes it to be particularly overloaded.</li>
<li>UX-based solutions: Finally, the delay can be intelligently hidden from the user by having a well designed user interface which reduces the sensation of delay experienced by a human user. Techniques to do this may include the use of animations, showing early results or engaging the user by sending relevant messages.</li>
</ol>
<p>Using these techniques, it is possible to significantly improve the experience of the end-users of a cloud application to the peculiar problem of a long tail.</p>
<p><img src="/images/14545544252879.jpg" alt=""></p>
<p><strong>References</strong></p>
<ol>
<li>Li, J., Sharma, N. K., Ports, D. R., &amp; Gribble, S. D. (2014). “Tales of the Tail: Hardware, OS, and Application-Level Sources of Tail Latency..” Proceedings of the ACM Symposium on Cloud Computing. ACM.</li>
<li>Wu, Haitao and Feng, Zhenqian and Guo, Chuanxiong and Zhang, Yongguang (2013). “ICTCP: Incast Congestion Control for TCP in Data-Center Networks.” IEEE/ACM Transactions on Networking (TON). IEEE Press.</li>
<li>Xu, Yunjing and Musgrave, Zachary and Noble, Brian and Bailey, Michael (2013). “Bobtail: Avoiding Long Tails in the Cloud.” 10th USENIX Conference on Networked Systems Design and Implementation. USENIX Association.</li>
<li>Dean, Jeffrey and Barroso, Luiz Andr{\’e} (2013). “The tail at scale.” Communications of the ACM. ACM.</li>
<li>Tene, Gil (2014). “Understanding Latency - Some Key Lessons and Tools.” QCon London.</li>
</ol>
<h2 id="Economics_for_Cloud_Applications"><a href="#Economics_for_Cloud_Applications" class="headerlink" title="Economics for Cloud Applications"></a>Economics for Cloud Applications</h2><p>CSPs are taking great pains to attract users from their traditional deployments. Public IaaS cloud prices have been steadily and steeply falling ever since the launch of their services. On average, for most major CSPs, prices have fallen by 20-30% per year since 2013.</p>
<p><img src="/images/14545544739052.jpg" alt="Figure 2.37: Average cost reduction of CSP Services"></p>
<p>However, despite these decreasing prices, cloud adoption still must be done with care. To truly gain the cost benefits of the cloud, it is important to understand, budget, plan, monitor and carefully analyze your usage. Also, it is difficult to choose between CSPs for individual use cases, since there is no standard way for CSPs to package resources, nor do they always follow the same pricing models.</p>
<p><strong>Pricing Models</strong></p>
<p>Cloud providers generally charge for resources based on one of the following 3 types of parameters:</p>
<ol>
<li>Time Based: Resources are charged based on the amount of time they are provisioned to the user. For example, you pay a certain amount per hour/day/month/year to have a virtual machine running on an IaaS cloud. The granularity of the charging period varies from cloud provider to cloud provider. Amazon, for example charges users per hour on a non-prorated basis.</li>
<li>Capacity Based: Users are charged based on the amount of a particular resource that is utilized or consumed. This is a popular charging model for cloud storage systems. For example, users are charged a certain amount for storing a gigabyte on cloud object storage systems such as S3 and Azure Blobs.</li>
<li>Performance Based: In many cloud providers, users can select a higher performance level for resources by paying a higher rate. For virtual machines, larger, more powerful machines with more CPU, Memory and Disk capacity can be provisioned at a higher hourly rate.</li>
</ol>
<p>Based on these charging parameters, CSPs, such as AWS, use one of the following common pricing models:</p>
<ol>
<li>On-demand / pay-as-you-go pricing: This is generally the most expensive pricing model for long-term usage. Payments are made for a very short period of usage (generally metered in minutes or hours). The advantages are that there is no need for a long-term contract, making it very flexible to scale in and out based on the current need. Although not common, it is possible for CSPs to increase costs during high demand and decrease it during low demand. This is a great model for service providers, as well as for cloud users who are just beginning to use the cloud.</li>
<li>Reserved instances / Subscription-based pricing: Instead of paying an hourly or per-minute rate, a user can choose to pre-pay and reserve a resource for a fairly long period of time (weeks or months). This often leads to significantly high markdowns (20-50%) but requires a long-term commitment. Within reserved pricing models, the payment schemes can vary from prepaid to contractually obliged periodic payments.</li>
<li>Spot pricing: Spot pricing is a way for CSPs to deal with excess unutilized capacity by offering it for sale at significantly lower prices than on-demand resources. For example,AWS often offers markdowns of 80-90% on spot EC2 resources. The prices are determined by a user auction, where users bid the maximum amount that they are willing to pay for a resource. The biggest downside is that the resources can often be terminated at any time if the spot price increases beyond the actual bid price. Spot resources are ideal for short-running, non-critical jobs that can be executed speculatively.</li>
</ol>
<p>Generally reserved instances should be used to meet the base requirements of the system. If an application needs 2 instances 80% of the time, 3 instances 15% of the time and 4 instances 5% of the time, you would generally reserve 2 instances for the lifetime of the application and scale out either using on-demand or spot instances. As mentioned, on-demand instances should only be chosen while scaling out if the application is business critical or if the differential between the on-demand price and the current spot price is offset by the risk of sudden termination. This is often a business-case specific decision.</p>
<p><strong>How to Optimize Cost Utilization</strong></p>
<p>To use the cloud in a cost-effective manner, enterprises must develop a mature process for choosing the resources to deploy, monitor and visualize usage, as well as a clear mechanism to identify waste and optimize utilization.</p>
<p><img src="/images/14545545777962.jpg" alt="Figure 2.38: Cost Optimization Process"></p>
<p>Before considering cost requirements, an organization must plan the amount of work that it is capable of completing in a given period based on fixed resources like the amount of staff, while dealing with physical constraints like inventory management, overhead due to transportation, material handling etc. The provisioning of IT resources must be designed to meet or exceed the physical capacity of the organization. This is extremely important, because the elasticity provided by the cloud tempts development teams to simply add resources as needed, without considering the cost implications of their decisions.</p>
<p>The first step when attempting to reduce expenditure on the cloud is to match resource types with the actual requirement for the application. This may mean selecting between EC2 VMs with different memory configurations, or number of cores. There is no simple way to do this, apart from testing and benchmarking the application across different resource types.</p>
<p>Even if an application performs better on a more expensive resource class, it is important to verify if the performance improvement is proportional to the cost increase. For example, if there is a 1.2x improvement in an application using a VM that is 7.5x more expensive than the base, it might make more sense to horizontally scale out the base resource to improve the performance.</p>
<p>It is important to build a monitoring and visualization system to monitor the various resources being used. The monitoring system must be designed to trigger scaling events in response to observed patterns of overload or idleness. Often, infrastructure teams choose to scale up or out aggressively, while they scale down or in more conservatively. Though this approach is more expensive in terms of resource provisioning, it theoretically provides a higher Quality of Service than operating near peak capacity all of the time.</p>
<p>That being said, organizations often underestimate the need to scale down and terminate rarely used resources. When planning to run different components of an application, it is important to categorize the utilization into different bins, based on the approximate duration for which it will be used. For instance, any jobs that are run for a short time on a nightly or weekly basis should not use resources 24/7. Idle resources should also be flagged and terminated (based on certain rules) by the monitoring system.</p>
<p>An important technique that ties into cost optimization is that of tagging resources. Tagging is the process of assigning labels to resources that allow them to be identified by monitoring and analytics tools. Tagging also enables custom rules to be defined per-tag, including access control lists to resources, billing alerts and specific scaling policies. Commonly used tags specify the owner (user or group) of a particular resource, the environment to which it belongs (e.g. production, backup, staging, testing), the cost center in charge of paying the bill, etc. When analyzing expenditure, this enables the generation of grouped views based on particular applications, as well as on specific development or testing teams.</p>
<p><strong>Case Study: Netflix’s Ice System</strong></p>
<p>As one of the largest AWS customers in the world, it is crucial for Netflix to have clear visibility about their expenditure. To support this requirement, they use an internally designed tool known as Ice. Ice relies on tags and resource metadata to build a dynamic dashboard that allows resources to be grouped by user, team, region, type, pricing model or any custom tag. It also supports the amortization of one time costs such as reserved instances over the lifetime of the resource. All of the data is periodically generated using AWS Billing APIs.</p>
<p><img src="/images/14545546158885.jpg" alt="Figure 2.39: Netflix Ice"></p>
<p>This tool has been released as a part of their Open Source initiative. Many companies (as well as CMU’s cloud computing course staff) use Ice or variants to gain insights into AWS usage and expenditure. It helps influence if any future resources should be reserved for the long-term (at cheaper prices), and identifies users or teams who overspend. All large cloud deployments should follow a similar process of planning &amp; budgeting, monitoring &amp; visualization, and forecasting &amp; optimization.</p>
<h2 id="Programming_the_Cloud_Summary"><a href="#Programming_the_Cloud_Summary" class="headerlink" title="Programming the Cloud Summary"></a>Programming the Cloud Summary</h2><ul>
<li>Cloud applications must take precautions to ensure that they use resources that help them meet their bandwidth and latency requirements, as well as follow security best practices.</li>
<li>Applications deployed on the cloud are often subject to performance variance due to the shared nature of the cloud.</li>
<li>The cloud makes it easy to maintain several different environments apart from production. Applicaton pipelines are maintained using code repository and version control systems and automated using Continuous Integration tools.</li>
<li>Planning for failure is crucial. Redundancy is the key technique used to ensure resilience- often ensure using replicas deployed across availability zones and regions.</li>
<li>Redundant resources are generally monitored and accessed using a central Highly Available load balancer. High Availability is ensure by switching over to a backup instance when one fails.</li>
<li>Companies like Netflix and Facebook inject large random (or planned) failures in their data centers and cloud operations to test for fault tolerance.</li>
<li>Load Balancing also supports horizontal scaling, whereby more identical resources can be thrown at a problem. The other type of scaling is vertical- where the size or capacity of existing resources is increased.</li>
<li>Horizontal scaling across too many nodes leads to the problem of Tail Latency, where the performance of the application is determined by it’s slowest component. This is both due to variability of performance on the cloud and also because applications with a large fan-out trigger bursts of activity at each stage.</li>
<li>Finally, the lack of standardization and high competitiveness of the cloud market leads to interesting opportunties and challenges to minimize costs.</li>
</ul>
</div><div class="tags"><a href="/tags/CMU/">CMU</a><a href="/tags/云计算/">云计算</a><a href="/tags/讲义/">讲义</a></div><div class="post-nav"><a href="/2016/01/16/cc-7/" class="pre"><i class="icon-previous">云计算 第 7 课 AWS 动手玩</i></a><a href="/2016/01/15/cc-6/" class="next">云计算 第 6 课 AWS API<i class="icon-next"></i></a></div><div data-thread-key="2016/01/16/cc-0/" data-title="云计算 第 0 课 阅读材料" data-url="http://wdxtub.com/2016/01/16/cc-0/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/01/16/cc-0/" data-title="云计算 第 0 课 阅读材料" data-url="http://wdxtub.com/2016/01/16/cc-0/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search" class="search-form-input"/><input type="hidden" name="sitesearch" value="http://wdxtub.com"/></form></div><div class="widget"><div class="widget-title">分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Game/">Game</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Gossip/">Gossip</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Memory/">Memory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Movie/">Movie</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading/">Reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Story/">Story</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Technique/">Technique</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Traveling/">Traveling</a></li></ul></div><div class="widget"><div class="widget-title">标签</div><div class="tagcloud"><a href="/tags/启程/" style="font-size: 15px;">启程</a> <a href="/tags/英雄/" style="font-size: 15px;">英雄</a> <a href="/tags/硬币/" style="font-size: 15px;">硬币</a> <a href="/tags/今何在/" style="font-size: 15px;">今何在</a> <a href="/tags/大圣归来/" style="font-size: 15px;">大圣归来</a> <a href="/tags/氐惆/" style="font-size: 15px;">氐惆</a> <a href="/tags/回头无岸/" style="font-size: 15px;">回头无岸</a> <a href="/tags/电影/" style="font-size: 15px;">电影</a> <a href="/tags/动画/" style="font-size: 15px;">动画</a> <a href="/tags/孙悟空/" style="font-size: 15px;">孙悟空</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a> <a href="/tags/记录/" style="font-size: 15px;">记录</a> <a href="/tags/博客/" style="font-size: 15px;">博客</a> <a href="/tags/思考/" style="font-size: 15px;">思考</a> <a href="/tags/论道/" style="font-size: 15px;">论道</a> <a href="/tags/精选/" style="font-size: 15px;">精选</a> <a href="/tags/早起/" style="font-size: 15px;">早起</a> <a href="/tags/效率/" style="font-size: 15px;">效率</a> <a href="/tags/抱怨/" style="font-size: 15px;">抱怨</a> <a href="/tags/生活/" style="font-size: 15px;">生活</a> <a href="/tags/自尊/" style="font-size: 15px;">自尊</a> <a href="/tags/CMU/" style="font-size: 15px;">CMU</a> <a href="/tags/周记/" style="font-size: 15px;">周记</a> <a href="/tags/面试/" style="font-size: 15px;">面试</a> <a href="/tags/改变/" style="font-size: 15px;">改变</a> <a href="/tags/读书/" style="font-size: 15px;">读书</a> <a href="/tags/教育/" style="font-size: 15px;">教育</a> <a href="/tags/家庭/" style="font-size: 15px;">家庭</a> <a href="/tags/环境/" style="font-size: 15px;">环境</a> <a href="/tags/回合制/" style="font-size: 15px;">回合制</a> <a href="/tags/格子/" style="font-size: 15px;">格子</a> <a href="/tags/模拟经营/" style="font-size: 15px;">模拟经营</a> <a href="/tags/COC/" style="font-size: 15px;">COC</a> <a href="/tags/美剧/" style="font-size: 15px;">美剧</a> <a href="/tags/苏州/" style="font-size: 15px;">苏州</a> <a href="/tags/旅行/" style="font-size: 15px;">旅行</a> <a href="/tags/醒醒/" style="font-size: 15px;">醒醒</a> <a href="/tags/笔记/" style="font-size: 15px;">笔记</a> <a href="/tags/知识管理/" style="font-size: 15px;">知识管理</a> <a href="/tags/英语学习/" style="font-size: 15px;">英语学习</a> <a href="/tags/透析/" style="font-size: 15px;">透析</a> <a href="/tags/蝴蝶效应/" style="font-size: 15px;">蝴蝶效应</a> <a href="/tags/步行/" style="font-size: 15px;">步行</a> <a href="/tags/Kindle/" style="font-size: 15px;">Kindle</a> <a href="/tags/口译/" style="font-size: 15px;">口译</a> <a href="/tags/徐州/" style="font-size: 15px;">徐州</a> <a href="/tags/原创/" style="font-size: 15px;">原创</a> <a href="/tags/童话/" style="font-size: 15px;">童话</a> <a href="/tags/感情/" style="font-size: 15px;">感情</a> <a href="/tags/心智/" style="font-size: 15px;">心智</a> <a href="/tags/时间/" style="font-size: 15px;">时间</a> <a href="/tags/自我/" style="font-size: 15px;">自我</a> <a href="/tags/感恩节/" style="font-size: 15px;">感恩节</a> <a href="/tags/朋友/" style="font-size: 15px;">朋友</a> <a href="/tags/纽约/" style="font-size: 15px;">纽约</a> <a href="/tags/成绩/" style="font-size: 15px;">成绩</a> <a href="/tags/多看/" style="font-size: 15px;">多看</a> <a href="/tags/访谈/" style="font-size: 15px;">访谈</a> <a href="/tags/游戏/" style="font-size: 15px;">游戏</a> <a href="/tags/英雄联盟/" style="font-size: 15px;">英雄联盟</a> <a href="/tags/笔试/" style="font-size: 15px;">笔试</a> <a href="/tags/改进/" style="font-size: 15px;">改进</a> <a href="/tags/旅途/" style="font-size: 15px;">旅途</a> <a href="/tags/心情/" style="font-size: 15px;">心情</a> <a href="/tags/将军/" style="font-size: 15px;">将军</a> <a href="/tags/回忆/" style="font-size: 15px;">回忆</a> <a href="/tags/清醒思考的艺术/" style="font-size: 15px;">清醒思考的艺术</a> <a href="/tags/明智行动的艺术/" style="font-size: 15px;">明智行动的艺术</a> <a href="/tags/科幻/" style="font-size: 15px;">科幻</a> <a href="/tags/预言/" style="font-size: 15px;">预言</a> <a href="/tags/顽皮狗/" style="font-size: 15px;">顽皮狗</a> <a href="/tags/PS4/" style="font-size: 15px;">PS4</a> <a href="/tags/写作/" style="font-size: 15px;">写作</a> <a href="/tags/技术/" style="font-size: 15px;">技术</a> <a href="/tags/开篇/" style="font-size: 15px;">开篇</a> <a href="/tags/新参者/" style="font-size: 15px;">新参者</a> <a href="/tags/小说/" style="font-size: 15px;">小说</a> <a href="/tags/死亡/" style="font-size: 15px;">死亡</a> <a href="/tags/Swift-千金方/" style="font-size: 15px;">Swift 千金方</a> <a href="/tags/iOS/" style="font-size: 15px;">iOS</a> <a href="/tags/斯坦福-CS193p/" style="font-size: 15px;">斯坦福 CS193p</a> <a href="/tags/教程/" style="font-size: 15px;">教程</a> <a href="/tags/闭包/" style="font-size: 15px;">闭包</a> <a href="/tags/技巧/" style="font-size: 15px;">技巧</a> <a href="/tags/Apple/" style="font-size: 15px;">Apple</a> <a href="/tags/Swift/" style="font-size: 15px;">Swift</a> <a href="/tags/Overview/" style="font-size: 15px;">Overview</a> <a href="/tags/概览/" style="font-size: 15px;">概览</a> <a href="/tags/ARC/" style="font-size: 15px;">ARC</a> <a href="/tags/扩展/" style="font-size: 15px;">扩展</a> <a href="/tags/协议/" style="font-size: 15px;">协议</a> <a href="/tags/泛型/" style="font-size: 15px;">泛型</a> <a href="/tags/访问控制/" style="font-size: 15px;">访问控制</a> <a href="/tags/运算符重载/" style="font-size: 15px;">运算符重载</a> <a href="/tags/类型/" style="font-size: 15px;">类型</a> <a href="/tags/元组/" style="font-size: 15px;">元组</a> <a href="/tags/运算符/" style="font-size: 15px;">运算符</a> <a href="/tags/控制/" style="font-size: 15px;">控制</a> <a href="/tags/函数/" style="font-size: 15px;">函数</a> <a href="/tags/属性/" style="font-size: 15px;">属性</a></div></div><div class="widget"><div class="widget-title">最新文章</div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/02/03/xiao-fang/">小方</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/02/csapp-lab2/">深入理解计算机系统 习题课 2 Bomblab</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/01/cc-11/">云计算 第 11 课 Horizontal Scaling and Advanced Resource Scaling</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/01/fastcode-2/">How to Write Fast Code 第 2 课 Multicore 编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/01/fastcode-1/">How to Write Fast Code 第 1 课 背景知识</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/01/fastcode-0/">How to Write Fast Code 第 0 课 往年笔记与问题集</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/01/game-import/">游戏引进评估的一些思考</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/31/deadline-and-unplug/">Deadline 与不插电</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/31/sad-r3/">软件架构与设计 习题课 3 从不同视角描述系统</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/29/die-internationale/">第三周 - Die Internationale</a></li></ul></div><div class="widget"><div class="comments-title">最近评论</div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title">友情链接</div><ul></ul><a href="http://wdxtub.com/library/" title="我的笔记" target="_blank">我的笔记</a><ul></ul><a href="http://wdxtub.com/bookclips/" title="我的书摘" target="_blank">我的书摘</a><ul></ul><a href="http://wdxtub.com/interview/" title="刷题笔记" target="_blank">刷题笔记</a></div></div></div></div><div id="footer">© <a href="/." rel="nofollow">小土刀.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><a id="rocket" href="#top" class="show"></a><script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/totop.js" type="text/javascript"></script><script src="/js/fancybox.pack.js" type="text/javascript"></script>
<script src="/js/jquery.fancybox.js" type="text/javascript"></script><link rel="stylesheet" href="/css/jquery.fancybox.css" type="text/css"><script>var duoshuoQuery = {short_name:'wdxblog'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div><!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body></html>