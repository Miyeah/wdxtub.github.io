<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="一个逗比的碎碎念"><title>云计算 第 0 课 阅读材料 | 小土刀</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">云计算 第 0 课 阅读材料</h1><a id="logo" href="/.">小土刀</a><p class="description">Agony is my triumph</p></div><div id="nav-menu"><a href="/." class="current"><i class="icon-home"> 首页</i></a><a href="/about/"><i class="icon-guestbook"> 技术</i></a><a href="/life/"><i class="icon-about"> 生活</i></a><a href="/archives/"><i class="icon-archive"> 归档</i></a><a href="/atom.xml"><i class="icon-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post post-page"><h1 class="post-title">云计算 第 0 课 阅读材料</h1><div class="post-meta">2016-01-16 | <span class="categories">分类于<a href="/categories/Technique/"> Technique</a></span></div><span data-thread-key="2016/01/16/cc-0/" class="ds-thread-count"></span><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction_to_Cloud_Computing"><span class="toc-number">1.</span> <span class="toc-text">Introduction to Cloud Computing</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Computing_Overview"><span class="toc-number">1.1.</span> <span class="toc-text">Cloud Computing Overview</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition_of_Cloud_Computing"><span class="toc-number">1.1.1.</span> <span class="toc-text">Definition of Cloud Computing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pre-Cloud_Computing_Domains_and_Applications"><span class="toc-number">1.1.2.</span> <span class="toc-text">Pre-Cloud Computing Domains and Applications</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Evolution_of_Cloud_Computing"><span class="toc-number">1.1.3.</span> <span class="toc-text">Evolution of Cloud Computing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cloud_Building_Blocks_and_Service_Models"><span class="toc-number">1.1.4.</span> <span class="toc-text">Cloud Building Blocks and Service Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cloud_Deployment_Models"><span class="toc-number">1.1.5.</span> <span class="toc-text">Cloud Deployment Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Popular_Cloud_Stacks"><span class="toc-number">1.1.6.</span> <span class="toc-text">Popular Cloud Stacks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cloud_Use_Cases"><span class="toc-number">1.1.7.</span> <span class="toc-text">Cloud Use Cases</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-number">1.1.8.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Economics_2C_Benefits_2C_Risks_2C_Challenges_and_Solutions"><span class="toc-number">1.2.</span> <span class="toc-text">Economics, Benefits, Risks, Challenges and Solutions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Benefits_2C_Risks_2C_and_Challenges_of_Cloud_Computing"><span class="toc-number">1.2.1.</span> <span class="toc-text">Benefits, Risks, and Challenges of Cloud Computing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Business_Case_for_Cloud_Computing__28for_Users_29"><span class="toc-number">1.2.2.</span> <span class="toc-text">Business Case for Cloud Computing (for Users)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Economics_of_Cloud_Computing__28for_Providers_29"><span class="toc-number">1.2.3.</span> <span class="toc-text">Economics of Cloud Computing (for Providers)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Service_Level_Agreements_and_Objectives"><span class="toc-number">1.2.4.</span> <span class="toc-text">Service Level Agreements and Objectives</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cloud_Security_-_Threats"><span class="toc-number">1.2.5.</span> <span class="toc-text">Cloud Security - Threats</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cloud_Security_-_Control__26amp_3B_Auditing"><span class="toc-number">1.2.6.</span> <span class="toc-text">Cloud Security - Control & Auditing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-1"><span class="toc-number">1.2.7.</span> <span class="toc-text">Summary</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Cloud_Infrastructure"><span class="toc-number">2.</span> <span class="toc-text">Cloud Infrastructure</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Data_Center_Trends"><span class="toc-number">2.1.</span> <span class="toc-text">Data Center Trends</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction_to_Data_Centers"><span class="toc-number">2.1.1.</span> <span class="toc-text">Introduction to Data Centers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition_and_Origins"><span class="toc-number">2.1.2.</span> <span class="toc-text">Definition and Origins</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Size_2C_Density_and_Efficiency_Growth"><span class="toc-number">2.1.3.</span> <span class="toc-text">Size, Density and Efficiency Growth</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Challenges_in_Cloud_Data_Centers"><span class="toc-number">2.1.4.</span> <span class="toc-text">Challenges in Cloud Data Centers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-2"><span class="toc-number">2.1.5.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Data_Center_Components"><span class="toc-number">2.2.</span> <span class="toc-text">Data Center Components</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#IT_Equipment"><span class="toc-number">2.2.1.</span> <span class="toc-text">IT Equipment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Infrastructure_and_Facilities"><span class="toc-number">2.2.2.</span> <span class="toc-text">Infrastructure and Facilities</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Geographic_Location_Criteria"><span class="toc-number">2.2.3.</span> <span class="toc-text">Geographic Location Criteria</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Costs"><span class="toc-number">2.2.4.</span> <span class="toc-text">Costs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Power_and_Efficiency"><span class="toc-number">2.2.5.</span> <span class="toc-text">Power and Efficiency</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Redundancy"><span class="toc-number">2.2.6.</span> <span class="toc-text">Redundancy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reliability_Metrics"><span class="toc-number">2.2.7.</span> <span class="toc-text">Reliability Metrics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-3"><span class="toc-number">2.2.8.</span> <span class="toc-text">Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Management"><span class="toc-number">2.3.</span> <span class="toc-text">Cloud Management</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Cloud_Middleware"><span class="toc-number">2.3.1.</span> <span class="toc-text">Cloud Middleware</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Resource_Provisioning"><span class="toc-number">2.3.2.</span> <span class="toc-text">Resource Provisioning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cloud_Orchestration_and_Automation"><span class="toc-number">2.3.3.</span> <span class="toc-text">Cloud Orchestration and Automation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Case_Study__3A_OpenStack"><span class="toc-number">2.3.4.</span> <span class="toc-text">Case Study : OpenStack</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cloud_Software_Stack_Summary"><span class="toc-number">2.3.5.</span> <span class="toc-text">Cloud Software Stack Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Software_Deployment_Considerations"><span class="toc-number">2.4.</span> <span class="toc-text">Cloud Software Deployment Considerations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming_the_Cloud"><span class="toc-number">2.4.1.</span> <span class="toc-text">Programming the Cloud</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Deploying_Applications_on_the_Cloud"><span class="toc-number">2.4.2.</span> <span class="toc-text">Deploying Applications on the Cloud</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Build_Fault-tolerant_Cloud_Services"><span class="toc-number">2.4.3.</span> <span class="toc-text">Build Fault-tolerant Cloud Services</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Load_Balancing"><span class="toc-number">2.4.4.</span> <span class="toc-text">Load Balancing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scaling_Resources"><span class="toc-number">2.4.5.</span> <span class="toc-text">Scaling Resources</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dealing_with_Tail_Latency"><span class="toc-number">2.4.6.</span> <span class="toc-text">Dealing with Tail Latency</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Economics_for_Cloud_Applications"><span class="toc-number">2.4.7.</span> <span class="toc-text">Economics for Cloud Applications</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Programming_the_Cloud_Summary"><span class="toc-number">2.4.8.</span> <span class="toc-text">Programming the Cloud Summary</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Virtualizing_Resources_for_the_Cloud"><span class="toc-number">3.</span> <span class="toc-text">Virtualizing Resources for the Cloud</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction_and_Motivation"><span class="toc-number">3.1.</span> <span class="toc-text">Introduction and Motivation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Background_and_Motivation"><span class="toc-number">3.1.1.</span> <span class="toc-text">Background and Motivation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Virtualization_and_Cloud_Computing"><span class="toc-number">3.1.2.</span> <span class="toc-text">Virtualization and Cloud Computing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Limitations_of_General-Purpose_Operating_Systems"><span class="toc-number">3.1.3.</span> <span class="toc-text">Limitations of General-Purpose Operating Systems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Managing_System_Complexity__3A_Abstractions"><span class="toc-number">3.1.4.</span> <span class="toc-text">Managing System Complexity : Abstractions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Managing_System_Complexity__3A_Interfaces"><span class="toc-number">3.1.5.</span> <span class="toc-text">Managing System Complexity : Interfaces</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Resource_Sharing_Basics"><span class="toc-number">3.1.6.</span> <span class="toc-text">Resource Sharing Basics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Resource_Sharing_in_Multiprocessor_Systems"><span class="toc-number">3.1.7.</span> <span class="toc-text">Resource Sharing in Multiprocessor Systems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction_and_Motivation_Summary"><span class="toc-number">3.1.8.</span> <span class="toc-text">Introduction and Motivation Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Virtualization"><span class="toc-number">3.2.</span> <span class="toc-text">Virtualization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What_Is_Virtualization_3F"><span class="toc-number">3.2.1.</span> <span class="toc-text">What Is Virtualization?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Virtual_Machine_Types"><span class="toc-number">3.2.2.</span> <span class="toc-text">Virtual Machine Types</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Virtualization_Summary"><span class="toc-number">3.2.3.</span> <span class="toc-text">Virtualization Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Resource_Virtualization_-_CPU"><span class="toc-number">3.3.</span> <span class="toc-text">Resource Virtualization - CPU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The_Conditions_for_Virtualizing_ISAs"><span class="toc-number">3.3.1.</span> <span class="toc-text">The Conditions for Virtualizing ISAs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Full_Virtualization_and_Paravirtualization"><span class="toc-number">3.3.2.</span> <span class="toc-text">Full Virtualization and Paravirtualization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Emulation"><span class="toc-number">3.3.3.</span> <span class="toc-text">Emulation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Virtual_CPU"><span class="toc-number">3.3.4.</span> <span class="toc-text">Virtual CPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Resource_Virtualization_3A_CPU_Summary"><span class="toc-number">3.3.5.</span> <span class="toc-text">Resource Virtualization: CPU Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Resource_Virtualization_-_Memory"><span class="toc-number">3.4.</span> <span class="toc-text">Resource Virtualization - Memory</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#One-Level_Page_Mapping"><span class="toc-number">3.4.1.</span> <span class="toc-text">One-Level Page Mapping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Two-Level_Page_Mapping"><span class="toc-number">3.4.2.</span> <span class="toc-text">Two-Level Page Mapping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Memory_Overcommitment"><span class="toc-number">3.4.3.</span> <span class="toc-text">Memory Overcommitment</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reclamation_Techniques_and_VMware_Memory_Ballooning"><span class="toc-number">3.4.4.</span> <span class="toc-text">Reclamation Techniques and VMware Memory Ballooning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Resource_Virtualization_3A_Memory_Summary"><span class="toc-number">3.4.5.</span> <span class="toc-text">Resource Virtualization: Memory Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Resource_Virtualization_-_I/O"><span class="toc-number">3.5.</span> <span class="toc-text">Resource Virtualization - I/O</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#I/O_Basics"><span class="toc-number">3.5.1.</span> <span class="toc-text">I/O Basics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Virtualizing_I/O_Devices"><span class="toc-number">3.5.2.</span> <span class="toc-text">Virtualizing I/O Devices</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Xen_u2019s_Approach_to_I/O_Virtualization"><span class="toc-number">3.5.3.</span> <span class="toc-text">Xen’s Approach to I/O Virtualization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Resource_Virtualization_3A_I/O_Summary"><span class="toc-number">3.5.4.</span> <span class="toc-text">Resource Virtualization: I/O Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Case_Study"><span class="toc-number">3.6.</span> <span class="toc-text">Case Study</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A_Taxonomy_of_Virtualization_Suites"><span class="toc-number">3.6.1.</span> <span class="toc-text">A Taxonomy of Virtualization Suites</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Amazon_u2019s_Elastic_Compute_Cloud"><span class="toc-number">3.6.2.</span> <span class="toc-text">Amazon’s Elastic Compute Cloud</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Amazon_EC2_Properties"><span class="toc-number">3.6.3.</span> <span class="toc-text">Amazon EC2 Properties</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Case_Study_Summary"><span class="toc-number">3.6.4.</span> <span class="toc-text">Case Study Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Storage_and_Network_Virtualization"><span class="toc-number">3.7.</span> <span class="toc-text">Storage and Network Virtualization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Networking_within_Data_Centers"><span class="toc-number">3.7.1.</span> <span class="toc-text">Networking within Data Centers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Software_Defined_Data_Centers_and_Networking"><span class="toc-number">3.7.2.</span> <span class="toc-text">Software Defined Data Centers and Networking</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Storage_within_Data_Centers"><span class="toc-number">3.7.3.</span> <span class="toc-text">Storage within Data Centers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Software_Defined_Storage"><span class="toc-number">3.7.4.</span> <span class="toc-text">Software Defined Storage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary-4"><span class="toc-number">3.7.5.</span> <span class="toc-text">Summary</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Cloud_Storage"><span class="toc-number">4.</span> <span class="toc-text">Cloud Storage</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Cloud_Storage-1"><span class="toc-number">4.1.</span> <span class="toc-text">Cloud Storage</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Data"><span class="toc-number">4.1.1.</span> <span class="toc-text">Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Applications_and_Requirements"><span class="toc-number">4.1.2.</span> <span class="toc-text">Applications and Requirements</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Storage_Devices"><span class="toc-number">4.1.3.</span> <span class="toc-text">Storage Devices</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Storage_Abstractions"><span class="toc-number">4.1.4.</span> <span class="toc-text">Storage Abstractions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Local_File_Systems"><span class="toc-number">4.1.5.</span> <span class="toc-text">Local File Systems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Distributed_File_Systems"><span class="toc-number">4.1.6.</span> <span class="toc-text">Distributed File Systems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Databases"><span class="toc-number">4.1.7.</span> <span class="toc-text">Databases</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Designing_Databases_-_Schema_and_Scalability"><span class="toc-number">4.1.8.</span> <span class="toc-text">Designing Databases - Schema and Scalability</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Designing_Databases_-_Consistency_and_the_CAP_Theorem"><span class="toc-number">4.1.9.</span> <span class="toc-text">Designing Databases - Consistency and the CAP Theorem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Relational_Databases"><span class="toc-number">4.1.10.</span> <span class="toc-text">Relational Databases</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NoSQL_Databases"><span class="toc-number">4.1.11.</span> <span class="toc-text">NoSQL Databases</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NewSQL_Databases"><span class="toc-number">4.1.12.</span> <span class="toc-text">NewSQL Databases</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Object_Storage_as_a_Cloud_Service"><span class="toc-number">4.1.13.</span> <span class="toc-text">Object Storage as a Cloud Service</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cloud_Storage_Summary"><span class="toc-number">4.1.14.</span> <span class="toc-text">Cloud Storage Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Case_Studies_3A_Distributed_File_Systems"><span class="toc-number">4.2.</span> <span class="toc-text">Case Studies: Distributed File Systems</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Case_Studies_3A_NoSQL_Databases"><span class="toc-number">4.3.</span> <span class="toc-text">Case Studies: NoSQL Databases</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Case_Studies_3A_Cloud_Object_Storage"><span class="toc-number">4.4.</span> <span class="toc-text">Case Studies: Cloud Object Storage</span></a></li></ol></li></ol></div></div><div class="post-content"><p>这一部分内容主要是云计算的理论知识，由于时间缘故使用英文原文，关键要点会作为单独的『课』来进行介绍。</p>
<a id="more"></a>
<hr>
<h1 id="Introduction_to_Cloud_Computing"><a href="#Introduction_to_Cloud_Computing" class="headerlink" title="Introduction to Cloud Computing"></a>Introduction to Cloud Computing</h1><h2 id="Cloud_Computing_Overview"><a href="#Cloud_Computing_Overview" class="headerlink" title="Cloud Computing Overview"></a>Cloud Computing Overview</h2><p>Welcome to this online course on cloud computing. This domain is emerging and fast-evolving. Here, we will introduce the big picture of cloud computing, as well as explain how it has evolved to its current state. Learning these concepts will give you a better understanding about some of the motivating factors behind cloud computing services and why it is one of the fastest growing technology segments in the industry today.</p>
<p>In the first unit of the course, we will:</p>
<ul>
<li>introduce some of the fundamental ideas behind cloud computing,</li>
<li>explore the evolution of computing and the emergence of the cloud,</li>
<li>compare and contrast the different service models and types of clouds,</li>
<li>discuss some cloud use cases and popular cloud stacks###</li>
</ul>
<p>Cloud computing can be thought of as an evolved response to the computing needs of today. Our world is increasingly connected and data driven. Users, whether at home or work, generate and consume large amounts of data from various sources. A massive challenge has arisen in terms of managing and exploiting this data. Starting with this module, and throughout the course, you will see how cloud computing plays a central role in meeting this challenge.</p>
<h3 id="Definition_of_Cloud_Computing"><a href="#Definition_of_Cloud_Computing" class="headerlink" title="Definition of Cloud Computing"></a>Definition of Cloud Computing</h3><p><strong>Cloud Computing</strong></p>
<p>Cloud computing offers the use of computing resources as a service over the network. A cloud computer is simply a large distributed computing infrastructure that users have access to over a network. Similar to some other domains, cloud computing came about through the maturity of enabling technologies while attempting to satisfy economic needs. In this course, we will provide an introduction to cloud computing and then cover relevant topics, in varying detail, including hardware and software infrastructure, resource management (virtualization), cloud storage, and programming models.</p>
<p>In the first unit of this course we will start with a simple overview of cloud computing, its definition, motivations, evolutions, building blocks, service models and use cases. We will also discuss economics, risks, benefits and security.</p>
<p><strong>What is Cloud Computing?</strong></p>
<blockquote>
<p>Cloud Computing(definition)<br>The delivery of computing as a service over a network, whereby distributed resources are provided to the end user as a utility.</p>
</blockquote>
<p>Information technology (IT) has become an essential requirement for most organizations to function effectively. Typically, and depending on a specific organization’s needs, IT has three components associated with it (Figure 1.1) - application software, development platforms and the underlying infrastructure:</p>
<p><img src="/images/14529625854236.jpg" alt=""><br>Figure 1.1: Typical Components of Information Technology.</p>
<p>Traditionally, an organization that needs to deploy a particular IT solution has to procure, setup and maintain the infrastructure and the application; certain organizations may decide to develop their own software, in which case they need to manage development platforms as well. The organization hence “owns” the solution, which allows full control over the solution, including, for example, access security and customization, however, it has some drawbacks:</p>
<ol>
<li>Organizations must pay upfront to buy a particular solution, which commits significant capital for long-lived IT resources.</li>
<li>Organizations are solely responsible for the management of their IT solutions. Organizations must have hardware maintenance contracts for the acquired IT solutions. System administrators will have to be hired to monitor hardware and software which has to be updated and maintained. Organizations also have to pay for power and cooling to keep the hardware running. Therefore, in addition to upfront costs, organizations have to budget for recurring costs.</li>
<li>The IT solution typically has a fixed size and will have to be modified to scale when the needs grow or shrink. (For example, as the number of employees grows, the organization will have to purchase additional hardware and/or software to keep up with increasing demands).</li>
<li>Typically IT systems suffer from low average utilization. Utilization refers to the proportion of time (expressed usually as a percentage) that an IT system is being used to capacity. For example, email services in a large organization typically see the most amount of traffic at 8 am, when people sign in and check email. Utilization tapers off towards close of business and is practically nil after hours. Further, since IT systems consume energy, even at idle, they leave a prominent carbon footprint.</li>
</ol>
<p>Many of the disadvantages listed above emanate from the ownership of IT. However, with the evolution of computing technology, it is no longer necessary for organizations to own IT systems. Many of the IT needs of organizations can be provided to them as services. Cloud computing is the transformation of owned IT products into services that can be availed from a cloud service provider.</p>
<p>The transformation of a certain technology from a product to a service is not new. A similar transformation evolved for electricity, which initially had to be produced near the device or service requiring it. The development of large power plants, electric transmission systems and grids has led to the rise of electric power as a utility, (a service that people can obtain and pay for as needed).</p>
<p>The following video (Video 1.1) discusses the transformation of IT from a product to a service:</p>
<p><a href="http://youtu.be/HaVqHgg7zv4" target="_blank" rel="external">Video 1.1: Introduction to Cloud Computing</a></p>
<p>In cloud computing, users or organizations use computing resources as a service and pay for them as a utility, in a pay-as-you-go model. When a request is made for computing resources, the cloud provider typically provisions these resources, in a rapid manner (minutes or hours). As the need for these resources changes, users or organizations can rapidly scale up or down their resources on demand.</p>
<p>The cloud model offers users and organizations several benefits, including: reduced upfront cost, as IT services can be obtained in a pay-as-you-go model; the convenience of fast resource provisioning, which significantly reduces the time to market for IT solutions; and rapid scalability of computing resources, as they can be scaled up and down on demand. Cloud providers’ resources are shared by multiple users, thereby improving utilization and reducing carbon footprint.</p>
<p>In spite of all of its advantages, cloud computing is still an emerging and maturing technology and comes with many risks and challenges which will be later covered in this unit.</p>
<h3 id="Pre-Cloud_Computing_Domains_and_Applications"><a href="#Pre-Cloud_Computing_Domains_and_Applications" class="headerlink" title="Pre-Cloud Computing Domains and Applications"></a>Pre-Cloud Computing Domains and Applications</h3><p><strong>Domains and Application Examples</strong></p>
<p>Now that we have defined what cloud computing is, let us look at examples of how computing was utilized in different domains such as business computing, scientific computing and personal computing before the emergence of cloud computing.</p>
<p>Business computing: Examples of traditional management information systems include logistics and operations, enterprise resource planning (ERP), customer relation management (CRM), office productivity and business intelligence (BI). Such tools enabled more streamlined processes that led to improved productivity and reduced cost across a variety of enterprises.</p>
<p>As an example, CRM software allows companies to collect, store, manage and interpret a variety of data about past, current and potential future customers. CRM software offers an integrated view (in real-time or near real-time) of all organizational interactions with customers. For example, for a manufacturing company, CRM software could be used by a sales team to schedule meetings, tasks and follow-ups with clients. A marketing team could target clients with campaigns based on specific patterns. Billing teams can track quotes and invoices. As such, it is a centralized repository for storing this information. To enable this functionality, a variety of hardware and software technologies are utilized by the organization and sales teams in order to collect the data which needs to be stored and analyzed using various database and analytics systems.</p>
<p>Scientific computing: Scientific computing uses mathematical models and analysis techniques implemented on computers to attempt to solve scientific problems. A popular example is computer simulation of physical phenomena. This field has disrupted the traditional theoretical and laboratory experimental methods by enabling scientists and engineers to reconstruct known events or to predict future situations through developing programs to simulate and study different systems under different circumstances. Such simulations typically require a very large number of calculations which are often run on expensive supercomputers or distributed computing platforms.</p>
<p>Personal computing: In personal computing, a user runs various applications on a general-purpose computer. Examples of such applications include office productivity software such as word processing and spreadsheets, or communication such as email clients or entertainment such as video games or playing multimedia files. A personal computing user typically owns, installs and maintains the software and hardware utilized to carry out such tasks.</p>
<p><strong>Addressing Scale</strong></p>
<p>hich can provide the necessary upgrades to the service level. In many cases, vertical scaling consists of upgrading or replacing servers and storage systems with newer, faster servers or storage arrays with increased capacity. This process could take months to plan and execute, along with a window where the service might experience some downtime.</p>
<p>In certain types of systems, scaling is also done horizontally, by increasing the amount of resources dedicated to the system. An example of this is in high-performance computing, where additional servers and storage can be added to improve the performance of the system, thereby leading to a higher number of calculations that can be performed per second, or increasing the storage capacity of the system. Just like vertical scaling, this process can take months to plan and execute, with downtimes also a possibility.</p>
<p>Since companies owned and maintained their IT equipment, as the cost of addressing scale continued to rise, companies identified other methods to reduce cost. Large companies consolidated the computing needs of different departments into a single large data center whereby they consolidated real estate, power, cooling, and networking in order to reduce cost. On the other hand, small and medium size companies could lease real-estate, network, power, cooling and physical security by placing their IT equipment in a shared data center. This is typically referred to as a co-location service which was adopted by small to medium sized companies who did not want to build their own data centers in-house. Co-location services continues to be adopted in various domains as a cost-effective approach to reduce operational expenses.</p>
<p>Scale has impacted all aspects of business computing. For example, scale has impacted CRM systems through the increase of clients or through the amount of information that we store and analyse about clients. Business computing has addressed scale through vertical and horizontal scaling as well as consolidation of IT resources to data centers and co-location. In scientific computing, parallel and distributed systems have been adopted in order to scale up the size of the problems and the precision of their numerical simulations. One definition of parallel processing is the use of multiple homogenous computers that share state and function as a single large computer in order to run large scale or high precision calculations. Distributed computing is the use of multiple autonomous computing systems connected by a network in order to partition a large problem into subtasks that are run concurrently and communicate via messages over the network. The scientific community continued to innovate in these domains in order to address scale. In personal computing, scale has impacted it through increased user demands brought on by richer content and diverse applications. Users therefore scale up their owned personal computing device to keep up with these demands.</p>
<p><strong>Rise of Internet Services</strong></p>
<p>The late 90s marked a steady increase in the adoption of these computing applications and platforms across domains. Soon, software was expected to not only be functional, but also capable of producing value and insight for business and personal requirements. The use of these applications became collaborative; applications were mixed and matched to feed information to each other. IT was no longer just a cost center for a company, but a source of innovation and efficiency.</p>
<p><img src="/images/14529628961629.jpg" alt=""><br>Figure 1.2: Comparing Traditional and Internet-Scale Computing.</p>
<p>The 21st century has been marked by an explosion in the volume and capacity of wireless communications, the World Wide Web, and the Internet. These changes have led to a network- and data-driven society, where producing, disseminating and accessing digitized information is simplified. The Internet is estimated to have created a global marketplace of billions of users, up from 25 million in 1994 (Figure 1.3 (a)) [1] . This rise in data and connections is valuable to businesses. Data creates value in several ways, including by enabling experimentation, segmenting populations and supporting decision-making with automation. [2] By embracing digital technologies, the world’s top 10 economies are expected to increase their output by over a trillion dollars by 2020. [3]</p>
<p>The increasing number of connections enabled by the Internet has also driven its value. Researchers have hypothesized that the value of a network varies superlinearly as a function of the number of users. Thus, at internet scale, gaining and retaining customers is a priority, and this is done by building reliable and responsive services, and making changes based on observed data patterns.</p>
<p><img src="/images/14529629235564.jpg" alt=""><br>Figure 1.3 (a): Increasing number of Internet Users per year. [1]</p>
<p><img src="/images/14529629323895.jpg" alt=""><br>Figure 1.3 (b): Increasing number of data stored per year. [5]</p>
<p>Some examples of Internet-scale systems include:</p>
<ol>
<li>Search engines that crawl, store, index, and search (upto petabyte-sized) large data sets. For instance, Google started as a giant web index that crawled and analyzed web traffic, once every few days and matched these indices to keywords. Now, it updates its indices in near-real-time and is one of the most popular ways to access information on the Internet. Their index has trillions of pages with a size of thousands of terabytes. [4]</li>
<li>Social networks like Facebook and LinkedIn that allow users to create personal and professional relationships and build communities based on similar interests. Facebook, for instance, now supports over a billion active users per month.</li>
<li>Online retail services like Amazon maintain a global inventory of millions of products, which are sold to over 200 million customers, with net sale volumes of almost $90 billion annually.</li>
<li>Rich, streaming multimedia applications allow people to watch and share videos and other forms of rich content. One such example, YouTube, handles uploads of 300 minutes of video per second.</li>
<li>Real-time communications systems for audio, video and text chatting like Skype which clock more than 50 billion minutes of calls per month.</li>
<li>Productivity and collaboration suites that serve millions of documents to many concurrent users allowing real-time, persistent updates. For e.g. Office 365 claims to support 50 million monthly active collaborators.</li>
<li>CRM applications by providers like SalesForce are deployed at over a hundred thousand organizations. Large CRMs now provide intuitive dashboards to track status, analytics to find the customers that generate the most business and revenue forecasting to predict future growth.</li>
<li>Data mining and business intelligence applications that analyze the usage of other services (like those above) to find inefficiencies and opportunities for monetization.</li>
</ol>
<p>Clearly, these systems are expected to deal with a high volume of concurrent users. This requires an infrastructure with the capacity to handle large amounts of network traffic, generate and securely store data, all without any noticeable delays. These services derive their value by providing a constant and reliable standard of quality. They also provide rich user interfaces for mobile devices and web browsers, making them easy to use, but harder to build and maintain.</p>
<p>We summarize some of the requirements of Internet-scale systems here:</p>
<ol>
<li>Ubiquity—being accessible from anywhere at any time, from a multitude of devices. For instance, a salesperson will expect their CRM service to provide timely updates on a mobile device to make visits to clients shorter, faster and more effective. The service should function smoothly under a variety of network connections.</li>
<li>High-availability—the service must be “always up”. Uptimes are measured in terms of number of nines. Three nines, or 99.9%, implies that a service will be unavailable for 9 hours a year. Five nines (about 6 minutes a year) is a typical threshold for a high-availability service. Even a few minutes of downtime in online retail applications can impact millions of dollars of sales.</li>
<li>Low latency—fast and responsive access times. Even slightly slower page load times have been shown to significantly reduce the usage of that web page. For instance, increasing search latency from 100 ms to 400 ms decreases the number of searches per user from 0.8% to 0.6%, and the change persisted even after the latency was reduced to original levels.</li>
<li>Scalability—the ability to deal with seasonality and virality, which causes peaks and troughs in the traffic over long and short periods of time. On days like “Black Friday” and “Cyber Monday”, retailers like Amazon must handle several times the network traffic than on average.</li>
<li>Cost effectiveness—an Internet-scale service requires much more infrastructure than a traditional application as well as better management. One way to streamline costs is by making services easier to manage, and reducing the number of administrators handling a service. Smaller services can afford to have a low service-to-admin ratio (e.g. 2:1, meaning a single administrator must maintain two services); however, to maintain profitability, services like Microsoft Bing must have high service-to-admin ratio (e.g. 2500:1, meaning a single administrator maintains 2500 services) [6] .</li>
<li>Interoperability—many of these services are often used together and hence must provide an easy interface for reuse and support standardized mechanisms for importing and exporting data. For example, many other services (like Uber) may integrate Google Maps within their products to provide simplified location and navigation information to users.</li>
</ol>
<p>We will now explore some of the early solutions to the various problems above [7] . The first challenge to be tackled was the large round-trip time for early web services that were mostly located in the United States. The earliest mechanisms to deal with the problems of low latency (due to distant servers) and server failure simply relied on redundancy. One technique for achieving this was by “mirroring” content, whereby copies of popular web pages would be stored at different locations around the world. This minimized the amount of load on the central server, reduced the latency perceived by end-users, and allowed traffic to be switched over to another server in case of failures. The downside of this was an increase in complexity to deal with inconsistencies if even one copy of the data were to be modified. Thus, this technique is more useful for static, read-heavy workloads, such as serving images, videos or music. Due to the effectiveness of this technique, most Internet-scale services use content delivery networks (CDNs) to store distributed global caches of popular content. For example, Cable News Network (CNN) now maintains replicas of their videos on multiple “edge” servers at different locations worldwide, with personalized advertising per location.</p>
<p>Of course, it did not always make sense for individual companies to buy dozens of servers across the world. Cost efficiencies were often gained by using shared hosting services. Here, shares of a single web server would be leased out to multiple tenants, amortizing the cost of server maintenance. Shared hosting services could be highly resource-efficient, as the resources could be over-provisioned under the assumption that not all services would be operating at peak capacity at the same time (an over-provisioned physical server is one where the aggregate capacity of all the tenants is greater than the actual capacity of the server). The downside was that it was nearly impossible to isolate the tenants’ services from those of their neighbors. Thus, a single overloaded or error-prone service could adversely impact all its neighbors. Another problem arose because tenants could often be malicious and try to leverage their co-location advantage to steal data or deny service to other users.</p>
<p>To counter this, Virtual Private Servers were developed as variants of the shared hosting model. A tenant would be provided with a virtual machine (VM) on a shared physical server (we talk more about virtual machines and their properties later). These VMs were often statically allocated and linked to a single physical machine, thus they were difficult to scale and often needed manual recovery from any failures. Though they could no longer be overprovisioned, they had better performance and security isolation between co-located services than simple resource sharing.</p>
<p>Another problem of sharing public resources was that it required storing private data on third-party infrastructure. Some of the Internet-scale services we described above could not afford to lose control over data storage, since any disclosure of their customers private data would have disastrous consequences. Hence, these companies needed to build their own global infrastructure. Before the advent of the public cloud, such services could only be deployed by large corporations like Google and Amazon. Each of these companies would build large, homogeneous data centers across the globe using commodity off-the-shelf components, where a data center could be thought of as a single, massive warehouse-scale computer (WSC). A WSC provided an easy abstraction to globally distribute applications and data, while still maintaining ownership.</p>
<p>Due to the economies of scale, the utilization of a data center could be optimized to reduce costs. This was still not as efficient as publicly sharing resources, these warehouse-scale computers had many desirable properties that served as foundations for building Internet-scale services. The scale of computing applications progressed from serving a fixed user base to serving a dynamic global population. Standardized WSCs allowed large companies to serve such large audiences. An ideal infrastructure would combine the performance and reliability of a WSC, with the sharing hosting model. This would enable even a small corporation to develop and launch a globally competitive application, without the high overhead of building large datacenters.</p>
<p>Another approach to share resources was Grid Computing, which enabled the sharing of autonomous computing systems across institutions and geographical locations. Several academic and scientific institutions would collaborate and pool together their resources towards a common goal. Each institution would then join a “virtual organization” by dedicating a specific set resources via well-defined sharing rules. Resources would often be heterogeneous and loosely coupled requiring complex programming constructs to stitch together. Grids were geared towards supporting non-commercial research and academic projects and relied on existing open source technologies.</p>
<p>The cloud was a logical successor that combined many of the features of the solutions above. For example, instead of universities contributing and sharing access to a pool of resources using a Grid, the cloud allows them to lease computing infrastructure that was centrally administered by a Cloud Service Provider (CSP). As the central provider maintained a large resource pool to satisfy all clients, the cloud made it easier to dynamically scale up and down demand within a short period of time. Instead of open standards like the grid, however, cloud computing relies on proprietary protocols and needs the user to place a certain level of trust in the CSP.</p>
<p>The rest of this unit covers how the cloud evolved to make computing a public utility that could be metered and used.</p>
<p><strong>References</strong></p>
<ol>
<li>Real Time Statistics Project (2015). Internet Live Stats. www.internetlivestats.com/.</li>
<li>IBM (2015). What is Big Data?. www-01.ibm.com/software/data/bigdata/what-is-big-data.html.</li>
<li>Accenture (2015). Increased Use of Digital Technologies Could Add $1.36 Trillion to World’s Top 10 Economies in 2020. newsroom.accenture.com/subjects/strategy/increased-use-of-digital-technologies-could-add-over-1-trillion-dollars-to-worlds-top-10-economies-in-2020-according-to-new-study-from-accenture.htm.</li>
<li>Google Inc. (2015). How Search Works. <a href="https://www.google.com/insidesearch/howsearchworks/thestory/" target="_blank" rel="external">https://www.google.com/insidesearch/howsearchworks/thestory/</a>.</li>
<li>Hilbert, Martin and Lopez, Priscila (2011). The world’s technological capacity to store, communicate, and compute information.</li>
<li>Hamilton, James R and others (2007). On Designing and Deploying Internet-Scale Services.</li>
<li>Brewer, Eric and others (2001). Lessons from giant-scale services.</li>
</ol>
<h3 id="Evolution_of_Cloud_Computing"><a href="#Evolution_of_Cloud_Computing" class="headerlink" title="Evolution of Cloud Computing"></a>Evolution of Cloud Computing</h3><p><strong>Events and Innovations</strong></p>
<p>The cloud-computing concept first appeared during the early 1950s, when several academics, including Herb Grosch, John McCarthy, and Douglas Parkhill, [1] [2] envisioned computing as a utility similar to electric power. Over the next few decades, several emerging technologies laid the foundations for cloud computing (Figure 1.4). More recently, rapid growth of the World Wide Web and the advent of large Internet giants, such as Google and Amazon, finally led to the creation of an economic and business environment that allowed the cloud-computing model to flourish.</p>
<p><img src="/images/14529690211629.jpg" alt=""><br>Figure 1.4: Evolution of cloud computing.</p>
<p><strong>Evolution of Computing</strong></p>
<p>Since the 1960s, some of the earliest forms of computers that were used by organizations were mainframe computers. Multiple users could share and connect to mainframes over basic serial connections using terminals. The mainframe was responsible for all the logic, storage, and processing of data, and the terminals connected to them had limited computational power, if any. These systems continued in widespread use for more than 30 years and, to some degree, continue to exist today.</p>
<p>With the birth of personal computing, cheaper, smaller, more powerful processors and memory led to a swing in the opposite direction, in which users ran their own software and stored data locally. This situation, in turn, led to issues of ineffective data sharing and rules to maintain order within an organization’s IT environment.</p>
<p>Gradually, through the development of high-speed network technologies, local area networks (LANs) were born that allowed computers to connect and communicate with each other. Thus, vendors designed systems that could encapsulate the benefits of both personal computers and mainframes, resulting in client-server applications that became popular over LANs. Clients would typically run client software (and process some data) or a terminal (for legacy applications) that connected to a server. The server, in the client-server model, possessed the application, storage, and data logic.</p>
<p>Eventually, in 1990s, the global information age emerged, with the Internet rapidly being adopted. Network bandwidth improved by many orders of magnitude, from ordinary dial-up access to dedicated fiber connectivity today. In addition, cheaper and more powerful hardware emerged. Furthermore, the evolution of the World Wide Web and dynamic websites necessitated multitier architectures.</p>
<p>Multitier architectures enabled the modularization of software by separating the application presentation, application logic, and storage as individual entities. With this modularization and decoupling, it was not long before these individual software entities were running on distinct physical servers (typically due to differences in hardware and software requirements). This led to an increase of individual servers in organizations; however, it also led to poor average utilization of server hardware. In 2009, the International Data Corporation (IDC) estimates that the average x86 server has a utilization rate of approximately 5 to 10%. [3]</p>
<p>Virtual machine technology matured well enough in the 2000s to become available as commercial software. Virtualization enables an entire server to be encapsulated as an image, which can be run seamlessly on hardware and enable multiple virtual servers to run simultaneously and share hardware resources. Virtualization thus enables servers to be consolidated, which accordingly improves system utilization.</p>
<p>Simultaneously, grid computing gained traction in the scientific community in an effort to solve large-scale problems in a distributed fashion. With grid computing, computer resources from multiple administrative domains work in unison for a common goal. Grid computing brought forth many resource-management tools (e.g., schedulers and load balancers) to manage large-scale computing resources.</p>
<p>As the various computing technologies evolved, so did the economics of computing. Even during the early days of mainframe-based computing, companies such as IBM offered to host and run computers and software for various organizations, such as banks and airlines. In the Internet Age, third-party Web hosting also become popular. With virtualization, however, providers have unparalleled flexibility in accommodating multiple clients on a single server, sharing hardware and resources between them.</p>
<p>The development of these technologies, coupled with the economic model of utility computing, is what eventually evolved into cloud computing.</p>
<p><strong>Enabling Technologies</strong></p>
<p>Cloud computing has various enabling technologies (Figure 1.5), which include networking, virtualization and resource management, utility computing, programming models, parallel and distributed computing, and storage technologies.</p>
<p><img src="/images/14529690849852.jpg" alt=""><br>Figure 1.5: The enabling technologies in cloud computing.</p>
<p>The emergence of high-speed and ubiquitous networking technologies have greatly contributed to cloud computing as a viable paradigm. Modern networks make it possible for computers to communicate in a fast and reliable manner, which is important if we are to use services from a cloud provider. This enabled the user experience with software running in a remote data center to be comparable to the experience of software running on a personal computer. Webmail is a popular example, as is office productivity software. In addition, virtualization is key to enabling cloud computing. As mentioned above, virtualization allows managing the complexity of the cloud via abstracting and sharing its resources across users through multiple virtual machines. Each virtual machine can execute its own operating system and associated application programs. Virtualization for cloud computing is covered in Unit 3.</p>
<p>Technologies such as large-scale storage systems, distributed file systems, as well as novel database architectures are crucial for managing and storing data in the cloud. Cloud storage technologies are covered in Unit 4.</p>
<p>Utility computing offers numerous charging structures for the leasing of compute resources. Examples include pay-per-resource-hour, pay-per-guaranteed-throughput, and pay-per-data stored per month etc.</p>
<p>Parallel and distributed computing allow distributed entities located at networked computers to communicate and coordinate their actions in order to solve certain problems, represented as parallel programs. Writing parallel programs for distributed clusters is inherently difficult. To achieve high programming efficiency and flexibility in the cloud, a programming model is required.</p>
<p>Programming models for clouds give users the flexibility to express parallel programs as sequential computation units (e.g., functions as in MapReduce and vertices as in GraphLab). Such programming models’ runtime systems typically parallelize, distribute, and schedule computational units, manage inter-unit communication, and tolerate failures. Cloud programming models are covered in Unit 5.</p>
<p><strong>RReferences</strong></p>
<ol>
<li>Simson L. Garfinkel (1999). Architects of the Information Society: Thirty-Five Years of the Laboratory for Computer Science at MIT. MIT Press.</li>
<li>Douglas J. Parkhill (1966). The Challenge of the Computer Utility. Addison-Wesley Publishing Company, Reading, MA.</li>
<li>Michelle Bailey (2009). “The Economics of Virtualization: Moving Toward an Application-Based Cost Model.” VMware Sponsored IDC Whitepaper.</li>
</ol>
<h3 id="Cloud_Building_Blocks_and_Service_Models"><a href="#Cloud_Building_Blocks_and_Service_Models" class="headerlink" title="Cloud Building Blocks and Service Models"></a>Cloud Building Blocks and Service Models</h3><p><strong>Cloud Building Blocks</strong></p>
<p>Remember, cloud computing offers the use of computing resources as a service over the network. Before we discuss the service models offered on a cloud, we ought to think about the different layers of hardware and software the are required to build cloud services. Of course, not all service requirements are identical; some cloud users may only desire access to raw infrastructure to build applications on. Others may wish to not deal with the infrastructure at all, but rather, simply develop and deploy applications using an easy-to-use platform. To meet these varied requirements, cloud service providers divide their offerings into various abstract layers.</p>
<p>Here, we introduce a stacked abstraction of the cloud through presenting typical building blocks and discuss their association with three service models in cloud computing. We present four main building blocks in cloud computing: application software, development platforms, resource sharing, and infrastructure, as shown in Figure 1.6. The infrastructure includes the physical resources in a data center. The resource sharing layer typically entails software and hardware techniques that allow the sharing of the physical resources while offering a certain level of isolation. The development platforms are utilized to develop cloud applications.</p>
<p><img src="/images/14529692293425.jpg" alt=""><br>Figure 1.6: Cloud computing building blocks.</p>
<p><strong>Cloud Building Blocks</strong></p>
<p>Application software: The top layer in the stack is the application software, which normally is the system component that the end user utilizes.</p>
<p>Development platforms: The next layer, development platforms, allows application developers to write application software in terms of a cloud’s application programming interface (API). Development platforms typically provide specifications that developers can use for routines, data structures, object classes, libraries and variables.</p>
<p>Resource sharing: Resource sharing mechanisms, the third layer, embody some key cloud ideas:</p>
<ul>
<li>Provide software, computation, network and storage services.</li>
<li>Allow a shared environment whereby multiple hardware images (e.g., virtual machines) and system images (e.g., general-purpose OSs) can run side by side on a single infrastructure along with security, resource, and failure isolations. These isolation properties are provided by a combination of hardware and software techniques that are covered in detail in Unit 3.</li>
<li>Consolidate physical servers into virtual servers that run on fewer physical servers.</li>
<li>Deliver agility and elasticity to rapidly respond to users’ resource and service demands.</li>
</ul>
<p>These ideas usually are addressed through virtualization, a technology discussed in detail in Unit 3.</p>
<p>Infrastructure: Physical resources comprise the bottom layer and, in cloud computing, are primarily deployed on the cloud provider’s side. The broad resource classes, detailed in Unit 2, include the following:</p>
<ul>
<li>Compute resources, typically servers, which are computers designed for enterprise computing (as opposed to user workstations). They usually are rack mounted to utilize space efficiently.</li>
<li>Storage resources maintain the cloud’s data, and application storage use usually is charged in terms of capacity usage (e.g., per gigabyte or terabyte usage).</li>
<li>Network resources enable communication between servers as well as servers and clients.</li>
<li>Software that manages the compute, network and storage infrastructure.</li>
</ul>
<p>Next we will discuss which of these abstractions can be provided as a leased service over a network. For example the services and resources required by a software developer will be different compared to someone who would like to have access to a WebMail application running on the cloud.</p>
<p><img src="/images/cc19.jpg" alt="c"></p>
<p><strong>Cloud Computing Services</strong></p>
<p>In a broad sense, cloud services differ based on the needs of different users. This section reviews three popular types of cloud services:</p>
<ul>
<li>Software as a service (SaaS)</li>
<li>Platform as a service (PaaS)</li>
<li>Infrastructure as a service (IaaS)</li>
</ul>
<p>SaaS is any application in which the end user has access to a software application over the network and pays based on a variety of business models some of which are free. PaaS is the offering of software development platforms as a service which are utilized to develop SaaS applications. Finally, IaaS, is the leasing of virtualized infrastructure over the network. In this last model, the end user has the flexibility to install and use any software they please on the leased infrastructure.</p>
<p>The following video (Video 1.2) reviews these services:</p>
<p><a href="http://youtu.be/ltJmJEI0gGA" target="_blank" rel="external">Video 1.2: Service Models in Cloud Computing</a></p>
<p><strong>The Software-as-a-Service Model</strong></p>
<blockquote>
<p>Software as a Service(definition)<br>Software as a service (SaaS) is a software delivery model in which software and associated data are hosted on a cloud. SaaS applications typically are accessed by users using a thin client via a Web browser.</p>
</blockquote>
<p>SaaS is one of the most common cloud service models in which the cloud provider delivers software as an Internet service (as discussed in Video 1.3). SaaS users simply use their browsers to access the software, thus eliminating the need to install, run, and maintain (update, patch, reconfigure etc.) the application on the user’s computer. The Web browser loads the SaaS application service dynamically and transparently.</p>
<p><a href="http://youtu.be/bzfdewWofSU" target="_blank" rel="external">Video 1.3: Software as a Service</a></p>
<p>SaaS has become a common software delivery model for many business applications, including accounting, collaboration, customer relationship management (CRM), management information systems (MIS), enterprise resource planning (ERP), invoicing, human resource management (HRM), content management (CM) as well as service desk management.</p>
<p>With SaaS, the provider maintains the software and required infrastructure to run it. The provider routinely develops the software, and enhancements are automatically made available to all users the next time a user logs on to the service. In addition, any application data that results from the use of the service resides on the cloud and is available to the user from any location.</p>
<p><strong>Characteristics of SaaS</strong></p>
<p>A vast majority of SaaS solutions are based on what is referred to as multitenant architecture. In this architecture, a single version of the application, with a single configuration, is used for every customer (referred to as a tenant). To enable the service to scale well, it might be installed on several servers at the provider’s side. Dynamic scaling is utilized to allow more users to use the service as it becomes more popular.</p>
<p>Typical characteristics of SaaS include:</p>
<ul>
<li>Web-based access to the software service.</li>
<li>Software is managed from a central location by the cloud provider.</li>
<li>Software is delivered in a one-to-many model in which “one” is the cloud provider and “many” are the cloud users.</li>
<li>The cloud provider handles software upgrades and patches.</li>
</ul>
<p><strong>Pricing Models</strong></p>
<p>Unlike traditional software, which is sold under the software licensing model (with an upfront license cost and an optional ongoing support fee), SaaS providers generally price applications using a monthly or annual subscription fee. This model enables SaaS to fulfill one of the main purported advantages of cloud computing - reducing the capital expenditure or the upfront cost of software. SaaS providers typically charge based on usage parameters, such as the number of users using the application.</p>
<p><strong>Use Cases for SaaS</strong></p>
<p>SaaS is a good model for certain types of applications, such as:</p>
<ul>
<li>Applications that are fairly standardized and do not require custom solutions. E-mail is a good example of a fairly standardized application.</li>
<li>Applications that have a significant need for remote/web/mobile access, such as mobile sales management software.</li>
<li>Applications that have a short-term need, such as collaborative software for a particular project.</li>
<li>Applications in which demand spikes significantly, such as tax or billing software that is used once a month.</li>
</ul>
<p>However, there are situations where SaaS may not be the right choice, such as:</p>
<ul>
<li>Applications that require offline access to data.</li>
<li>Applications that require significant customization.</li>
<li>Applications in which policies or regulations disallow data from being hosted externally.</li>
<li>Applications in which existing in-house solutions satisfy all of the organization’s needs.</li>
</ul>
<p><strong>Examples of SaaS</strong></p>
<p>Web mail is one of the early examples of SaaS. Webmail enabled users with a browser and an Internet connection to access their e-mail anywhere at anytime. Offerings from Hotmail, Yahoo!, and Gmail are extremely popular. These services are based on the “freemium” model, wherein basic services are free, and more advanced features are available with a subscription. Furthermore, providers earn revenue mainly from advertisements that are displayed to the users as they use the service.</p>
<p>Another popular example of SaaS is online office suites, such as Google Drive and Microsoft Office 365, which allow users to create, edit, and share documents online. Google utilizes the freemium model for individual users. Microsoft has a charge model based on the features required and the number of users per month.</p>
<p><strong>The Platform-as-a-Service Model</strong></p>
<blockquote>
<p>Platform as a Service(definition)<br>Platform as a service (PaaS) is a computing platform that allows for the creation of Web applications in a simplified manner without the complexity of purchasing and maintaining any of the underlying software and infrastructure.</p>
</blockquote>
<p>PaaS-based offerings allow users to develop, deploy, and scale applications on platforms that are offered by cloud providers (Video 1.4). PaaS is analogous to SaaS, except that, rather than software delivered over the Web, it is a platform for the creation of software that is delivered over the Web.</p>
<p><a href="http://youtu.be/mxXm5s0hK8A" target="_blank" rel="external">Video 1.4: Platform-as-a-Service</a></p>
<p><strong>Characteristics of PaaS</strong></p>
<p>PaaS offerings vary among providers but usually feature some basic functionality, which includes:</p>
<ul>
<li>Services to develop, test, deploy, host, and maintain applications in the same integrated development environment (IDE).</li>
<li>Web-based user interface (UI) creation tools to help create, modify, and test various UI scenarios.</li>
<li>Multitenant architecture in which multiple concurrent users utilize the same development tools.</li>
<li>Built-in scaling mechanisms of deployed software that can be handled automatically by the cloud provider by load-balancing and failover mechanisms.</li>
</ul>
<p><strong>Pricing Models</strong></p>
<p>Unlike the SaaS pricing model (which is a subscription or advertisement based model), PaaS usually is priced in terms of usage of the platform. For example, Google App Engine’s <a href="https://cloud.google.com/pricing/" target="_blank" rel="external">charge model</a> accounts for an application’s inbound and outbound bandwidth as well as certain API requests. Consequently, the more an application developed using PaaS gets used, the more the PaaS developer gets charged.</p>
<p><strong>Use Cases for PaaS</strong></p>
<p>PaaS is a good model for certain types of applications, such as:</p>
<ul>
<li>Rapid application development scenarios.</li>
<li>Applications that require Web-based infrastructure to handle varying loads from users.</li>
<li>Applications that may not need redeployment or migration to other platforms in the future.</li>
</ul>
<p>There are certain scenarios in which PaaS may not be ideal, such as:</p>
<ul>
<li>When the application needs to be highly portable in terms of where it is hosted because PaaS APIs can vary from one PaaS provider to another.</li>
<li>When proprietary languages or APIs could impact the development process or cause trouble in the future due to vendor lock-in.</li>
<li>When application performance requires customization of the underlying hardware and software.</li>
</ul>
<p><strong>Examples of PaaS</strong></p>
<p>Google App Engine is an example of a PaaS. Using Google’s APIs, developers can create Web and mobile applications that run on Google’s infrastructure.</p>
<p><strong>The Infrastructure-as-a-Service Model</strong></p>
<blockquote>
<p>Infrastructure as a service(definition)<br>Infrastructure as a service (IaaS) is a cloud computing model in which cloud providers make computing resources available to clients, usually in the form of instances or virtual machines.</p>
</blockquote>
<p>In the IaaS model, providers rent out compute resources in the form of instances or virtual machines, which have some form of configurable CPU, memory, disk, and network bandwidth attached to them (Video 1.5). Once provisioned, IaaS users can remotely connect to these instances and configure their choice of platforms and applications. This model offers the most amount of flexibility to the IaaS users in terms of software development and deployment. Rather than purchasing servers, software, data center space, or network equipment, users rent those resources as a fully outsourced service on demand.</p>
<p><a href="http://youtu.be/sjQSV-5RaLU" target="_blank" rel="external">Video 1.5: Infrastructure-as-a-Service</a></p>
<p><strong>Characteristics of IaaS</strong></p>
<p>IaaS has the following characteristics:</p>
<ul>
<li>Computing resources are provided to IaaS users as a service.</li>
<li>IaaS providers provide tools that enable IaaS users to configure the dynamic scaling of resources.</li>
<li>IaaS providers usually have different resource offerings at different costs and follow a utility pricing model (typically calculated hourly).</li>
<li>The same physical resources are shared among multiple users.</li>
</ul>
<p><strong>Pricing Models</strong></p>
<p>Unlike the SaaS pricing model (which is a subscription- or advertisement-based model) or the PaaS model (which usually is priced in terms of number of transactions or bandwidth or storage used), IaaS usually is priced on an hourly basis, per instance. For example, Amazon Elastic Compute Cloud (EC2) offers a spectrum of compute resources as virtualized OS instances, which vary in compute, memory, storage, and bandwidth. At the time of writing, the Amazon EC2 t2.micro instance costs about 1.3 cents an hour when provisioned at Amazon’s Northern Virginia data center.</p>
<p>Cloud providers can also choose to bill on a prorated or non-prorated basis. On a prorated basis, each partial hour is billed partially, while on a non-prorated basis, each partial hour is billed as a full hour. This difference becomes significant when IaaS users need a large number of instances for a short period of time for burst processing. Amazon EC2 instances are billed on a non-prorated basis.</p>
<p><strong>Use Cases for IaaS</strong></p>
<p>IaaS makes sense in a number of situations:</p>
<ul>
<li>When demand for computing resources is volatile. For example, e-commerce sites experience the most demand during holiday seasons.</li>
<li>For new organizations that do not have the capital to invest in infrastructure on site.</li>
<li>When organizations need to grow their IT resources rapidly, such as Internet startup companies.</li>
<li>For temporary projects or temporary infrastructural needs (when organizations require a large amount of compute power for a limited amount of time).</li>
</ul>
<p>IaaS may not be the best option when:</p>
<ul>
<li>Regulatory compliance does not allow data to be offshored or outsourced.</li>
<li>When applications have strict quality-of-service (QoS) requirements.</li>
<li>When organizations have existing in-house customized infrastructure to meet their IT needs.</li>
</ul>
<p><strong>Examples of IaaS</strong></p>
<p>Amazon Web Services (AWS), Microsoft Azure and Rackspace are cloud service providers that offer IaaS products. Specifically, AWS’s Elastic Compute Cloud (EC2) is one of the first commercially successful IaaS products. AWS EC2 rents out instances from various data center locations scattered around the world. Users can choose from various instance types, from a low-memory, single CPU (which costs about several cents an hour), all the way up to multicore, high-performance, GPU-accelerated instances (which can cost upto several US dollars an hour).</p>
<h3 id="Cloud_Deployment_Models"><a href="#Cloud_Deployment_Models" class="headerlink" title="Cloud Deployment Models"></a>Cloud Deployment Models</h3><p><strong>Types of Clouds</strong></p>
<p>There are three well-known deployment models for cloud computing: public, private, and hybrid clouds. A public cloud is owned by a cloud provider but is made available to the public. A private cloud is typically owned by an organization, which also controls the access to the cloud. A hybrid cloud is a combination of public and private clouds. We discuss the different types in terms of ownership, infrastructure, end-user availability, cost, security, and data location.</p>
<p><strong>Public Cloud</strong></p>
<p>In a public cloud, the cloud infrastructure is owned by a cloud provider and is accessible to the public over the Internet (Figure 1.7). The cloud provider hosts the cloud infrastructure, and end users can access it remotely without the need to buy and setup a working environment (i.e., buying hardware and software). Public cloud resources are shared among different end users. Public cloud users are typically charged for the duration for which these services are used. However, public cloud charge models vary across providers. The security and terms of use are defined by the provider, and hence, end users must work within the constraints of the provider when using their services.</p>
<p><img src="/images/14529700702878.jpg" alt=""><br>Figure 1.7: Public cloud.</p>
<p><strong>Private Cloud</strong></p>
<p>In this second type of cloud, the cloud infrastructure is owned by an organization (Figure 1.8). The infrastructure is accessible to specific users via the organization’s intranet. The cloud environment needs to be procured, set up, operated, and maintained by the organization itself. The private cloud resources are typically shared amongst an organizations end users. Unlike the public cloud, security and terms of use of a private cloud are defined by the organization. The entire infrastructure is located in the organization, hence, security can be compliant with the organization’s policies.</p>
<p><img src="/images/14529700996814.jpg" alt=""><br>Figure 1.8: Private cloud.</p>
<p><strong>Hybrid Cloud</strong></p>
<p>In a hybrid cloud, the infrastructure includes an owned private cloud and a leased public cloud (Figure 1.9). Hybrid clouds enable the idea of “cloud bursting,” in which an organization uses its private cloud for most of its needs and dynamically provisions resources in the public cloud when utilization exceeds the capacity of its private cloud.</p>
<p><img src="/images/14529701211775.jpg" alt=""><br>Figure 1.9: Hybrid cloud.</p>
<p>Other types of clouds continue to emerge, for example, Community Clouds which share infrastructure among different organizations that have common security or other concerns. For example, various non-profit organizations that work closely with government might build and share a community cloud. Another type is Distributed Cloud which provides cloud computing using a set of distributed machines located at different geographical locations. An example is Cloud@Home which leverages volunteered resources as a shared resource.</p>
<h3 id="Popular_Cloud_Stacks"><a href="#Popular_Cloud_Stacks" class="headerlink" title="Popular Cloud Stacks"></a>Popular Cloud Stacks</h3><p>We will now do a quick run-down of cloud stacks that are currently popular in the market. We will quickly glance over the services offered by the major cloud providers, viz. Amazon Web Services, Microsoft, Google as well as OpenStack, the open cloud computing platform.</p>
<p><strong>Amazon Web Services (AWS)</strong></p>
<p>As of 2015, AWS is a market leader in several cloud computing segments, particularly in the IaaS space. Amazon Web Services started by commoditizing and leasing out several services that were developed in-house by Amazon’s engineering team to the wider public. AWS started by offering S3, the object storage service, and then went on to provide EC2, the elastic compute cloud. AWS is currently one of the largest cloud computing companies.</p>
<p>AWS’s stack primarily consists of the following components:</p>
<p>Compute: Amazon’s primary compute solution is Elastic Compute Cloud (EC2), which provides users with virtual machines, or instances of various capacities for hourly or longer term rentals. EC2 forms the backbone of the AWS cloud stack in terms of compute infrastructure. EC2 instances can be managed directly through the AWS EC2 APIs, or through other services such as AutoScaling.</p>
<p>Storage: AWS offers multiple products in this space. Block storage is provided by Elastic Block Storage (EBS) volumes, which can be attached and detached from EC2 instances. Object storage is provided by the simple storage service (S3), which allows for binary large objects (BLOBs) to be stored and retrieved using a simple HTTP service. AWS also offers a varied suite of database services, including RDS which offers a managed SQL service, DyanmoDB, which offers a highly scalable, low-latency key-value store, and ElastiCache, an in-memory database store.</p>
<p>Networking: Amazon’s Virtual Private Cloud (VPC), Elastic Load Balancer (ELB) and Route 53 are networking services that can be used to manage the connectivity between your instances and services deployed in AWS and the outside world.</p>
<p>PaaS Products: AWS’s platforms are large and varied to cater to different application needs. AWS provides a suite of analytics platforms such as Elastic MapReduce (EMR), Amazon Kinesis and Redshift. Rapid web application development is possible through AWS Elastic Beanstalk. Amazon also offers many products to manage and control cloud deployments such as CloudFormation, OpsWorks and CodeDeploy.</p>
<p><strong>Microsoft Azure</strong></p>
<p>Microsoft Azure is one of the fastest growing cloud services in the market, with impressive growth and an increasingly expanding portfolio of cloud services. Azure also leverages Microsoft’s large data center presence worldwide, as well as CDN sites that are spread across 24 countries. Subsets of Microsoft’s Cloud Platform are available as the Windows Azure Pack, which allows an organization to build a private cloud which can seamlessly connect and interact with the Azure public cloud.</p>
<p>Compute: Microsoft offers Azure Virtual Machines, which can be configured to run Windows or various flavors of Linux. The virtual machines are managed by Azure Cloud Services, which provides a multi-language cloud management platform. A unique aspect of Azure is the staging environment and simulator, which allows developers to test out a cloud deployment before putting it into production.</p>
<p>Storage: Azure offers several storage solutions, including: Azure Blobs to store binary large objects; Azure Tables, to store NoSQL tables; and Azure Files, which offer SMB-based storage endpoints (Windows-compatible file servers) to mount and store files in the cloud. Azure also offers managed Relational Database services through the Azure SQL Database; a managed NoSQL document database service, DocumentDB; and high-performance key-value cache through Azure Redis Cache. Microsoft also offers a unique storage appliance called StorSimple, which is an SSD/HDD hybrid storage array deployed at the clients side, and also connects to Azure for backup, analytics and/or cloud deployment.</p>
<p>Networking: Microsoft also offers virtual private networking services through Azure Virtual Network. Another unique feature of Microsoft’s Azure cloud is the ability to purchase dedicated fiber connectivity to Microsoft’s data centers through ExpressRoute. Azure Traffic Manager can be used to load balance traffic to Azure Virtual Machines.</p>
<p>PaaS Products: Azure offers several PaaS products: Azure Websites is the primary PaaS platform, which enables developers to deploy scalable web applications on the Azure platform. Azure Mobile Services allow developers to create the infrastructure required to support mobile applications. In the analytics space, Azure offers several products including HDInsight, which is a managed Hadoop cluster service similar to Amazon’s EMR. Microsoft also offers managed Machine Learning and Stream Analytics services to developers.</p>
<p><strong>Google Cloud Platform</strong></p>
<p>Google’s Cloud Platform initially offered only PaaS products and APIs into Google’s most powerful products such as the Translate API. The Google Cloud Platform has now diversified into multiple services in response to the offerings of its competitors.</p>
<p>Compute: Google’s primary compute platform is the Google Compute Engine (GCE), which offers Linux virtual machines of various sizes depending on the application requirements. A unique differentiator of Google’s platform is that instances are billed by the minute, with a minimum charge of 10 minutes.</p>
<p>Storage: Google offers three primary storage services, namely Cloud Storage, which is an object storage service similar to S3 and Azure Blobs. Google’s Cloud Datastore is the managed NoSQL datastore service that allows users to store non-relational data with high scalability, but optionally supports transactions and SQL queries on your data. In addition, Google offers a traditional managed SQL database service called Cloud SQL.</p>
<p>Networking: Google offers several networking products to manage the connections between Google’s cloud services and the outside world, namely Load Balancing, Interconnect and DNS services.</p>
<p>PaaS Products: Google’s primary PaaS offering is Google App Engine (GAE), which allows developers to deploy an application using Google’s SDK. In addition, Google offers data analytics platforms such as BigQuery, which allows users to run SQL-like queries against multi-terabyte datasets. Cloud Endpoints allows developers to create RESTful services accessible from Mobile and browser clients. In addition, Google’s established products such as Prediction and Translate are available as APIs for access to developers to integrate into their own application.</p>
<p><strong>OpenStack</strong></p>
<p>All of the stacks we have looked at so far are proprietary stacks hosted by the companies on their public clouds. The OpenStack model is markedly different as it’s an open-source cloud stack that is available for both public and private clouds. OpenStack defines itself as a “cloud operating system that controls large pools of compute, storage, and networking resources throughout a datacenter, all managed through a dashboard that gives administrators control while empowering their users to provision resources through a web interface”. OpenStack can be deployed on anywhere from a bunch of machines to an entire datacenter. Public clouds that offer OpenStack include Rackspace and HP Helion.</p>
<p>Compute: OpenStack’s compute offering offers similar services as the public cloud counterparts, with the ability to manage virtualized and commodity server resources, with API-based access. A unique aspect of OpenStack’s compute system (called Nova) is support for a wide range of Hypervisors such as XenServer and KVM, as well as a wide range of hardware support, which includes ARM-based systems.</p>
<p>Storage: OpenStack offers two types of storage services: an object storage service (called Swift), as well as block storage services (called Cinder). These can be deployed and scaled according to environment and the application needs. Database systems can be deployed on top of virtual machines and storage services, if required, but OpenStack does not use or promote any particular type of database solution. Public clouds that use OpenStack, like Rackspace, offer MySQL, Percona or MariaDB deployed on top of OpenStack VMs as a service.</p>
<p>Networking: OpenStack offers a pluggable, scalable and API-driven system called Neutron to manage networks, VLANs and IP address pools for virtual machines. A novel feature of OpenStack networking is support for Software Defined Networks such as OpenFlow, which enable fine-grained configuration of networking hardware in response to provisioning or traffic requirements. More information on Software Defined Networks will be covered later.</p>
<p>PaaS Products: OpenStack itself does not have any PaaS services, but public cloud providers that are built on top of OpenStack have a few. For example, Rackspace provides several platforms for website hosting and managed Hadoop clusters.</p>
<p><strong>References</strong></p>
<ol>
<li>Li Ang, et. al. (2010). CloudCmp: Shopping for a Cloud Made Easy . Proceedings of the 2nd USENIX conference on Hot topics in cloud computing .</li>
</ol>
<h3 id="Cloud_Use_Cases"><a href="#Cloud_Use_Cases" class="headerlink" title="Cloud Use Cases"></a>Cloud Use Cases</h3><p><strong>Use Cases for the Cloud</strong></p>
<p>With the rapid evolution of cloud technologies, there are new use cases emerging every day. In this section, we discuss some of the common cloud use cases.</p>
<p><strong>Web/Mobile Applications</strong></p>
<p>A main driver for cloud computing comes from Web hosting. Websites and Web applications typically are hosted on a server with a dedicated internet connection. Older Web hosting services either provided dedicated servers to clients or gave a fraction of a larger UNIX system to multiple clients. Now, with the advent of cloud computing, Web/mobile applications can be built on top of existing IaaS/PaaS or even SaaS services.</p>
<ul>
<li>SaaS based: Using the SaaS model, organizations can deploy one-size-fits-all applications on the Web. Common examples include WebMail, social networking sites, and utility websites, such as personal organizers, calendars, and planners.</li>
<li>PaaS based: Application developers can use a range of online platforms and tools to create SaaS and mobile applications. Platforms such as Google App Engine, Parse, and AppScale are popular platforms on which Web and mobile applications can be built.</li>
<li>IaaS based: Applications that need even more customization and flexibility can adopt the IaaS model by renting out virtual machines from providers such as EC2 and Rackspace and deploy a fully customized software stack to run the Web application.</li>
</ul>
<p>Consider the following scenarios:</p>
<ul>
<li>Animoto, an online video slideshow creator, decided to deploy a Facebook application. Traffic to the service surged, which resulted in Animoto scaling up from 50 servers to 3,500 servers in 3 days. Such elastic scalability is made possible through cloud computing.</li>
<li>Online retail stores that use cloud computing, such as Amazon and Target.com, have been able to size up infrastructure for peak activity (such as the day after Thanksgiving, or Black Friday). Salesforce.com hosts customers ranging from 2 seat to more than 20,000 seat customers, all using the same Web platform.</li>
</ul>
<p><strong>Big Data Analytics</strong></p>
<p>Many organizations have to deal with large amounts of data. This data may emanate from such areas as sensors, experiments, transactional data, and Web page activity. Big data processing usually requires a lot of computational and storage resources but, depending on an organization’s needs, may be periodic or seasonal. For example, Amazon may have business intelligence and analytics jobs setup for the end of the day, which may require a few hours of time from a few hundred servers. In these scenarios, cloud computing makes sense because these resources can be acquired on demand. Many firms even have fully automated analytics pipelines that automatically collect, analyze, and store data, with resources being provisioned on demand. Examples of big data scenarios include the following:</p>
<ul>
<li>The Union Pacific Railroad mounts infrared thermometers, microphones, and ultrasound scanners alongside its tracks. These sensors scan every train as it passes and send readings to the railroad’s data centers, in which pattern-matching software identifies equipment at risk of failure.</li>
<li>Traditional retailers, such as Walmart, Sears, and Kmart, are following in the footsteps of online retailers, such as Amazon, by analyzing consumer spending habits to offer personalized marketing campaigns and offers to individual customers.</li>
<li>Companies such as Time Warner and Comcast are using big data to track media consumption habits of their subscribers and provide value-added information to advertisers and customers. The video games industry tracks the gameplay habits of millions of console owners. Companies such as Riot Games sift through 500GB of structured data and over 4TB of operational logs every day.</li>
</ul>
<p><strong>On-Demand, High-Performance Computing</strong></p>
<p>Modern science is impossible without high-performance computing (HPC). In addition to physical experimentation, computer-based simulation has become popular in fields ranging from astrophysics, quantum mechanics, and oceanography to biochemistry. Such workloads are computationally demanding and typically are run on dedicated clusters or at supercomputing facilities.</p>
<p>Scientists are now increasingly looking toward the cloud for HPC resource demands. Amazon EC2 offers extremely powerful instances with more CPU and even GPU-acceleration for HPC use. Scientists find the availability of vast amounts of computational power appealing, particularly for small projects or time-sensitive, bursty analytics, such as experimental runs before submitting research paper deadlines. Examples of HPC on the cloud include the following:</p>
<ul>
<li>A 3,809-instance EC2 cluster was set up by Cycle Computing, Inc. for a pharmaceutical company to run molecular modeling jobs. The cluster has a total of 30,472 cores, 26.7TB of RAM, and 2PB of disk storage.</li>
<li>Companies such as Pfizer, Unilever, Spiral Genetics, Integrated Proteomics Applications, and Bioproximity run bioinformatics and genomics workloads on Amazon EC2 instances.</li>
<li>NASA JPL uses high-performance Amazon EC2 instances to process high-resolution satellite images.</li>
</ul>
<p><strong>Online Storage and Archival</strong></p>
<p>One of the important resources that is available through cloud computing is storage. From personal storage solutions, such as Dropbox, to large-scale Internet storage systems, such as Amazon S3, online storage is a major cloud computing use case. The online storage options include:</p>
<ul>
<li>Web-based object storage: Services such as Amazon S3 allow users to store terabytes of data as simple objects that can be accessed over HTTP. Many websites use Amazon S3 to store static content, such as images.</li>
<li>Backup and recovery: Services such as CrashPlan and Carbonite provide online backup of customer data, which is a great option as a secure, off-site backup solution.</li>
<li>Media streaming and content distribution: Services such as Amazon CloudFront not only store large amounts of data but assist in content delivery. Requests to pull data from CloudFront are automatically routed to the nearest server, thereby decreasing latency for time-sensitive media, such as video.</li>
<li>Personal storage: Services such as Dropbox and Google Drive are popular among users to store personal documents online for anytime, anywhere access.</li>
</ul>
<p><strong>Rapid Application Development and Testing</strong></p>
<p>One of the major advantages of the cloud is the ability to rapidly deploy and test applications. An entire computing environment can be deployed in minutes and then torn down and discarded just as easily after the testing is complete. For many companies, the value is in allowing developers to quickly create enhancements and features and test them without any risk. Specialized hardware and servers do not need to be ordered and installed. Within mere minutes, a virtual server can be spun up on EC2. Applications can also be easily stress/load tested. Existing servers can be cloned to perform scalability studies as well.</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>Introduction to Cloud Computing Summary</p>
<ul>
<li>Cloud computing is the delivery of computing as a service over a network, whereby distributed resources are provided to the end user as a utility.</li>
<li>The idea of utility computing originated in the 1950s and 1960s, but the enabling technologies evolved decades later and have finally matured to a state in which cloud computing is a viable option for organizations to invest in.</li>
<li>The enabling technologies of cloud computing are networks, virtualization and resource management, utility computing, programming models, parallel distributed computing, and storage technologies.</li>
<li>Cloud computing consists of four building blocks: application software, development platforms, resource sharing, and infrastructure.</li>
<li>Cloud service models exist at various levels in the building blocks.</li>
<li>Software as a service (SaaS) is at the application software layer and is the delivery of SaaS over the Internet (typically through a Web browser).</li>
<li>Platform as a service (PaaS) is at the development platform layer and can be defined as a computing platform that allows for the creation of Web applications in a simplified manner without the complexity of purchasing and maintaining any of the underlying software and infrastructure.</li>
<li>In the Infrastructure as a service (IaaS) model, providers rent out compute resources in the form of instances or virtual machines, which have some form of CPU, memory, disk, and network bandwidth attached to them.</li>
<li>There are three well-known deployment models for cloud computing: public, private, and hybrid.</li>
<li>Popular cloud providers include Amazon Web Services, Microsoft Azure, Google Cloud Platform and OpenStack. Each provider typically offers a stack consisting of compute, storage and networking services, among others.</li>
<li>Some of the most popular use cases for the cloud include: web and mobile applications, big data analytics, on-demand high performance computing, online storage and archival, and rapid application development and testing.</li>
</ul>
<hr>
<h2 id="Economics_2C_Benefits_2C_Risks_2C_Challenges_and_Solutions"><a href="#Economics_2C_Benefits_2C_Risks_2C_Challenges_and_Solutions" class="headerlink" title="Economics, Benefits, Risks, Challenges and Solutions"></a>Economics, Benefits, Risks, Challenges and Solutions</h2><p>Now that we have covered the fundamental ideas behind cloud computing, it is important to explore some of the benefits, risks and ongoing challenges facing cloud computing. Since one of the main drivers of cloud adoption is reducing upfront costs and leveraging the economies of scale, we will then look at the economics behind the cloud from two perspectives- that of cloud users and service providers.</p>
<p>Since computing has migrated from a product to a service, there should be methods to define relationships between the users and service providers. The cloud has adopted traditional contractual guarantees like Service Level Agreements (SLAs) that meet a cloud user’s Service Level Objectives (SLOs). We will see how Cloud Service Providers define SLAs and SLOs to ensure a level of service to their clients. We will also briefly introduce the concept of cloud service auditing to verify conformity with SLAs.</p>
<p>Apart from contractual concerns, security and privacy are the biggest challenges that are preventing sensitive applications from moving to the cloud. We will explore some of the unique security challenges raised by the cloud and the controls required to resolve them.</p>
<p>Although the cloud has benefited from rapid adoption, there is no one-size-fits-all solution that encompasses all IT requirements. Keep in mind that the cloud is an emerging and fast-evolving paradigm with many active research areas exploring its future.</p>
<h3 id="Benefits_2C_Risks_2C_and_Challenges_of_Cloud_Computing"><a href="#Benefits_2C_Risks_2C_and_Challenges_of_Cloud_Computing" class="headerlink" title="Benefits, Risks, and Challenges of Cloud Computing"></a>Benefits, Risks, and Challenges of Cloud Computing</h3><p><strong>Benefits of the Cloud</strong></p>
<p>The popularity of cloud computing is driven by its numerous benefits, including improved economics, simplified IT management, scalability, flexibility, improved utilization, and a reduced carbon footprint. The following video (Video 1.6) discusses these benefits.</p>
<p><a href="http://youtu.be/uKpFLuqOy-o" target="_blank" rel="external">Video 1.6: The benefits of cloud computing</a></p>
<p>Although presented earlier and in the video above, let us consider the cloud benefits individually:</p>
<ul>
<li>Economic model: Organizations typically estimate their IT requirements for a period of 1 to 5 years in advance in a process called capacity planning. Capacity planning leads organizations to estimate IT investments for peak loads, which could lead to either excess capacity at times (underutilized resources) or deficient capacity when loads exceed projections (which could lead to service degradation). With the pay-as-you-go economic model, organizations pay for the resources that they need. Organizations no longer have to pay upfront cost, invest in and procure expensive computing infrastructure, nor do they have to pay recurring costs to manage their infrastructure. This is particularly important for startups because by leasing compute resources they benefit from reduced upfront cost and reduced time to market when creating and making their offerings available to the general public.</li>
<li>Simplified IT management: Users of cloud services need not allocate time and resources to setup, operate, and maintain their IT resources. The cloud provider, however, competes for clients and hence invests significant resources to manage and maintain their offerings with high reliability.</li>
<li>Scalability: In a traditional, in-house computing environment, organizations can take anywhere from a few days to several months to procure, set up, and operate IT infrastructure. Cloud service providers provision rented computing resources for their clients in a matter of hours or even minutes. Clients not only can scale up resources on demand but can scale them down during lull periods to save money. Therefore, clouds enable the important property of elasticity, wherein resources can be both provisioned and deprovisioned in a dynamic or programmatic manner to adapt to workload changes. In order to support elasticity, many cloud service providers make automated scaling solutions available to dynamically alter resource provisioning as demand fluctuates.</li>
<li>Flexibility: For certain cloud services, providers offer their users the flexibility to configure any software platform to run on any available operating system as a virtualized image on custom-provisioned, rented infrastructure. Cloud offers a shift from an organization’s inflexible IT design decisions (that are tied to specific development platforms and infrastructure) to more flexible, elastic, and modular choices.</li>
<li>Improved utilization: Resource utilization is significantly improved with cloud computing because physical resources are shared across users (multitenancy). Through virtualization, servers are now consolidated as operating system images that are sharing the same system resources. Hence, utilization is improved, which leads to overall savings in power and cooling and reduces the carbon footprint.</li>
<li>Rapid and Global Deployment: By employing the services of cloud service providers that also have a global data center presence, startups can compete with established players by rapidly rolling out applications and services across a global audience. This is particularly important of social media startups which may see viral growth trends as services become popular across multiple countries.</li>
</ul>
<p><strong>Risks of Cloud Computing</strong></p>
<p>By embracing cloud services, users and organizations can take advantage of the above benefits, however, using these services introduces several risks, such as:</p>
<ul>
<li>Vendor lock-in: Cloud computing is slowly becoming standardized. OpenStack is an open source cloud computing platform which aims to standardize the cloud-computing software stack but is not fully compatible with Amazon and is incompatible with current offerings from Microsoft, and Google. Lack of standardization can lead to the situation of vendor lock-in, such as when a client signs up for a nonstandard cloud service, develops applications, and deploys data on it. The lack of standardization makes it unlikely for the client to move to another vendor seamlessly. The client often requires a third-party cloud migration specialist or an additional service to move the application to a different platform.</li>
<li>Security risks: Because cloud computing with public clouds can result in an organization’s data being shipped beyond its four walls, security becomes a primary risk and concern. For certain domains, it is simply unacceptable for users or organizations to do so, in which case they may have to resort to building a private cloud or own resources with restricted access to meet their needs. However, certain markets that have tight security requirements have niche solutions. An example is Amazon GovCloud, which meets certain US federal government requirements for data security and integrity. GovCloud is physically distinct from other cloud infrastructures that Amazon makes available to the public, thereby reducing exposure.</li>
<li>Privacy risks: The use of the cloud also raises many privacy-related concerns. Depending on the laws under which a cloud service provider operates, governments may have the power to search and seize data from the provider without the client’s explicit consent or notification. Furthermore, clients cannot be fully assured of data confidentiality when using public clouds. We discuss some of security risks associated with cloud computing later in this module.</li>
<li>Reliability risks: Clouds are also plagued with reliability issues. In December 2012, Netflix users experienced a service outage due to Amazon’s “connectivity issues and degraded performance” from their servers in Virginia. Amazon EC2’s Northern Virginia data center, one of the most popular public clouds, went down for a few days in 2011, affecting websites such as Reddit and Foursquare. Windows Azure also faced a similar problem, and their services went down for 2½ hours in Western Europe. Public clouds hence are a potential reliability risk that can affect organizations. Clients must design for failures and use features such as Amazon’s multiple availability zones, in which clients can set up failover and redundant infrastructures to take over in case of failure, which comes at a price, of course. Cloud users attempt to mitigate the cloud reliability risk by signing Service Level Agreements (SLAs) that enable compensation when exposed to such events. Since cloud services can only be accessed over the network, any disruption of connectivity will cause the application to fail, leading to a loss of reputation and/or revenue.</li>
</ul>
<p>Some of these risks are not specific only to cloud computing, but are typical for any service provider- be they banking or health services. As with any other service, cloud service providers must carefully consider the implications of these risks and design solutions to mitigate them as their credibility and reputation directly impacts their rate of adoption. Cloud adopters who offer their own services must also safeguard their reputation against these risks.</p>
<p><strong>Challenges in Cloud Computing</strong></p>
<p>Along with the benefits and risks, there are several challenges associated with the adoption of cloud computing:</p>
<ul>
<li>Application engineering and development: A cloud inherently offers the promise of on-demand, dynamically scalable infrastructure. Programming a cloud, however, is more complex than writing code for a single machine. New programming paradigms (such as MapReduce, Spark or GraphLab), coupled with provider APIs to manage infrastructure, help developers manage complexity but still present a steep learning curve. In addition, skilled developers with cloud experience are rare, and both costs and time for application development increase substantially with clouds. Finally, these new cloud programming models and APIs are continually evolving, which may add to recurring engineering and development costs.</li>
<li>Movement of data: Use of public clouds typically requires connecting to the cloud over the Internet. Because of this requirement, movement of data to and from the cloud is significantly slower than in an organization’s local area network (LAN). Although the cloud allows applications to target large amounts of data (big data), data movement can become a limiting factor for cloud adoption. For example, Amazon allows clients to upload large datasets for free or ship hard disks with data so that they can load them into the required cloud service.</li>
<li>Quality of service (QoS): As mentioned earlier, cloud infrastructure is typically shared among many users. This sharing presents a challenge for cloud providers to offer QoS assurances to their clients. This challenge could disallow certain performance-sensitive applications from being migrated to the cloud. QoS in clouds is an important area in cloud research. For example, regulating I/O bandwidth to specific virtual machines could offer predictable performance for critical applications. I/O virtualization is covered in Unit 3.</li>
</ul>
<h3 id="Business_Case_for_Cloud_Computing__28for_Users_29"><a href="#Business_Case_for_Cloud_Computing__28for_Users_29" class="headerlink" title="Business Case for Cloud Computing (for Users)"></a>Business Case for Cloud Computing (for Users)</h3><p><strong>Evolution of the IT Business Model</strong></p>
<p>An organization’s IT costs are many-fold, which include expenses for hardware and software as well as expenses for support and management. Typically, these costs fall into two categories:</p>
<ul>
<li>Capital expenses (CapEx): The initial investment for a particular IT service or solution. For example, when an organization decides to implement a software solution to address a particular need, say, enterprise resource planning (ERP), CapEx would include all physical hardware and software purchases. CapEx investments are for the lifetime of the long-lived solution. CapEx are an upfront expense, which are either paid as a lump sum or financed with extra charges.</li>
<li>Operational expenses (OpEx): The recurring costs incurred while operating a particular system. For the ERP case, that would include utility fees (such as power and cooling) to keep the infrastructure running, space leases if the facility is rented, personnel costs to support the system, and software support and license fees. OpEx are typically a monthly recurring charge.</li>
</ul>
<p>The business model for IT software has evolved over the years into the following forms:</p>
<ol>
<li>Traditional model: An organization purchases licensed software, which it then owns and maintains.</li>
<li>Open-source model: Software is essentially free, but the organization pays vendor support costs.</li>
<li>Outsourcing model: An organization hires another company, possibly overseas, to manage and maintain the software.</li>
<li>Hybrid model: A software vendor sells highly standardized software to many clients, along with software management and support, thereby amortizing costs of expertise, software management, and support over several clients.</li>
<li>Cloud computing model: Software is developed and delivered over the Internet to many clients at lower costs.</li>
</ol>
<p>The following video (Video 1.7) discusses the evolution of these models with examples: </p>
<p><a href="http://youtu.be/yOTcTwWbEpk" target="_blank" rel="external">Video 1.7: Economics of cloud computing, software service models, and costs</a></p>
<p><strong>Reducing Capital Expenditure</strong></p>
<p>Organizations choose to reduce their capital expenditures so that they limit the commitment of large investments for long-lived IT resources. Shifting expenses away from capital expenditures into operational expenditure enables organizations to stretch their IT budgets and limit upfront costs. Specifically, organizations opt to make investments that have a bigger return on investment in the short term rather than investing in long-lived, depreciating IT resources. Operating expenses are pay-as-you-go, meaning organizations pay by the month and get value every month. With cloud computing, they can simply rent the resources and incur little to no capital expenditures.</p>
<p>The Cloud Computing paradigm offers a transition of the IT Business Model from CapEx to OpEx. CapEx in IT systems is a long-term investment that freezes a large sum of money into a single investment. OpEx, on the other hand, is a recurring expense which could enable the company the agility to utilize the funds in other profit yielding investments.</p>
<h3 id="Economics_of_Cloud_Computing__28for_Providers_29"><a href="#Economics_of_Cloud_Computing__28for_Providers_29" class="headerlink" title="Economics of Cloud Computing (for Providers)"></a>Economics of Cloud Computing (for Providers)</h3><p><strong>Cloud Service Provider Economics</strong></p>
<p>Since the cloud user has no longer needs to invest in Capital Expenditures, from a cloud service provider’s perspective, long-term CapEx as well as recurring OpEx costs are unavoidable. An important challenge for cloud service providers is to satisfy the demands of their clients while achieving high-average utilization in order to make a profit, which depends on their ability to build data centers with high efficiency and reliability at manageable costs. Cloud service providers amortize their costs over a large number of users.</p>
<p>Cloud service providers build large and reliable data centers in order to attract a large number of users in order to improve their profitability. Just like other utility providers, cloud service providers can then procure and maintain hardware and software at significant savings per unit.</p>
<p><strong>Economies of Scale</strong></p>
<p>Cloud service providers organize their infrastructure into large data centers, which typically leverage three main areas:</p>
<ul>
<li>Supply-side savings: Large-scale data centers lower costs per server.</li>
<li>Demand-side aggregation: Aggregating demand for computing allows server utilization rates to increase.</li>
<li>Multitenancy efficiency: When changing to a multitenant application model, increasing the number of tenants (i.e., customers, or users) lowers the application management and server cost per tenant.</li>
</ul>
<p>Cloud service providers undertake the difficult task of building and maintaining data centers for users. For this model to be feasible, cloud service providers will have to leverage economies of scale and bring in many users. Providers benefit from economies of scale in the following areas:</p>
<ul>
<li>Cost of power: Electricity is rapidly becoming the largest element of total cost of ownership (TCO) in a data center, contributing to approximately 15% to 20% of total costs. Large cloud service providers can place their data centers in locations with lower cost of power and sign bulk purchase agreements with electric providers to reduce electric costs significantly.</li>
<li>Infrastructure labor costs: Cloud computing enables repetitive management tasks to be automated. In addition, in larger facilities, a single system administrator can service thousands of servers with the use of advanced management software.</li>
<li>Buying power: Cloud service providers can purchase equipment in bulk from manufacturers, which can lead to major discounts over smaller buyers. In addition, cloud providers standardize their servers and equipment, which helps in lowering purchase and support costs compared to smaller IT departments.</li>
</ul>
<p>Technologies in data centers and their design considerations are covered in detail in Unit 2.</p>
<h3 id="Service_Level_Agreements_and_Objectives"><a href="#Service_Level_Agreements_and_Objectives" class="headerlink" title="Service Level Agreements and Objectives"></a>Service Level Agreements and Objectives</h3><p>In this course so far, we have talked about the fundamental ideas behind cloud computing and some of the service models that have emerged under the cloud computing paradigm. Assuming an organization wants to move their infrastructure and services to a cloud provider, several questions arise. For example, how does an organization:</p>
<ul>
<li>define its requirements in terms of the services that they require from the cloud service provider?</li>
<li>identify the type and quantity of the services that it requires?</li>
<li>negotiate the level of service and support that it expects from a cloud provider?</li>
<li>monitor and validate the type and quality of service that was guaranteed by the cloud service provider?</li>
</ul>
<p>When an organization needs to formally state its service requirements in business and legal terms, it defines these requirements in terms of service level objectives.</p>
<blockquote>
<p>Service Level Objective(definition)<br>A service level objective is defined as a key element that defines some aspect of the service which is expected from the service provider.</p>
</blockquote>
<p>A common service level objective with cloud service providers, for example, is an uptime guarantee, where-in a service is guaranteed to be available and running within normal operational parameters for a certain percentage of the time.</p>
<p>Service level objectives are typically defined and negotiated between the client and a service provider in a larger contract known as the service level agreement.</p>
<p><strong>Service Level Agreements</strong></p>
<blockquote>
<p>Service-Level-Agreement(definition)<br>A service level agreement (SLA) is a contract between a service provider (either internal or external) and the client that defines the level of service expected from the service provider.</p>
</blockquote>
<p>Service level agreements exist in many industries in a supplier-customer relationship exists for a service that is provided by the supplier to the customer periodically. Service level agreements in information technology, in their current form, have been used since late 1980s by fixed line telecom operators as part of their contracts with corporate customers.</p>
<p>A typical SLA may consist of the following segments:</p>
<ul>
<li>a definition of services to be provided by the service provider to the client,</li>
<li>methods to measure performance,</li>
<li>protocols to manage problems,</li>
<li>a list of customer duties,</li>
<li>warranties that need to be honored by the service provider,</li>
<li>procedures involved for disaster recovery, and</li>
<li>process and policies regarding the termination of the agreement.</li>
</ul>
<p><strong>SLAs in Cloud Computing</strong></p>
<p>SLAs have evolved over the years to cater to different types of IT services. The evolution of shared infrastructure services such as clouds have necessitated the use of strong service level agreements. SLAs by definition can define any level of service, but a well-structured and negotiated SLA between a cloud service provider and a client will ideally [1] :</p>
<ul>
<li>Codify the specific parameters and minimum levels required for each element of the service, as well as remedies for failure to meet those requirements.</li>
<li>Affirm the client’s ownership of its data stored on the service provider’s system, and specifies the client’s rights to get it back.</li>
<li>Detail the system infrastructure and security standards to be maintained by the service provider, along with the client’s rights to audit their compliance.</li>
<li>Specify the client’s rights and cost to continue and discontinue using the cloud service provider’s service.</li>
</ul>
<p><strong>Auditing in Cloud Computing</strong></p>
<p>Although cloud computing provides numerous advantages, one of its main challenges continues to be the reliability of cloud services. A fast evolving approach to address reliability is cloud auditing. Let’s assume that a client has employed one or more cloud services from a cloud service provider. The cloud computing business model abstracts away many aspects of the infrastructure from the client which now become the responsibility of the cloud service provider. The cloud services are managed by the cloud service provider to implement the services agreed upon in the SLA.</p>
<p>Auditing evaluates whether the cloud services comply with the SLA through monitoring. A third party auditor is requested and trusted by the client to assess the cloud service(s). Hence, public auditability of cloud services is necessary to allow clients to resort to an external auditor to check the integrity of the cloud services. The cloud service provider makes available resource usage and performance monitoring and takes measures to ensure the security of its services to its clients through providing physical security, isolation, authentication, firewalls and APIs. A third party auditor should be able to efficiently audit the cloud services without overloading the client and without adding vulnerabilities to the client’s services.</p>
<p>Given the nature of cloud services, near real time auditability is becoming necessary. This requires real time monitoring and evaluation in order to trigger a rapid response to safeguard the client’s service and reputation. In public clouds, this has to be achieved while preventing the exposure of client data to other cloud clients. Near real-time auditing is rapidly evolving and becoming a requirement for reliable cloud computing services which will require audit trails and monitoring of service, performance and security metrics among others.</p>
<p><strong>References</strong></p>
<ol>
<li>Thomas Trappler If It’s in the Cloud, Get It on Paper: Cloud Computing Contract Issues . <a href="http://www.educause.edu/ero/article/if-its-cloud-get-it-paper-cloud-computing-contract-issues" target="_blank" rel="external">http://www.educause.edu/ero/article/if-its-cloud-get-it-paper-cloud-computing-contract-issues</a>.</li>
</ol>
<h3 id="Cloud_Security_-_Threats"><a href="#Cloud_Security_-_Threats" class="headerlink" title="Cloud Security - Threats"></a>Cloud Security - Threats</h3><p>Now that we understand how the agreement between client and provider is met, let’s take a look at a major concern for cloud service providers and users alike: security.</p>
<p>As cloud service providers compete for market dominance, their security features have become a key service differentiator.</p>
<p>At one level, cloud service providers can leverage the economies of scale. By implementing security measures at a large scale, they are able to provide more affordable defensive mechanisms at a lower cost. Typically, this includes network monitoring and filters, patch management, hardening, incident response &amp; forensics, and various types of threat management.</p>
<p>They also generally provide an accessible interface to modify security settings, allowing secure key rotation, timely updates and patches. Additionally, since all actions are virtualized, these can be regularly snapshotted and analyzed forensically for exploitation using vulnerabilities that are yet unknown (also known as zero-day vulnerabilities).</p>
<p>Let us look at the cloud from the point of view of a traditional enterprise which used in-house IT infrastructure. Enterprises find that they lose control as a function of asset ownership as they move away from traditional servers towards private clouds and then up the stack from IaaS to SaaS (Figure 1.10). In all three service models, the cloud vendor has full ownership of the underlying infrastructure (networks, storage and hosts). In PaaS, the service provider may additionally claim partial ownership of the application infrastructure. Finally, in the SaaS model, the application infrastructure is fully owned by the service provider.</p>
<p><img src="/images/14529717394416.jpg" alt=""><br>Figure 1.10 - Enterprises lose control as you move up the public cloud stack</p>
<p>In all three models, however, the enterprise has full ownership over all its data. Unfortunately, it does not have full control over this data, as it is stored outside the network perimeter. This lack of control over sensitive data storage and transfer is one of the leading inhibitors to large-scale cloud adoption. Two-thirds of potential adopters have placed “data security and privacy” as the biggest risk in cloud computing (Figure 1.11).</p>
<p><img src="/images/14529717521135.jpg" alt=""><br>Figure 1.11 - Security concerns are the biggest barrier to large-scale cloud adoption</p>
<p><strong>Threats</strong></p>
<p>The biggest threats at a high-level are those caused due to vendor lock-in (since applications are not very portable between platforms), compliance risks (e.g. meeting most compliance standards is more complex on public clouds), and a loss of governance (most cloud service providers do not provide SLAs related to data security assurances).</p>
<p>At a lower level, the threats are due to shared infrastructure, lack of a hard perimeter, and limited control over physical data storage and deletion. It must be noted that attacks against hypervisors and shared hosting are significantly rarer and more difficult than attacks against OSes and networks that plague traditionally deployed applications.</p>
<p>The Cloud Security Alliance provides a taxonomy of threats, which are summarized here:</p>
<p><strong>Threat ##1: Abuse and Nefarious Use of Cloud Computing</strong>(IaaS, PaaS)</p>
<p>Criminals can leverage the anonymity provided by public clouds to launch malicious attacks at low cost. Public clouds have been used for Command and Control Botnets, CAPTCHA cracking, rainbow table computation, launching dynamic attacks. Each of these is a malicious action that relies on brute force, which is provided by the data center.</p>
<p>Vendors have attempted to counter this threat by adding strict registration checks and comprehensively monitoring all network traffic. For instance, a cloud service provider may monitor metadata about all emails originating from it to find out if a customer is misusing it to send spam.</p>
<p><strong>Threat ##2: Insecure Interfaces and APIs</strong>(IaaS, PaaS, SaaS)</p>
<p>As mentioned earlier, cloud vendors provide easy to use consoles, dashboards, interfaces and web services to interact with the cloud. However, this brings an additional threat to the entire network if any vulnerability exists in these interfaces. Thus, even if the entire cloud infrastructure is designed securely, a single vulnerability in the provider’s website may allow an attacker to take over a customer’s account.</p>
<p>Most vendors now use strong, multi-factor authentication, detailed logging, anomaly detection and secure defaults to counter this threat. Web interfaces are released to the public only after extremely strict checking for vulnerabilities in the code as well as in the implementation.</p>
<p><strong>Threat ##3: Malicious Insiders</strong>(IaaS, PaaS, SaaS)</p>
<p>The threat of malicious insiders is expanded when using public clouds. Unlike an in-house IT deployment, Enterprise employees are now not the only ones with access to the datacenter. Since the service runs on an external machine and stores data on the provider’s resources, it is always possible that a disgruntled or motivated employee of the cloud service provider could do something that adversely impacts the provider’s service.</p>
<p>To counter this threat, cloud service providers enforce strict standards for employees and conduct detailed audits and monitoring. They also contractually define HR and breach notification policies as a part of the service contract.</p>
<p><strong>Threat ##4: Shared Technology Issues</strong>(IaaS)</p>
<p>This is one of the fundamental “new” threats due to the cloud paradigm. The cloud works by providing multiple tenants (for e.g., you and your classmates) with virtualized access to shared infrastructure. Isolation between co-tenants is provided by a sandbox known as a “hypervisor”, which mediates access between virtual machines and the underlying infrastructure.</p>
<p>Although co-tenants should be unable to access their neighbors’ details, several exploits over the years have allowed tenants to break out of their sandboxes and steal data from another tenant’s memory, network etc. An example of this could be that as you write code on AWS to solve class projects, a classmate manages to log in to a VM on the same physical machine as you, and then use some properties of the shared physical machine to steal your code. Isolating all users completely is a very hard problem, even with the hypervisors of today.</p>
<p>To mitigate this, cloud service providers add strong monitoring capabilities, using SLAs to ensure timely vulnerability and patch management, and conducting regular audits. Apart from that, hypervisors must be periodically hardened against any potential new attacks.</p>
<p>For e.g. <a href="http://xenbits.xen.org/xsa/" target="_blank" rel="external">the xen security advisory page</a> shows security advisories against the Xen hypervisor. Each time an attack becomes known, AWS must patch all their servers so that an attacker cannot use these exploits.</p>
<p><strong>Threat ##5: Data Loss or Leakage</strong>(IaaS, PaaS, SaaS)</p>
<p>This is another drawback of externally hosted clouds. Often, regulations mandate that an enterprise bear legal responsibility for any sensitive data that is used or stored by their applications.</p>
<p>Even if this data is encrypted and stored on the cloud, the key must also be on the cloud to decrypt this data.</p>
<p>New research in encryption technologies has led to the rise of homomorphic and split-key management. Homomorphic keys allow data processing to be carried out on encrypted data. Thus, the key itself does not need to be transferred to the cloud. Split-key solutions work by having a master key (stored securely off-cloud) and several per-application/module slave-keys. As the master is never on the cloud, the threat of data breach is reduced.</p>
<p>Unfortunately, these techniques are still limited and costly to implement. At a contractual level, it is important to define backup, data retention, data wiping, secure key management and storage processes and sufficient auditing privileges. This implies that a certain set of well-designed standards must be followed, such as the <a href="http://csrc.nist.gov/publications/nistpubs/800-144/SP800-144.pdf" target="_blank" rel="external">NIST Guidelines on Security and Privacy in Public Cloud Computing</a>.</p>
<p>It is important to also deal with all jurisdictional issues in the contract. Even though data is stored with the cloud service provider, contracts are framed so that any liability in case of a breach is due to the application owner. Hence, most cloud service providers are required to complete ISO 27001, SAS-70 and other region-relevant audits, which indicate the process maturity and the presence of security controls.</p>
<p><strong>References</strong></p>
<ol>
<li>Anthes, Gary (2010). “Security in the Cloud .” Communications of the ACM. Number 53.11. 16-18 Pages.</li>
<li>Nanavati, Mihir (2014). “Cloud Security: A Gathering Storm .” Communications of the ACM. Number 57.5. 70-79 Pages.</li>
<li>Top Threats Working Group (2013). The Notorious Nine: Cloud Computing Top Threats in 2013 .</li>
</ol>
<h3 id="Cloud_Security_-_Control__26amp_3B_Auditing"><a href="#Cloud_Security_-_Control__26amp_3B_Auditing" class="headerlink" title="Cloud Security - Control &amp; Auditing"></a>Cloud Security - Control &amp; Auditing</h3><p>When running an application on the cloud, different aspects of security must be controlled by different entities. For instance, Figure 1.12 (from AWS) shows the break up of security responsibilities between the provider and the customer.</p>
<p><img src="/images/14529720083304.jpg" alt=""><br>Figure 1.12 Security Responsibilites in AWS.</p>
<p>Many classes of applications require different infrastructure, process and security certifications. Most cloud service providers will comply with a majority of the popular certifications and audit requirements followed in the US and Europe. The following table from Putcher et. al. [3] compares the most popular providers (Figure 1.13):</p>
<p><img src="/images/14529720330687.jpg" alt=""><br>Figure 1.13 Security Responsibilites in AWS.</p>
<p>Understanding the details of these certifications is not a goal for this course, but Mather [1] provides a good reference for those interested in digging deeper into these aspects.</p>
<p>To develop an application that passes these compliance checks, both the cloud service providers as well as application developers must apply a minimal set of security controls, which we will explore below. As with the rest of this course, we look at controls from a predominantly IaaS perspective. Obviously, as we move up the stack, the cloud service provider has to ensure the security of the resources it is responsible for.</p>
<p>For an IaaS cloud, the following table gives an overview of the security controls to be implemented by both parties:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Domain</th>
<th style="text-align:left">Cloud Service Provider’s Responsibility</th>
<th style="text-align:left">Customer Responsibility</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Identity and Access Management</td>
<td style="text-align:left">A cloud service provider must provide information to customers about who is using the service. This requires that they: (1)Deliver and maintain an authentication service (so that users cannot access resources without explicit privileges) (2)Create a service that allows account management policy configuration (this means that customers can add/remove users, roles) (3)Adopt insider misuse protections (monitor employees, restrict access to sensitive server locations)</td>
<td style="text-align:left">Using the authentication and access control service provided by the cloud provider, customers must: (1)Define roles, groups and permissions (2)Create and disseminate credentials (3)Use access control logging (this means that the customer will have a log of all sensitive user events) (4)Use Multifactor Authentication where appropriate</td>
</tr>
<tr>
<td style="text-align:left">Availability and Fault-tolerance</td>
<td style="text-align:left">To ensure that the cloud is resilient to failure, cloud service providers must have: (1)Tape backups and redundancy of storage, compute systems (2)Geo-distributed datacenters</td>
<td style="text-align:left">The redundancy provided by the cloud service provider has to be leveraged by the customer, who should: (1)Add redundant options for connectivity to all endpoints (2)Use application-layer backups and snapshots of instances, storage state (a snapshot of a VM instance, or a database, stores its state at a fixed moment in time, allowing a recovery to be performed from that point)</td>
</tr>
<tr>
<td style="text-align:left">Patching &amp; Configuration Management</td>
<td style="text-align:left">(1)Ensure sandboxing of tenants using hypervisors, overlay networks (this will be explained later) (2)Regular vulnerability assessments and penetration testing (when an internal or external team of “hackers” systematically attempts to break into a system) of bare metal, hypervisor and networks</td>
<td style="text-align:left">(1)Patch OS, machine images with latest security updates (2)Use appropriate user roles with the least privilege for each application (for e.g. when you are running a web server on the cloud, ensure that it does not have access to any infrastructure keys, or even to local “root”. This way, if your website is breached, the rest of your application is isolated) (3)Restrict traffic to instances using firewalls, Virtual Private Clouds, and segment network into zones (block all network traffic from untrusted sources)</td>
</tr>
<tr>
<td style="text-align:left">Monitoring &amp; Detection</td>
<td style="text-align:left">Verify that customer resources are not being used for nefarious activities (either intentionally or unintentionally), and take appropriate actions</td>
<td style="text-align:left">(1)Install host-based Intrusion Detection and Anti-Malware systems (these detect any misuse of your cloud network or hosts) (2)Define alerts and response strategies for incidents and breaches (be prepared for attacks and automate a recovery and logging protocol)</td>
</tr>
<tr>
<td style="text-align:left">Data Security</td>
<td style="text-align:left">(1)Cross-tenant data access controls and privacy safeguards (as described on the previous page, ensure that customers on the same physical infrastructure are isolated) (2)Data integrity verification and repair from redundant data stores (when storing data in several replicas, ensure their consistency and accuracy)</td>
<td style="text-align:left">(1)Use secure protocols (like SSL/TLS, IPSec) for data in transit (these ensure that your network traffic cannot be read) (2)Encrypt data-at-rest (encrypt all the data you store on the cloud, such that even a rogue employee of the cloud service provider cannot disclose this information)</td>
</tr>
<tr>
<td style="text-align:left">Cryptographic Object Security</td>
<td style="text-align:left">(1)Support data encryption in all provided storage/file systems/DBs (for e.g. Windows environments could allow Bitlocker implementations) (2)Securely manage customer’s account and access credentials</td>
<td style="text-align:left">(1)Create and distribute access keys (for cloud service provider APIs) as well as remote connectivity (like SSH, VNC, RDP) (2)Do not store keys on cloud where possible, so that key will not be in the same place as the data.</td>
</tr>
</tbody>
</table>
<p>Most providers will often build in services simplifying the process for customers to implement security controls. For e.g. AWS provides Security Groups, which is simply an external network firewalls.</p>
<p>The process of verifying the presence of these controls is known as a security audit. These can be done internally (by hiring a technical consultant) or externally (by a certifying agency). To host sensitive information on the cloud, both the provider and the customer must pass these audits. However, the lack of demarcation of responsibilities in case of a breach have meant that in most cases, applications using sensitive information like bank records or medical data cannot be hosted “on-the-cloud”.</p>
<p><strong>References</strong></p>
<ol>
<li>Mather, Tim et. al. (2009). Cloud security and privacy: an enterprise perspective on risks and compliance.</li>
<li>Winkler, Vic J.R. (2011). Securing the Cloud: Cloud Computer Security Techniques and Tactics . Elsevier.</li>
<li>Pucher, Alex et. al. (2012). A Survey on Cloud Provider Security Measures .</li>
</ol>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p><strong>Cloud Building Blocks Summary</strong></p>
<ul>
<li>Cloud computing offers many benefits, including a pay-as-you-go economic model, simplified IT management for users, scalability, flexibility, improved utilization, and a decreased carbon footprint.</li>
<li>Cloud computing also has many risks and challenges, including vendor lock-in, security risks, privacy-related concerns, developer training and reengineering, evolving tools, and movement of data.</li>
<li>Cloud computing offers a compelling economic model for businesses through the pay-as-you-go model and can significantly lower management and overall costs of IT.</li>
<li>Cloud service providers leverage economies of scale to provide services at low costs. They require large data centers and many clients in order to amortize the costs over the entire user base.</li>
<li>Service-level objectives (SLOs) allow an organization to formally state its service requirements to a service provider</li>
<li>A Service-level agreement (SLA) is a contract that is negotiated between a service provider and a client that defines the level of service expected from the service provider</li>
<li>Auditing evaluates whether the cloud services comply with the SLA through monitoring. As with auditing in other fields, a mutually trusted third party is involved in the process to ensure compliance and fairness.</li>
<li>The shared and public nature of cloud computing introduces new risks in terms of information security. The primary threats are abuse of clouds, insecure interfaces and APIs, malicious insiders, shared technology issues and data loss or leakage.</li>
<li>The responsibility of securing applications on the cloud is split between the cloud service provider and the client. The provider must make available controls and systems that can be used to secure applications, resources and data on the cloud. The client must make sure that they make full use of the provided controls and systems and follow best security practices when using clouds.</li>
</ul>
<hr>
<h1 id="Cloud_Infrastructure"><a href="#Cloud_Infrastructure" class="headerlink" title="Cloud Infrastructure"></a>Cloud Infrastructure</h1><h2 id="Data_Center_Trends"><a href="#Data_Center_Trends" class="headerlink" title="Data Center Trends"></a>Data Center Trends</h2><p>In this unit we will present and discuss the data center which is a collection of physical computing resources that are provisioned and shared within the cloud computing paradigm. Innovations in data center efficiency and management are important enablers of the economics behind the cloud computing model. Recall in the previous unit, we discussed some of the benefits of cloud computing as well as the economics behind it. Major advances in data center design play a fundamental role in being able to make this possible. Large-scale data centers can house and run infrastructure more efficiently at scale than what most organizations can manage at small scale with their own infrastructure. Understanding data centers will enable you to be more informed, in terms of performance, cost and potential sources of failure, as you attempt to deploy robust applications and services on the cloud.</p>
<p>This module will serve as a starting step in understanding a few data center fundamentals. We will start with the definition and origins of a data center, followed by a discussion of the current trends in data center technologies: namely the increase in power and resource densities of data centers, as well as the focus on power and efficiency in data centers.</p>
<h3 id="Introduction_to_Data_Centers"><a href="#Introduction_to_Data_Centers" class="headerlink" title="Introduction to Data Centers"></a>Introduction to Data Centers</h3><p><strong>Data Centers</strong></p>
<p>Data centers include a room or building, IT equipment, and facilities to securely house, power, and cool that equipment. Over the years, data centers have evolved from a location of concentrated IT equipment to modular, agile, and highly virtualized compute centers. With growing use of Web-based services, explosion of mobile devices, and ever-increasing rates of data generation (and consumption), the demand for new data centers continues to grow. One of the main contributors to this growth has been the advent of the cloud computing paradigm, in which cost effectiveness is directly linked to economies of scale and the efficiencies gained with new data center design. All layers of cloud software and services run on top of physical resources, largely servers, storage, and networking equipment, and all of these require power. This equipment also generates heat and so requires cooling. A small data center might fit in one specialized room, while a large installation might be a dedicated, warehouse-sized facility (see Video 2.1).</p>
<p><a href="http://youtu.be/ouhskMuknoM" target="_blank" rel="external">Video 2.1: Data centers.</a></p>
<p>Data center design requirements depend on its size and use. Cloud-centric data centers could come in two varieties. An infrastructure as a service (IaaS) cloud provider offers a variety of machine types, and the customers pick and choose to build their own applications. Software as a service (SaaS) and Platform as a service (PaaS) providers typically use large-scale (many thousands) homogenous compute nodes with custom applications that are presented to end users directly. Other types of data centers include enterprise/traditional IT, which houses computers to support functions for day-to-day business operations, and high-performance computing (HPC) data centers, which house large clusters for scientific applications.</p>
<p>In the last 5 years, specific attention has been paid to the efficiency of data centers, dramatically decreasing their operational costs and carbon footprint. This increase in efficiency has led to a fast evolution of data center design, and these trends are likely to continue.</p>
<p>Effective use of cloud resources and development of large-scale, dynamic applications for the cloud require an understanding of the physical resources that make up the cloud. In this unit, we start with trends in data centers, present components that make up a data center, and discuss data center design considerations and requirements.</p>
<p><strong>Why Study Data Centers?</strong></p>
<p>If you think of the cloud as a massive computer, [1] you can still break it down into its constituent parts—processors, memory, and switch. [2] When you are programming for the cloud, you are writing programs that solve a problem or provide a service but with the ability to scale.</p>
<p>A few decades ago, to be a computer user meant to be a programmer. Early programmers knew the instruction set architecture (ISA) well and, because hardware resources were scarce, had to optimize in assembly language. With the advent of high-level languages, such as C/C++, Java, and Python, why do students still have to learn computer organization, caching, and assembly language? When you know what the compiler is doing for you, it makes you a better programmer because you understand what is going on behind the scenes. Similarly, you become better at debugging.</p>
<p>Fast forward to today, when you are developing applications on the cloud, there does not yet exist a compiler that allocates massive virtualized resources automatically to solve your specific problem. It is up to you, the cloud programmer, to do the management and to make your applications cost efficient at scale. Analogous to understanding the components within a single computer, knowing the underlying components of a data center will improve your abilities to program and debug your cloud-based applications.</p>
<p>Most of you will never go on to design and build your own large-scale data center, but understanding what goes into implementing the underlying infrastructure will help you appreciate all of things that cloud providers are doing for you.</p>
<p><strong>References</strong></p>
<ol>
<li>Barroso, Luiz André, and Urs Hölzle (2009). “The datacenter as a computer: An introduction to the design of warehouse-scale machines.” Synthesis Lectures on Computer Architecture.</li>
<li>Gordon Bell and Allen Newell (1970). “The PMS and ISP descriptive systems for computer structures.” Joint Computer Conference.</li>
</ol>
<h3 id="Definition_and_Origins"><a href="#Definition_and_Origins" class="headerlink" title="Definition and Origins"></a>Definition and Origins</h3><p><strong>Defining Data centers</strong></p>
<p>Formally, a data center can be defined as follows:</p>
<blockquote>
<p>Data center(definition)<br>Infrastructure dedicated to housing computer and networking equipment, including power, cooling, and networking.</p>
</blockquote>
<p>The term data center became popular in 1990s, referring to large rooms dedicated to housing computer and networking equipment, though computer rooms themselves date back much further. Early computers (mainframes) were massive—the size of many refrigerators. They also generated a lot of heat and required clean air filtration to increase reliability. For these reasons, early computers could not be placed into a regular office, so custom rooms were built. A lot of these same ideas go into server rooms today. The only difference is that instead of housing one computer, they hold from dozens to hundreds to even tens of thousands of servers in a single facility.</p>
<p>Modularity is important for data centers because it allows an organization to expand as needed. One of the enablers of a modular data center has been standardized racks onto which IT equipment is mounted. Historically, server racks have evolved from early relay racks found in railroad signaling. It is unclear why the railroad companies chose the original 19-inch, post-to-post width, but the same form factor made its way into early telecommunications and then audiovisual equipment at radio and television stations. Figure 2.1 shows early equipment racks in a radio operators room.</p>
<p>Did you know?</p>
<p>The width of early railroad relays dictated the width of a modern 19-in. rack. But the standard gauge of modern railroad tracks (4 ft, 8½ in.) dates back to ancient Greek stone pathways, which the Romans adopted and brought to Europe during the age of the Roman Empire (Wikipedia, 2014).</p>
<p><img src="/images/14536650557456.jpg" alt="Figure 2.1: 1940s radio operators room showing early equipment racks (Inland Marine Radio History Archive, 2012)."></p>
<p>A 1933 U.S. patent, F.C. Lavarack, 1,919,166, is an example of a standardized equipment rack for relays. In Figure 2.2, you can see some of the original drawings in the patent.</p>
<p>Some of the advantages of Lavarack’s design over common predecessors include:</p>
<ul>
<li>Fire safety: Rack posts and other fittings were cast out of iron (and later steel), this became superior to earlier wooden enclosures that could possibly ignite and damage all equipment inside.</li>
<li>Field assembly: Racks could be assembled using common hand tools and low-skill workers.</li>
<li>Regularly spaced holes: Support a wide variety of equipment.</li>
<li>Vertical mounting surface: Easier installation, maintenance, and wiring.</li>
</ul>
<p>Many of today’s standard, 19-inch equipment racks have evolved from Lavarack’s design.</p>
<p><img src="/images/14536650902905.jpg" alt="Figure 2.2: Relay rack patent drawings (figure from Relay Rack patent)."></p>
<h3 id="Size_2C_Density_and_Efficiency_Growth"><a href="#Size_2C_Density_and_Efficiency_Growth" class="headerlink" title="Size, Density and Efficiency Growth"></a>Size, Density and Efficiency Growth</h3><p><strong>Growth of Data Centers</strong></p>
<p>Over the past few decades, data centers have grown both in size (in terms of the number of racks and cabinets) and in density. Figure 2.3 is a view of a data center that is owned by Google.</p>
<p><img src="/images/14536652172800.jpg" alt="Figure 2.3: A view of one of Google&#39;s data centers (Source)."></p>
<p>Greater density has become possible because of advances in CPUs, integrated circuits (ICs), and printed circuit board (PCB) design. This leads to faster and more powerful computers within the same area.</p>
<p>The minimum size of an element on an integrated circuit (called the feature size) has become smaller by orders of magnitude over the last four decades. Individual transistors have reduced in size from about 10 microns in 1971 to about 0.022 microns in 2014. As individual transistors get smaller, more can fit on the same silicon, so each transistor consumes less power.</p>
<p>Within the same thermal constraints, CPUs have gone from single core to 16core, with transistor counts going from millions to billions. Additionally, accelerators/coprocessors have emerged that provide hundreds of additional floating point units (FPUs) each, with increasing floating point operations per second (FLOPs), per Watt. Also, recent storage arrays have gone from supporting three or four mechanical hard drives per rack unit to 15 to 21 drives per rack unit.</p>
<p>Hence, although the power efficiency of individual components has improved over the years, computers themselves have become more dense, packing in more processing cores, memory, and storage per square foot. This density caused the overall power consumption per rack and per square foot to rise dramatically over the last few years (Figure 2.4). This trend means that both power and cooling requirements also increase per data center.</p>
<p>A large data center now consumes several megawatts of power, which is roughly the same power requirements of a small town.</p>
<p><img src="/images/14536652393106.jpg" alt="Figure 2.4: Trends in power density."></p>
<p><strong>Data Center Efficiency</strong></p>
<p>Information and communications technology (ICT) now accounts for approximately 2% of the global carbon footprint. [1] Within that amount, data centers currently account for approximately 15% (or 0.3% of total global emissions). With the proliferation of ICT worldwide, as more people gain access, the energy footprint of ICT is set to grow considerably for the foreseeable future.</p>
<p>Over the past decade or so, there has been an increased focus on “green” IT, or power-efficient computing. Later in this module, we discuss various methods available in the industry to reduce power consumption and carbon footprint.</p>
<p>Efforts in improving power efficiency in IT exist across many parts of the data center:</p>
<ul>
<li>Servers: Entire servers attempt to reduce power consumption by going into idle states, in which they temporarily power down or reduce the power consumption of components when the system is underutilized. For example, a typical server that typically consumes 650W when busy can scale down to about 200W when idle. In addition, virtualization enables better management of IT resources and allows organizations to consolidate individual servers onto fewer physical servers.</li>
<li>Server components: Within individual servers, CPUs and other integrated circuits have gained in performance while maintaining or reducing power consumption. Efforts have also been made to reduce idle power consumption (time periods of low CPU utilization). In addition, dynamic clocking techniques for multicore CPUs enable these CPUs to lower the clock rate of individual cores based on usage and can significantly reduce idle power consumption.</li>
<li>Power: Systems that distribute and manage electrical power, as well as those providing backup supplies, have recently become targets in the drive for efficiency. Where individual data center racks were previously fed from 110 to 220 volts alternating current (VAC), high voltage (277 to 480VAC) is now becoming more popular because that configuration requires fewer step-down transformers.<ul>
<li>The efficiency of a power supply is the ratio of its output power divided by its input power. For instance, a fully loaded, 800W power supply at 80% efficiency would consume 1000W of power, with the remaining 200W lost as heat. These power supplies have gained efficiency recently, with some of them reaching 95%. An alternate design feeds DC to servers directly, instead of converting AC to DC on each rack. Delivering high-voltage, direct current (HVDC) to each server allows using more efficient, DC/DC supplies within the rack.</li>
<li>Large, centralized, uninterruptable power supply (UPS) systems also incur AC/DC conversion losses. To minimize these losses, Google has adopted a decentralized UPS plan, and for several years, its custom-built servers have each had dedicated battery backup. Some manufacturers now offer this server-based UPS configuration for data centers.</li>
</ul>
</li>
<li>Cooling: Significant advances have also been made in the area of cooling. Eventually, all of the electricity used to power the IT equipment turns into heat (and some noise). This heat has to be dissipated away from the equipment. Traditional server room cooling uses the raised floor plenum with computer room air conditioners (CRACs), but this has limited cooling density. Newer approaches, which improve efficiency and capacity, include hot-aisle containment, in-row cooling, liquid cooling to the rack, and even completely submerging the equipment in mineral oil. Evaporative cooling techniques, as part of the facility, are more energy efficient than chillers and compressors. In colder climates, many server rooms are designed to mix colder outside air as well as reclaim the heat generated by servers for use elsewhere in the building.<ul>
<li>The leading driver of increasing power efficiency in the data center is to decrease operating cost. Any power that is drawn from the electric company is billed, but only what is getting to the IT equipment is considered useful, while the rest of the losses eventually become heat. Similarly, the more efficient your cooling system is, the lower your monthly costs.</li>
</ul>
</li>
</ul>
<p>Techniques to calculate and improve power efficiency in data centers are covered in a later module in this Unit.</p>
<p><img src="/images/14536653383069.jpg" alt="Did You Know? A study by Google quantified the power consumption of each hardware subsystem in their servers."></p>
<p>Did you know? Each time you convert from AC to DC, or vice-versa, energy is lost. Similarly, conversions between voltage levels are never 100% efficient.</p>
<p><strong>References</strong></p>
<ol>
<li>GeSI (2008). “SMART 2020: Enabling the low-carbon economy in the information age.” Global e-Sustainability Initiative Report.</li>
</ol>
<h3 id="Challenges_in_Cloud_Data_Centers"><a href="#Challenges_in_Cloud_Data_Centers" class="headerlink" title="Challenges in Cloud Data Centers"></a>Challenges in Cloud Data Centers</h3><p><strong>Challenges and Requirements for Cloud Data Center Design</strong></p>
<p>With the advent of cloud computing, it becomes critical for data center designers to address the cloud’s current and evolving needs. In this paradigm, data centers physically host all the cloud services that are delivered to users. In turn, the cloud services are abstracted from the underlying physical resources on varying scales (over a private IP network [i.e., a private cloud] or over the Internet [i.e., a public cloud]), on demand, and for potentially millions of subscribers. Data center design requirements vary according to use, size, and desired functionalities. The cloud model redefines the way data center assets are designed and consumed. Cloud-based services and scale impose new requirements on data centers whereby traffic flows vary, I/O bandwidth and performance demands are significantly increased, and new security concerns are induced. We describe some of the challenges that cloud computing puts on data centers and identify associated requirements for designing cloud-centric data centers.</p>
<p><strong>Scalability</strong></p>
<p>With cloud computing, there is an ever-growing need for expansion and high capacity. For that sake, cloud data centers are typically designed around virtual machines (or instances), which are the units of computing in the cloud paradigm. In contrast to enterprise data centers, cloud data centers offer services to potentially millions of users. To address increasing user demands for services on the cloud, virtualization techniques are usually adopted. With virtualization, data center operators can automatically provision and deprovision virtual machines (VMs) as required, without adding or reconfiguring physical devices. Today. it is not uncommon to provision 20 or more VMs per rack-mount or blade server. Clearly, this load can greatly stress server’s resources (e.g., CPU, RAM, and network cards). In addition, this can dramatically increase the number of logical servers that operate over the physical data center network. For instance, with a rack of 64 servers and 20 VMs per a server, a cloud provider would require as many as 1200 IP subnets and VLANs (in networking, a LAN can be segmented into different broadcast domains, each referred to as a VLAN). Furthermore, with only 10 racks, 12,000 IP subnets and VLANs will be needed. This demand creates a major problem because it exceeds the limit (i.e., 4094) of usable VLANs specified by IEEE 802.1Q standard, let alone straining physical switches/routers. Cloud data center providers need to find solutions for such problems.</p>
<p>On the other hand, even with maximal use of virtualization techniques, at a point in time it will be necessary to add physical capacity to support growth. As such, cloud data centers need to be based on modular designs in order to support the easy addition of physical capacity without disrupting applications and services. Designers should specify chassis capacity to support long-term growth so that data center operators can include additional components to the chassis as necessary.</p>
<p><strong>Network Topologies</strong></p>
<p><img src="/images/14536654393746.jpg" alt="Figure 2.5: Traditional hierarchical, tree-style data center network topology."></p>
<p>Most of today’s data center networks are based on hierarchical, tree-style designs consisting of three main tiers: an access tier, an aggregation tier, and a core tier. Figure 2.5 shows a sample of a traditional tree-style network topology. First, the access tier is made up of cost-effective Ethernet switches connecting rack servers and IP-based storage devices (typically 10/100Mbps or 1GbE connections). Second, multiple access switches are connected via Ethernet (typically 1/10GbE connections) to a single aggregation switch. Third, a set of aggregation switches are connected to a layer of core switches. Because layer 2 VLANs do not involve IP routing, they are typically implemented across access and aggregation tiers. Conversely, layer 3 routing is implemented at core switches that forward traffic between aggregation switches, to an intranet, and to the Internet. A salient point is that the bandwidth between two servers is dependent on their relative locations in the network topology. For instance, nodes that are on the same rack have higher bandwidth between them as opposed to nodes that are off rack.</p>
<p>Indeed, the network is a key component in cloud data centers. Hierarchical topologies as depicted in Figure 2.5 do not truly suit clouds because they enforce inter-server traffic to traverse multiple switch layers, each adding to latency. Latency in this context refers to the delays incurred by the number of switches traversed, required processing and buffering. Minimal delays in clouds can result in poor user performance perception and loss of productivity. Hence, flatter network topologies with fewer layers to accommodate delay- and volume-intensive traffic are typically required for cloud data centers.</p>
<p><strong>Greater Utilization and Resiliency</strong></p>
<p>Usually, contemporary tree-style data center networks rely on some variant of the Spanning Tree Protocol (STP) for resiliency. STP is a data link management protocol that ensures a loop-free topology when switches/bridges are interconnected via multiple paths. STP allows only one active path across two switches, with the rest being set inactive (assuming many paths are available). On an active path failure, STP automatically selects another available inactive path instead of the failed one. This selection might take STP several seconds, which could turn unsuitable for delay-sensitive cloud applications (e.g., Web conferencing). Furthermore, setting idle backup paths is not the best choice for cloud data centers, especially with the exponential increase in user demands. Cloud data centers require more streamlined and resilient network designs that make full use of network resources and recover from failures in milliseconds to meet demands speedily and utilize resources efficiently.</p>
<p><strong>Secure Multitenant Environment</strong></p>
<p><img src="/images/14536654782240.jpg" alt="Figure 2.6: Workload-to-workload communications in a virtualized environment."></p>
<p>In modern data centers, workloads (e.g., databases, user applications, Web hosting) are typically deployed on distinct physical servers, with workload-to-workload communications occurring over physical connections. Accordingly, securing users can be achieved by conventional network-based intrusion detection/prevention systems. On the other hand, in cloud data centers, multiple VMs can be provisioned on a single rack server, with each belonging to a different user. Thus, workload-to-workload communications can occur within the same server over virtual connections in a manner completely transparent to existing security systems (see Fig. 2.6). Therefore, cloud data centers need to isolate users, protect virtual resources, and secure intra-server communications.</p>
<p><strong>Virtual Machine Mobility</strong></p>
<p>Cloud data centers can host user VMs at servers in one rack, across racks but in the same data center, or at servers across data centers. A cloud can encompass multiple data centers (e.g., Amazon allows users to provision virtualized instances across many data centers). For executing routine maintenance, balancing loads, and tolerating faults, clouds need to periodically and seamlessly migrate VMs between physical servers without impacting user services and applications. This migration does not only require expanding the layer three domain (the domain in which IP routing is required [e.g., WAN]) to move VMs across data centers, but it also requires extending the layer two domain (the domain in which no routing occurs and only broadcasting is employed [e.g., LAN]) in order to span multiple data centers.</p>
<p><strong>Fast and Highly Available Storage Transport</strong></p>
<p>Storage in a cloud data center must support VM mobility and be continually available. VMs that are migrated need to maintain communication with their storage systems. One way to get around this is to move VMs with pertaining storage/data. Clearly, this would require highly available, low-latency, and bandwidth-intensive cloud data center connectivity. Multiple storage models (e.g., Storage over IP [SoIP], Fiber Channel over Ethernet (FCoE), traditional Fiber Channel) are in use today and would further require high-performance and highly available cloud networks. We discuss cloud storage, its challenges, and protocols in the unit on cloud storage.</p>
<p>In summary, data centers tailored for clouds would require the following:</p>
<ul>
<li>Modular designs to support exponential growth and the easy addition of physical capacity without any service disruption.</li>
<li>Flatter network topologies with fewer layers and less equipment and cabling to accommodate delay- and volume-intensive traffic.</li>
<li>More efficient and resilient network designs that make full use of network resources and recover from failures in milliseconds.</li>
<li>Capability to fully isolate cloud users, protect virtual resources, and secure intra-server communications.</li>
<li>VM mobility to execute routine maintenance, achieve load balancing, and tolerate faults seamlessly and speedily.</li>
<li>Twenty-four seven, 365 days a year service availability and ability to keep migratory VMs connected to their storage systems.</li>
</ul>
<p><strong>Addressing Requirements for Cloud Data Centers</strong></p>
<p>Data center designers can address the previously discussed requirements at the infrastructure layer, the virtualization layer, or both. For instance, the scalability requirement needs to be addressed at both layers, whereas secure multitenancy can be mainly addressed at the virtualization layer. We study virtualization in detail in the unit “Resource Sharing and Virtualization.” In this unit, we are concerned with the infrastructure layer. Accordingly, we present only some of the IT devices (e.g., switches, routers), platforms, and protocols that can contribute to satisfying the requirements for cloud data centers. Because this is a cloud computing course, we do not discuss how the devices, protocols, and platforms work but focus on the benefits they bring to cloud data centers.</p>
<p>To start, data center planners can consider Multiprotocol Label Switching (MPLS) to address cloud infrastructure requirements. MPLS is a highly scalable mechanism that directs data from one server to another based on short path labels rather than long network addresses. Specifically, MPLS labels data packets and enables packet forwarding without examining the contents of packets, which makes packet forwarding quite fast because it avoids routing table lookups and solely depends on packet labels. Consequently, MPLS allows creating end-to-end circuits across any type of transport medium, a key requirement for server-to-server communication across cloud data centers.</p>
<p>As previously discussed, in cloud computing, different types of traffic are imposed, and variant bandwidth requirements are induced. Without considering MPLS, separate layer two networks might be necessary to build, which clearly is not a scalable solution in cloud data centers and can greatly increase both capital and operational expenses. By contrast, with MPLS, networks can be shared via creating virtual network connections called label switched paths (LSPs). Furthermore, quality of traffic flows over the LSPs can be flexibly controlled. Such traffic control facilitates end-to-end quality of service (QoS) and provides fast network convergence (approximately 50ms) in case of link failures, a remarkable improvement over STP. As a result, the transparency of network failures can be highly improved, and service disruptions can be reduced, which are other key requirements for greater resiliency on cloud data centers.</p>
<p>MPLS also allows enabling virtual private LAN service (VPLS) to extend layer two connectivity across multiple data centers. VPLS is a virtual private network (VPN) technology. VPN is typically utilized to implement secure connections between LANs located at different sites (i.e., data centers) using public Internet links. VPLS allows different LANs at different data centers to communicate as if they are one LAN, a key requirement for streamlining VM mobility. VM mobility can also benefit from the aforementioned traffic controlling capability offered by MPLS.</p>
<p><strong>Summary</strong></p>
<p>Several vendors are now marketing cloud-centric networking and compute products to address several of the design goals stated earlier. Most of these products are variations on what you would find in a traditional data center, with a shifted focus on density and concurrent users. A data center designer who wants to support private or public clouds has a growing catalog of products from which to choose.</p>
<p>Data center design has evolved rapidly over the last 5 years; this trend is not slowing down. The data centers 5 or 10 years from now will most likely look very different than the data centers of today. Data center designers are addressing new requirements while improving efficiency and the TCO to deliver a cloud service. With the advent of pre-engineered modular data centers, soon the facilities will become interchangeable components, much like the IT equipment itself. To meet the growing consumer demand for new cloud-based services, as well as address IT infrastructure needs of existing enterprises, we will see more and more data centers, with each generation more efficient than its predecessor.</p>
<h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>A data center is a term that refers to infrastructure dedicated to housing computer and networking equipment, including power, cooling, and networking.</li>
<li>Data centers have evolved from simple rooms housing mainframe computers to large warehouses that store thousands of individual servers.</li>
<li>As an organization’s IT needs continually increase, modularity in data center components allows an organization to expand its infrastructure as needed.</li>
<li>Servers are getting more and more dense, packing more CPU, RAM, and storage into each square foot. This led to greater power consumption per rack. It is common for existing data centers to run out of power or cooling before running out of floor space.</li>
<li>Industry aims to increase its focus on “green” IT, or power-efficient computing, both inside and outside the server. This efficiency reduces power consumption and carbon footprint, decreasing operating expenses and increasing profit.</li>
<li>Data centers are becoming more efficient in the last few years through advances in virtualization, power-aware server hardware, efficient power distribution, and advanced cooling techniques, among others.</li>
</ul>
<hr>
<h2 id="Data_Center_Components"><a href="#Data_Center_Components" class="headerlink" title="Data Center Components"></a>Data Center Components</h2><p>In this module, we will discuss the various components (equipment and facilities) of a data center. We will start with a discussion on IT equipment, and will clarify many server-specific terminologies that you may have encountered. Most hardware vendors provide servers that are configurable in terms of compute, storage and networking. We will present in detail these server components. While these topics can be information-dense, the choices made while configuring servers have a major impact on the cost and performance of applications that are run on them. Therefore, an exposure to the spectrum of servers and related technologies will be useful, especially if you pursue a career that utilizes or manages cloud infrastructure.</p>
<p>Also, in this module, we will discuss data center facilities. Specifically, we will present some of the recent advances in power and cooling techniques which provide additional efficiency to today’s data centers. We also cover a few additional facilities-specific aspects regarding fire and safety in data centers.</p>
<h3 id="IT_Equipment"><a href="#IT_Equipment" class="headerlink" title="IT Equipment"></a>IT Equipment</h3><p>In this section, we define IT equipment as anything that is mounted in a rack (called rack-mounted equipment). This equipment typically includes servers, dedicated storage arrays, network switches and routers, power distribution, and remote management devices. We are specifically referring to a four-post rack inside a cabinet enclosure. Additional types of racks are described on the next page of this module (“Facilities”).</p>
<p><strong>Servers</strong></p>
<p>A rack-mounted server is similar to a tower PC, except turned horizontally and made to fit into a thinner, deeper chassis (Figure 2.7). The heights are measured in multiples of rack units, where 1U = 1.75 inches (4.45 cm). A 1U server can be CPU and RAM dense but leaves little room for I/O expansion cards (usually two). A server that is 2U or 3U can have six to eight I/O card slots. Smaller chassis must also have smaller fans and therefore make considerable noise compared to your average desktop computer (this is acceptable because most server rooms are not occupied by humans). Systems that are 4U, 5U, or larger chassis usually have a specialized function: one example is an 80-core Intel server, which has CPU sockets and RAM on vertically mounted daughter cards; another is a quad-GPU accelerator server; a third is a server chassis with a 24, 36, or 48 internal hard drives.</p>
<p><img src="/images/14536663372743.jpg" alt="Figure 2.7: (a) Servers and other equipment mounted in a standard 19-inch rack . (b) A 1U server with its top cover removed to reveal internal components "></p>
<p>Rack-mounted servers have their own fans, power supply units (PSUs), network, and I/O, but blade servers share all of these across many nodes within the same blade enclosure (called blade chassis). There is no common blade standard, so each vendor’s blades work only with its enclosures. Blades are thin, vertical metal enclosures and slide into the front of a blade chassis and attach to a common backplane. Each blade has its own motherboard, CPU, RAM, and disk. The shared PSUs are typically more efficient than dedicated rack-mounted versions because they can power up or down incrementally, adjusting PSU capacity to load demand. For example, instead of 10 servers with 2 <em> 750W redundant PSU, a blade enclosure can power the equivalent of 10 servers with 4 </em> 2500W PSU, with one being redundant. Blades are denser than their horizontally mounted counterparts, allow for easier maintenance, and require fewer cables. The disadvantage is higher upfront cost if you only need a few servers, plus you are locked-in to a specific vendor.</p>
<p>Last, there are a few servers that look similar to a standard rack-mounted server, except they have two motherboards horizontally per 1U, each mounted on its own tray. They also share a PSU but, unlike blades, have their own fans and I/O. There are variations on this theme, such as four nodes per 2U or two nodes with multiple GPUs.</p>
<p>Did you know? A 1U rack-mounted server is sometimes referred to as a “pizza box.”</p>
<p>The following video (Video 2.2) describes various server form factors:</p>
<p><a href="http://youtu.be/0EM6jPYafys" target="_blank" rel="external">Video 2.2: Server Form Factors.</a></p>
<p>An important feature found in most rack-mounted servers is hot-swap capability. Components such as PSU, fans, and hard drives can be removed and replaced while the server stays running. This feature increases uptime/reliability on small- and medium-scale deployments. Large-scale application deployments require more sophisticated resiliency to be built into software layers, which is discussed in the next unit. These large-scale systems do not use hot-swap or redundant components for individual servers but instead consider the entire server to be failed (and replaced) as a unit.</p>
<p>The electrical components (e.g., capacitors, voltage regulators) of a server are typically more expensive and longer lasting than those parts used on desktop systems because the servers are designed to run 24/7 for many years on end. A workstation is similar to a server class computer, with similar CPU, high RAM capacity, and added reliability. The difference is that a workstation sits at the user’s desk, so it requires quieter fans. To add to the confusion, there are also rack-mount workstations, which are just like a server, but have remote viewing capability, so the end user sits at a thin client.</p>
<p>Newer servers are now being designed to run reliably at higher ambient room temperatures (up to 95°F, or 35°C), which decreases cooling requirements and therefore lowers operational expense.</p>
<p><strong>On the Motherboard/Mainboard</strong></p>
<p>CPU and memory: A typical server motherboard has more CPU sockets than a desktop system, and each of those sockets can control more DIMMs (dual inline memory modules). Another primary difference of server-class CPUs versus desktop class (Intel Xeon versus i3/i5/i7; AMD Opteron versus FX/A/Phenom) is more onboard cache, support for registered DIMMs, and support for error-correcting code (ECC) RAM.</p>
<p>Server-class CPUs also have dedicated circuitry that allows them to communicate with each other through dedicated channels in the motherboard. For Intel, this is known as QuickPath Interconnect (QPI), and for AMD, it is known as HyperTransport (HT). These follow a nonuniform memory access (NUMA) model, in which processes running on other CPUs (sockets) can access large amounts of RAM by going through QPI or HT to the RAM attached to another CPU. Combining high-density, registered DIMMs (16GB or 32GB), more DIMM slots per server (9 or 16), and multiple sockets with onboard interconnect, a single server can have 512GB or even 1TB of RAM available to a single process (although you get higher performance when you have multiple processes with multiple threads each, but that is a topic for another course).</p>
<p>Onboard management: Although many desktop motherboards now have onboard gigabit Ethernet (GigE) networking, this trend started with servers, and on modern servers, you will find two to four GigE ports. Other onboard devices that a server includes are a serial port for console redirection and an embedded management controller, which allows remote management even if the system is powered off (but still plugged in) or if the OS is not responding (i.e., kernel panic). Many server motherboards have onboard hard drive controllers, but these are also common in the form of an expansion card, which is discussed below.</p>
<p><strong>Expansion cards</strong></p>
<p>PCI Express: Often, a server requires additional I/O devices, depending on the applications intended to run on it. PCs and mainframes have always had some notion of expandability, and expansion buses have evolved from ISA to PCI to PCI-X to what is the current standard—PCI Express (PCIe). The biggest difference between PCI and PCIe is that PCIe is based on point-to-point, high-speed serial links, rather than an actual bus, which has multiple devices attached. Each of these high-speed links constitutes a lane, and multiple lanes work in parallel. So a PCIe device that is x8 has eight of these high speed lanes. Each generation of PCIe, from 1.0 to 3.0, effectively doubles the bandwidth of the previous generation.</p>
<p><img src="/images/14536664462775.jpg" alt="Table 2.1: PCIe generations throughput per x1, x4, x8, and x16 lane slots."></p>
<p>RAID: RAID (redundant array of inexpensive disks) adapters allow multiple hard drives to act as a single logical unit, with increased performance and redundancy or a mixture of both.</p>
<table>
<thead>
<tr>
<th>RAID Level</th>
<th>Name</th>
<th>Advantages</th>
<th>Disadvantage</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Called block-level striping</td>
<td>Improved performance</td>
<td>No fault tolerance, and a single drive failure destroys the entire array</td>
</tr>
<tr>
<td>1</td>
<td>Called mirroring because data is replicated entirely.</td>
<td>Can withstand drive failure. It has faster reads (sometimes), and this level maintains performance on failure.</td>
<td>One-half the capacity versus two independent drives. This level has slower writes.</td>
</tr>
<tr>
<td>10</td>
<td>Called RAID 1+0 or ten (mirroring+striping). It combines the speed of RAID 0 and redundancy of RAID 1.</td>
<td>Provides fault tolerance. It has improved performance. This level maintains performance on failure</td>
<td>One-half the capacity versus two independent RAID 0 arrays.</td>
</tr>
<tr>
<td>5</td>
<td>Block-level striping with distributed parity.</td>
<td>This level can withstands single drive failure, and it has more capacity versus RAID 1, 10, and 6.</td>
<td>Slower writes than RAID 0, 1, and 10, degraded performance on failure, and slow rebuild process.</td>
</tr>
<tr>
<td>6</td>
<td>Block-level striping with double-distributed parity.</td>
<td>Withstands two drive failures and has more capacity versus RAID 1 and 10 (with more than four drives).</td>
<td>Slower writes than RAID 5, degraded performance on failure, and slow rebuild process.</td>
</tr>
</tbody>
</table>
<p>Table 2.2: Different RAID levels with major advantages and disadvantages.</p>
<p>RAID controllers come with different number of ports (connected drives), support different interfaces (SATA, SAS), vary in amount of built-in cache (e.g., 512MB, 1GB), and have varying performance levels (MB/s throughput, I/O per second). Each of those factors can change the price of the adapter (from a few hundred to over $1000).</p>
<p>A host bus adapter (HBA) is similar to a RAID controller, except that it connects to external storage (discussed in the next section) and does not typically include any RAID features on the card itself. The card will encapsulate all communication, so the remote disk device will look like a local disk to the OS and is bootable.</p>
<p>Networking support: Ethernet network adapters can include single or multiport cards, with speeds ranging from 100Mbps (obsolete) to 1Gbps (common, inexpensive), then came 10Gbps (becoming more prevalent but still expensive), and now 40Gbps (available but not common yet).</p>
<p>A host channel adapter (HCA) refers to a high-speed, low-latency interconnect adapter, such as Infiniband. These are typically used on high-performance computing (HPC) clusters.</p>
<p>Solid-state storage cards are now available, such as those from Fusion-IO. They provide much faster performance than RAID controllers, especially on random I/O. They have a very high cost per gigabyte compared to hard disk drives (HDDs) or even solid-state drives (SSDs) (see the following “Storage” section).</p>
<p>Accelerators: Video cards, started in desktop computers, use GPUs for fast 3D rendering. These GPUs can also be used for scientific computation, so they made their way into HPC server markets, and now nVidia Tesla series accelerators can be found in more than 50 of the top 500 supercomputers list. Recently, Intel released an accelerator product, the Xeon Phi, which is based on Many Integrated Cores (MIC) architecture and is similar in performance to Tesla product (&gt;1 teraFLOPs). These cost approximately $3000 but can increase performance 2.5x to 10x (often) to 100x (atypical) for math-intensive applications. Note, however, that these will not fit into your average server case due to the large size of the card and additional PSU requirements.</p>
<p><img src="/images/14536667410289.jpg" alt=""></p>
<p>Server Configuration</p>
<p>Company X wants to expand its private cloud for hosting a new Web application composed of many Linux virtual machines running in a VMware ESXi 5 environment. They do not want to use more than one rack of equipment, so they like the density that blade servers provide. They have benchmarked their application and found that a blade server with 2 x Intel Xeon E5-2660 performs 40% faster (i.e., 1.4x) than a blade server that has 2 x AMD Opteron 6276 CPUs. They use a fast QDR Infiniband network and fast enterprise-class SSD drives in each blade server, so the only bottleneck for the applications is the CPU.</p>
<p>Each blade enclosure/chassis is 7U tall and can house 20 blade servers (each blade is a two-socket blade). The enclosure itself costs $12,200 and includes power supplies, fans, gigabit Ethernet, and Infiniband network switch modules. These particular blades must be bought in pairs because of the way they stack in the enclosure.</p>
<p>The price of the Opteron 6276 processor is <code>$850</code> (per socket). The price of the Xeon E5-2660 processor is <code>$1370</code> (per socket).</p>
<p><img src="/images/14536669237218.jpg" alt=""></p>
<p>Configured to Company X’s requirements, the AMD blade servers cost $4875 each, and the Intel blade servers cost $6700 each. The company intends to configure the enclosures with the exact number of blade servers as determined in the scalability testing mentioned above.</p>
<p><img src="/images/14536670010535.jpg" alt=""></p>
<p><strong>Storage</strong></p>
<p>When most people hear the word storage, it is likely that they will envision an HDD (Figure 2.8(a)). Inside an HDD, the rotating platters and moving read/write head have been similar for decades. What has evolved are higher areal density, new recording techniques, faster interfaces, and overall lower power. The two most common form factors of HDDs are 3.5 inches and 2.5 inches. Rotational speeds can vary, but common RPM values are 5400, 5900, 7200, 10,000, and 15,000. The higher the RPM, the lower the latency between random seeks.</p>
<p><img src="/images/14536670221645.jpg" alt="Figure 2.8: (a) Internal view of a traditional hard disk showing the rotating platters and read/write head. (b) SSD with the circuitry exposed."></p>
<p>Did you know? The maximum physical capacity of a hard drive is governed by what is called the superparamagnetic limit.</p>
<p>SSDs (Figure 2.8(b)) are becoming more affordable but still have a much higher cost per gigabyte than their mechanical counterparts. SSDs are completely electronic; there are no moving parts, which gives lower seek times, higher performance, and higher reliability. SSDs do have a limited number of writes, but one would have to continuously write to the drive 24 hours a day for several years to reach that limit. The terms SLC, MLC, and TLC refer to how many voltage levels there are in each cell: S is for single (1), M is for multi (2), and T is for triple (3). The fewer the levels, the more reliable it is (SLC &gt; MLC &gt; TLC). The more levels, the higher the density (TLC &gt; MLC &gt; SLC). SLC is generally reserved for high-throughput transactional servers. MLC is the most common replacement for desktop and laptop drives, plus servers that need higher I/O performance than a fast (10k or 15k rpm) HDD can provide. TLC is emerging as a lower cost alternative to MLC, but somewhat compromises performance and reliability to achieve a lower price.</p>
<p>The hard drives that are found in data centers are nearly identical to those found in desktop PCs. The primary difference between an enterprise and a regular SATA drive is added antivibration protection, which claims to increase the lifespan of drives that are rack mounted with many other drives in the same chassis. They also come with longer warranties—5 years instead of 3.</p>
<p>SATA (serial ATA) and SAS (serial attached SCSI) are the two predominant interfaces for all modern HDDs and most SSDs. For servers, SAS provides the ability for a single drive to “talk” to two drive controllers (in case one controller fails). SAS also requires lower CPU usage and provides the ability to daisy chain a large number of drives via an SAS expander. Most RAID controllers that support SAS also support SATA, but the inverse is not true.</p>
<p>Direct attached storage (DAS) refers to a rack mount chassis that holds additional drives, along with PSUs and fans, but minimal control logic. All of the work is done by the controller (on the server) that these units attach to. A typical connection is made through external SAS cable (high-speed serial, multilane, copper).</p>
<p>A storage area network (SAN) is a collection of hard drives in a chassis that can be connected to many servers. A system admisinstrator will carve the array into LUNs (storage logical unit numbers), where each LUN can be used for a different purpose. Many servers connect to the same SAN simultaneously, but only one server accesses a given LUN at a time (unless a coordinated sharing software is installed, such as VMware). Servers connect to the SAN through a network, typically Fibre-Channel or iSCSI over Ethernet (1Gb or 10Gb). Each LUN gets a unique World Wide Name (WWN), which can be used for zoning and masking, providing security so that servers can only read the LUNs that belong to them. Access can also be restricted by IP and subnet.</p>
<p>Most commercial SAN products span multiple rack units or even multiple racks, and have multiple redundant controllers and network paths to increase uptime. SANs have embedded RAID controllers, so disk failures are transparent to the attached servers. SAN products vary greatly and, depending on features, performance, and capacity, can range from $5000 to several million US dollars.</p>
<p>Network attached storage (NAS) is a device or server that shares a file system to multiple users over the network. Any Linux server can become a NAS, and many consumer dedicated NAS devices run Linux underneath. The two most common protocols used by a NAS are called CIFS (Common Internet File System, also called SMB [Server Message Block]) and NFS (Network File System). CIFS comes from Microsoft Windows’ heritage and is a per-user connection in which NFS is primarily used on UNIX/Linux and is a per-host connection.</p>
<p>Last, there are also many Distributed Parallel File Systems, which are typically used in compute clusters and implement a many-to-many scheme. These provide a much higher combined throughput than any single SAN or NAS and can scale in both performance and capacity.</p>
<p>The choice on which type of storage to use is largely dependent on the application, and most organizations will use a combination of the above. For example, a large storage array uses a parallel distributed file system on its internal back-end network but presents both SAN-style LUNs and provides NAS capability for other clients. A Windows file server will look like a NAS to a client mapping a network drive, but it is likely that the storage device in which files actually reside are in a SAN, which is outside of the Windows server itself. When you learn about resource sharing in the next unit, some of those features (live migration, fault tolerance) require a SAN storage device.</p>
<p><img src="/images/14536670862076.jpg" alt=""></p>
<p><img src="/images/14536671077200.jpg" alt=""></p>
<p><img src="/images/14536671199042.jpg" alt=""></p>
<p><strong>Networking</strong></p>
<p>In addition to servers, a data center houses all of the network equipment that is used to connect those servers to each other and to the outside world.</p>
<p>One popular design is the multitier topology (Figure 2.9), in which servers connect directly to switches, and those switches link to aggregation switches, which, in turn, connect to the rest of the organization’s network (i.e., core switches) and eventually to the upstream ISP.</p>
<p><img src="/images/14536671423623.jpg" alt="Figure 2.9: A stack of network switches mounted in a rack"></p>
<p>A fatter (higher bandwidth) connection typically links the bottom-tier and aggregation switches. For example, with 48x1Gb ports connecting the rack servers and the access tier, the access to the aggregation tier might have 2 * 10Gb uplinks.</p>
<p>Top of rack (TOR) refers to a configuration in which the lowest tier of network switches are found in each rack. This design can decrease cabling cost and complexity because the wires are shorter and can be copper even at 10Gb speeds. This is especially useful if most of the network traffic is to other servers in the same rack. Apache Hadoop, for example, can take advantage of this topology for choosing which nodes to store data (on rack and off rack) and which nodes from which to retrieve data (on rack). One disadvantage includes limited scalability. Ethernet switches are blocking, which means that if all the ports are busy at the same time, the backplane cannot handle all the traffic, so some requests are postponed. Also, the uplinks that are available are typically less than the full possible bandwidth of the Ethernet switch (e.g., you cannot have a 48Gbps uplink on a 48x1Gb switch). In most real-world scenarios, however, those uplinks are never saturated because not every server is using the full capacity simultaneously. Another disadvantage is port utilization. Switches come with a specific number of ports, typically 24 or 48, so if you have a number of servers in between that, you will have some ports in each rack that are never used.</p>
<p>End of row (EOR) refers to a configuration in which you have a row of adjacent racks, or a row of back-to-back racks,sometimes called a bay, and there is one additional rack dedicated for network switches, and all of the hosts in that row (or bay) are wired to that dedicated rack. Wiring is either done above the racks in a wiring tray or below the racks, underneath a raised floor (more on that in the next section). The switches in this dedicated rack can be a larger (10U+) modular chassis, in which each module communicates to other modules through a fast backplane. The advantage is that all servers in the same row (or bay) can communicate with each other at the same speed (and lower latency), so there are fewer bottlenecks. The disadvantage is higher upfront cost and more complex cabling (harder to debug if there is a problem). Gigabit links can still be copper at this distance, but anything 10Gbps or above should be fiber.</p>
<p>Ethernet is by far the most popular type of network in data centers (and elsewhere in organizations). Although its rise in popularity started in the 1980s with 10Mbps, both that and 100Mbps products are obsolete because of the low price and ubiquity of 1Gbps products. The primary driver for Ethernet popularity is the low cost and ease of installation of twisted pair wiring. The most common type of Ethernet wiring is unshielded twisted pair (UTP) (Figure 2.10(a)), which has four pairs of thin (28awg) copper wires and comes in different categories: category 5 (Cat5) is good for 10/100Mbps, Cat5e and Cat6 are good for 1Gbps, and Cat7 can go up to 10Gbps. Each generation has slight design changes over the previous, which allow for greater bandwidth and/or longer runs. Alternatively, shielded twisted pair (STP) has an extra metal shielding on the outside for use in electrically noisy environments (such as a factory or near high power lines). The shielding reduces interference but is generally not needed for most in-rack installations.</p>
<p>Ethernet can also travel over fiber (sometimes fibre) optic cable (Figure 2.10(b)). Fiber is unidirectional, so in a cable, you will see two strands, one send and one receive. The electrical signals are converted to photons, which stay inside the fiber due to the differences in indexes of refraction between the core (thin center) and cladding (thicker outside layer). The cladding is covered by a layer of fireproofing material. Fiber cables are thinner and lighter than copper cables, but care has to be taken in routing to maintain a minimum bend radius, otherwise some photons escape causing loss. Multimode fiber is most commonly used inside data centers because the cables and transceivers are much less expensive than single-mode fiber. Single-mode fiber is generally reserved for long-distance (&gt;100m to 40km) connections. Higher speed Ethernet of 40Gbps is actually 4x10Gbps links that work in unison (similar to PCIe lanes); similarly, 100Gbps is 10x10Gbps (copper, fiber) or 4x25Gbps (fiber only).</p>
<p><img src="/images/14536671655060.jpg" alt="Figure 2.10: (a) A cutout of an unshielded twisted pair (UTP) cable, showing four pairs of cables. (b) Fiber-optic network cables with LC (top) and ST (bottom) connectors."></p>
<p>Fibre Channel (FC) is a protocol designed to support SANs and remote block-level storage devices. Even though it typically runs over fiber, it can also run over copper. It can also be encapsulated in Ethernet in the form of Fibre Channel over Ethernet (FCoE) but is not routable like iSCSI. Fibre Channel HCA and switch generations are named by their speed (in Gbps)—##GFC, in which ## is 1, 2, 4, 8, 10, or 16. The most common now is 8GFC, which supports 8Gbps links (approximately 800MB/s in each direction).</p>
<p>iSCSI, or internet SCSI (pronounced “scuzzy”), is a protocol which packetizes SCSI (Small Computer Systems Interface, an old hard drive standard) commands to be transported over LANs or WANs. Many SANs support iSCSI, and it is popular because you can use your existing Ethernet network instead of deploying a dedicated FC network.</p>
<p>Infiniband (IB) has been popular in HPC clusters for almost a decade and is also gaining traction in other areas in the data center. The main advantage of Infiniband is much lower end-to-end latency compared to Ethernet (1.7 microseconds versus 12.5 microseconds for 10GigE versus 30 to 100 microseconds for GigE). It also scales better with the number of nodes in a cluster. The other key factor in IB performance is that the switches (Figure 2.11) are fully nonblocking, which means that the backplane supports running every single port at full speed, in both directions. Infiniband speeds are summarized by the chart below:</p>
<p><img src="/images/14536671966270.jpg" alt="Table 2.3: Summary of Infiniband speeds."></p>
<p>Infiniband and 40Gbit Ethernet are also being deployed as a converged network, or virtual fabric, which, along with virtualization of the OS, allows many virtual adapters (Ethernet, FC-HBAs) to run over fewer high-speed links. The connections can be configured dynamically through software and allow for fewer cables and easier maintenance and management.</p>
<p><img src="/images/14536672169882.jpg" alt="Figure 2.11: An infiniband switch"></p>
<p>The choice of network technology and topology to deploy in a data center also depends on your applications but will typically include multiple tiers, with the connection between the tiers being high-speed fiber. Copper UTP cabling is prevalent within the rack and will continue to function well for low-speed management networks.</p>
<p><img src="/images/14536673271675.jpg" alt=""></p>
<p><img src="/images/14536673906970.jpg" alt=""></p>
<p><img src="/images/14536674222579.jpg" alt=""></p>
<h3 id="Infrastructure_and_Facilities"><a href="#Infrastructure_and_Facilities" class="headerlink" title="Infrastructure and Facilities"></a>Infrastructure and Facilities</h3><p><strong>Data Center Facilities</strong></p>
<p>A data center’s functional units (servers, storage, networking) all rely on a facility’s infrastructure, which includes physical space, power, cooling, and safety. In assembling each of the latter component systems, designers prioritize redundancy issues. Redundant power sources, for example, minimize the risk of service outages should the building lose main power. Redundant cooling avoids physical damage to IT equipment during an unplanned outage and enables planned outages for HVAC equipment maintenance.</p>
<p><strong>Server Room</strong></p>
<p>A server room can vary in size from a single rack in a closet to several hundred square feet to a warehouse the size of a football field. Some authors use the phrases server room and data center interchangeably. For the purpose of the course, we define a server room as the actual room that houses all of the racks full of IT equipment, and a data center as the server room plus all of the power and HVAC equipment that may be located outside of that room.</p>
<p><img src="/images/14536674954350.jpg" alt="Figure 2.12: A 42U, four-post rack/cabinet, with sides and doors removed."></p>
<p>You were already introduced to the concept of a rack and a rack unit (U). Figure 2.12 shows a rack inside an open cabinet. Sometimes racks are also called cabinets. There are several variations of racks, but the most common are 19 inches wide (measured from the center of each hole on the same U). Some IBM equipment racks measure 24 inches post to post, inherited from older mainframes. Usually, network equipment is designed to mount only on two posts of the rack because wiring closets often have only two posts permanently mounted to the floor and/or wall. Servers, however, are designed to mount on four-post racks. The depth of the rear two posts is not standardized, and in most racks you can adjust the depth. Different rack-mount equipment has different depths, and each server or storage array will come with two mounting rails that connect to the front and rear posts on the left and right sides. There are two types of holes in vertical posts, square and round. Some mounting rails hook directly into square holes and have tool-less installation (fast). For round-hole racks, equipment is directly screwed in (these are more popular for telecom and A/V racks). If you need to mount round-hole style equipment or rails into a square-hole rack, you use cage nuts and bolts.</p>
<p>The most common height rack is 42U, and that is simply to fit through a normal doorway. The overall height, width, and depth of racks by different manufacturers are not exactly the same; the only guarantee is the post widths. Some racks have extended depth, which is useful for larger servers and/or routing cables and mounting zero-U (vertical mount outside of the 42U space) equipment. “Wide” racks have extra space to the left and right of the posts, which is useful for end-of-row networking racks because of the added space to run many cables. Racks also have casters that allow them to be rolled into place or moved if needed. However, these wheels are not meant to permanently support the full weight of a filled rack, which is why you have to screw down the four stabilization feet at each corner. Regions that are prone to earthquakes also have safety regulations that require racks to be bolted into concrete through metal plates on the front and rear of the rack.</p>
<p>Many server rooms have a raised floor, although it is not a strict requirement. This provides a plenum for cold air to be distributed throughout the room (tiles in front of racks have vent holes). Raised floors also provide space to run electrical or network cable or chilled water pipes for in-row cooling. Last, they provide more flexibility for future layout/configuration changes. Figure 2.13 shows what a raised floor looks like.</p>
<p><img src="/images/14536675592221.jpg" alt="Figure 2.13: An example of a raised floor."></p>
<p>The floor consists of an array of metal support pedestals, which are mounted to the subfloor; metal stringers that are placed horizontally between pedestals; and strong floor tiles that rest atop the stringers at each corner at a pedestal. When a tile is removed, the holes are large enough for a human to fit through, and the tiles are strong enough to withstand the weight of a filled rack. Tiles do have a rollover rating, however, and for safety reasons, any tile that has been rolled over (moving a filled rack on top of it) more times than it is rated for should be replaced. Also for safety reasons, on a stringerless floor, no more than two or three consecutive tiles should ever be lifted from the floor simultaneously. If there is a problem with one of the pedestals, weight can shift laterally, cascading to a catastrophic floor failure.</p>
<p>Above the racks are cable trays that run horizontally between racks. There are options to hang these from the ceiling, or some racks have optional trays that mount on top. For electrical safety, these are required to be properly earth grounded, even if they are carrying only network cable. When there are a many wires to run between two racks, hook-and-loop (Velcro) fasteners are most often used to bundle the wires together.</p>
<p>Most data centers implement strict physical security procedures—for good reason. If someone with bad intentions had physical access to a server, they could, for example, gain administrative privileges, steal data, eavesdrop network connections, and install viruses/Trojans. Common practice includes keycard/pin access and/or biometric scanner, full time security guard, cameras, and break-in detectors. In shared data centers with multiple tenants, one technique is to have a perimeter chain-link fence with a padlock around each customer’s set of racks. Watch Google’s security practices <a href="http://youtu.be/cLory3qLoY8" target="_blank" rel="external">here</a>.</p>
<p><img src="/images/14536676574968.jpg" alt=""></p>
<p><strong>Power</strong></p>
<p>The following video (Video 2.3) discusses various power distribution methods in data centers:</p>
<p><a href="http://youtu.be/Xl4VjEqitSk" target="_blank" rel="external">Video 2.3: Data center power distribution methods.</a></p>
<p>Reliability/uptime is often the number-one design consideration for a data center. Unfortunately, the power feeding the data center is not 100% reliable because of events such as bad weather conditions and downed power lines. In some locations, it is possible to get feeds from multiple electrical utility suppliers, but often this is not available. To keep the IT equipment powered on during a power outage, a generator can be installed. Backup generators come in two varieties, powered either by diesel fuel or natural gas. They could power the data center indefinitely as long as fuel is available, but both fuel sources are significantly more expensive than electricity from the grid. Generators are typically mounted outdoors due to fumes, noise, weight, and vibration. An automatic or universal transfer switch is a device that can choose a working power source (utility 1, utility 2, or generator) and connect it to the main power input to the data center.</p>
<p>Generators have a 15- to 60-second start-up time, so this is where an uninterruptable power supply (UPS) can provide power to the IT equipment until the utility power is restored or the generator is running. UPS have many lead-acid batteries (like in a car) strung in series. For example, a 480-volt UPS would have a string of forty (40) 12-volt batteries. A UPS also act as a line conditioner and will switch to a DC battery source if it detects poor AC conditions, such as surges, sags, overvoltage, undervoltage (brown out), and variations in wave shape or frequency.</p>
<p><img src="/images/14536677081141.jpg" alt="Figure 2.14: Diagrams of the (a)C13 power connector and (b)C19 power connector "></p>
<p>Between the UPS and the IT equipment, there are power distribution units (PDUs). Think of PDUs as similar to power strips you use at home but designed for higher voltages and amps, with more outlets and built-in circuit breakers. They often include monitoring features, so you can remotely see the power draw per branch (group of outlets connected to a single breaker). Some also include per-outlet power sensing (POPS) as well as remote ON/OFF switching for each outlet. The outlets for PDUs do not look like the electrical outlets in your house; instead, they are IEC 60320 C13 (said “C thirteen”) (Figure 2.14(a)) for 10- to 12-amp rating and C19 for 16 to 20 amps (Figure 2.14(b)).</p>
<p>For AC, higher voltage (400V and 480V) is more efficient for distributing power throughout a data center than 240V or 208V but still has to be stepped down before going to the actual server. Most server power supplies are universal and will accept input AC voltages ranging from 110V to 240V. The benefits to running at 208 to 240V versus running at 110 to 125V are slightly higher efficiency (5% to 10%) as well as getting the full rated power output (as labeled on the PSU). Most server room/data center installations will run at 200+ VAC for the efficiency, as well as lower pricing for electrical wiring (smaller gauge copper). In order to boost efficiency, some server PSUs also support 277V directly. Instead of traditional wire, some server rooms employ bus bars that mount over head (above the racks) and have circuit breaker whips that can attach at any horizontal location (these are like track lighting, only larger).</p>
<p>Figure 2.14: Diagrams of the (a)C13 power connector and (b)C19 power connector (Source).<br>Some vendors offer DC distribution, in which the AC-to-DC conversion is done per rack, per row, or per bay, rather than converting AC to DC within every server power supply. These systems have been measured to be more efficient than their AC counterparts, but only 2% to 4% for average loads. [1] Because DC power supplies are not a commodity, these are only suited for large-scale deployments with custom components.</p>
<p><img src="/images/14536677510030.jpg" alt=""></p>
<p><img src="/images/14536677649442.jpg" alt=""></p>
<p><img src="/images/14536678084746.jpg" alt=""></p>
<p><strong>Cooling</strong></p>
<p>Many of the advances in data center efficiency over the last 10 years have come from new designs and methods for cooling.</p>
<p><img src="/images/14536678293190.jpg" alt="Figure 2.15: Typical data center cooling technique. Figure shows the use of a CRAC and raised floor."></p>
<p>Commonly found in traditional server rooms are computer room air conditioners (CRAC or CAC). These continuously take in hot air and output cold air into the space under a raised floor or into ducts. The difference between a CRAC and a regular air conditioner is that CRACs also provide humidity control. Keeping a relative humidity around 40% is recommended. If the air is too wet, you get condensation (bad for electronics and anything metal), or if it is too dry, you get a higher risk of ESD, or electrostatic discharge (also harmful to electronics). The units’ fans have to be sized large enough to create positive pressure and airflow for the volume of the room and have sufficient cooling capacity to maintain the desired “cold-aisle” air temperature (more on hot and cold aisles below and in the next module). The CRACs remove heat through the use of a condenser (similar to what is in your refrigerator at home) or through a heat exchanger that uses chilled water supplied by chillers elsewhere on site.</p>
<p>Measurement of energy for electronics is usually in kW, but most HVAC equipment is measured in tons or BTU/h, so here are some conversions:</p>
<ul>
<li>1 kW = 3412 Btu/h</li>
<li>1 ton = 12,000 Btu/h</li>
</ul>
<p>Did you know? A BTU, or British thermal unit, is the amount of energy needed to heat 1 pound of water to 1°F, and a ton is the heat absorbed by melting 1 ton of ice in 24 hours.</p>
<p>Using vapor compression, chillers remove heat from water in a closed-loop, high-pressure system, usually outputting water that is approximately 42°F (5.5°C). Chillers themselves need to dissipate the heat they remove from the water, which can be through air cooling (fans) or water cooling (needs another water source and/or cooling tower). Chillers are sized based on water temperatures (entering and leaving) and flow rate (gallons per minute). The main sources of energy consumption in a chiller are the electric motors in the compressor and pump(s).</p>
<p>To reduce the load on these chillers, evaporative cooling techniques are now frequently deployed for large data center installations. When hot dry air passes over water, some of the water evaporates, absorbing energy and cooling the air. If you want to use evaporative cooling, it is a good idea to locate a new data center near an abundant water source.</p>
<p>The overall system can be made more efficient if it does not have to cool as much air. Air-side economization is a method of using or mixing outside air when it is cooler than the recirculated air. This method is cost effective in cold climates but not as useful in hot and humid regions.</p>
<p>When rack densities increase to 10kW or higher, it is helpful to move the cooling equipment closer to the rack. A product category that allows this is in-row cooling. This way the cold air can flow directly into the front of the IT equipment, and the hot exhaust air from the rear of the rack goes directly into the adjacent air conditioner. This method puts a focus back on cooling the racks, instead of cooling the room. Similar to in-row cooling, there are top-of-rack cooling systems. These systems are bolted to the top of each rack and provide localized cooling on a per-rack basis. The advantage of top of rack is you are not taking up floor space in the server room, while the disadvantage is higher difficulty for installation and maintenance. Smaller capacity, in-row systems use a compressor, whereas higher capacity models use chilled-water or external gas refrigerant. Top-of-rack systems typically use external refrigerant. The advantage of using a gas refrigerant is that there is no chance of water leaking near the IT equipment, but the disadvantage is the added cost of additional equipment in the server room, which removes the heat from the refrigerant loop (using chilled water). Both in-row and top-of-aisle systems offer a modular cooling approach. As long as the facility’s chilled-water plant has enough capacity, you can add coolers only when you add new IT equipment, thus staggering your capital expenditures.</p>
<p>Hot-aisle containment refers to a method of placing your racks into rows/bays such that adjacent rows face away from each other (i.e., cold-hot-cold) and then completely enclosing the hot aisle. This arrangement prevents hot and cold air mixing before it recirculates back through the air conditioner, which greatly improves efficiency. Some server room designs employ the opposite, cold-aisle containment, in which the room itself is hot, but the separate cold air is fed into the contained fronts of racks.</p>
<p>Although it has been mainstream amongst overclocking enthusiasts for years, water-cooling products (or glycol or other liquid) are also becoming available from several vendors. There are two approaches: one is to have specialized rack doors that are in essence huge heatsinks, with cold water fed in and hot water returned; the second is to have cold-in and hot-out water hoses going to every server in the rack, and inside each server are specialized heatsinks for the CPU (and GPU) that the water circulates through. In both cases, the servers still require fans to cool the other components (e.g., RAM, hard drives).</p>
<p>One vendor even offers an extreme liquid cooling technique. The (sealed) rack is turned sideways and filled with mineral oil (nonconductive dielectric), and the servers are fully submersed vertically. The fans are removed, and the hard drives have to be sealed (or use SSD). It is best to use servers with front-facing I/O ports.</p>
<p>Many modern buildings’ HVAC systems are designed to reclaim heat that is produced in the server room and use it elsewhere, such as hot water or heating (in cold climates), thus reducing overall energy costs.</p>
<p><img src="/images/14536678975205.jpg" alt=""></p>
<p><img src="/images/14536679199064.jpg" alt=""></p>
<p><strong>Safety</strong></p>
<p>In addition to the safety notes mentioned earlier, there are some features of a data center that are safety specific.</p>
<p><strong>Fire Suppression</strong></p>
<p>The preferred system for putting out fires in a server room is to use a clean agent. These agents are stored and transported under high pressure so that it is a liquid and takes up less space. When activated, they are a gas that comes out of misting nozzles in the ceiling. The “clean” term means they do not leave residue or require cleanup as do handheld fire extinguishers (dry chemical) or water sprinkler systems.</p>
<p>Halon was the most popular fire suppressant in this category, but it is a CFC (greenhouse gas), and manufacturing of Halons was banned in 1994. The old systems still exist (must use recycled Halon) but cannot be used in new installations.</p>
<p>A popular clean agent of today is DuPont’s FM-200 (CF3CHFCF3), which is nontoxic, and with a properly designed system, the gas will fill the room and extinguish all fires within 10 seconds (hint: do not leave the doors open). It is safe for humans to breath but can create fumes when it reacts with fire. Standard practice is to leave the room (sealed) for 10 minutes to ensure all fire is out.</p>
<p>Another method of fire suppression is the use of inert gases, such as CO2. These gases work by reducing the ratio of oxygen in the air. The problem with these systems is that they are dangerous to humans and also not as effective (depending on type of fire).</p>
<p>Traditional sprinkler systems use a large amount of water to decrease combustibility of everything in the room. They are not as effective for electrical fires, damage electronics, and require extensive cleanup. Sometimes they are required to be in every room by the municipality, so you might still find sprinklers alongside an FM-200 system. With a wet-pipe system, water is already in the sprinkler pipes, and heat from the fire melts the caps and releases the water. A more appropriate dry-pipe system has normally empty pipes, and smoke detectors will electronically trigger a preaction valve to fill the pipes (but the caps still have to melt before water comes out). The main purpose of sprinklers is to protect the building from collapse, not to protect the electronics in the room.</p>
<p>No matter what system is in place, modern facilities have electronic sensors throughout for monitoring and alerting building engineers, security, the fire department, and other pertinent parties in an automated fashion.</p>
<p><strong>OSHA Compliance</strong></p>
<p>Occupational Safety and Health Administration (OSHA) is a governmental entity (under the U.S. Department of Labor) that is tasked with providing regulations to maintain a safe workplace environment. Some of the rules you might find in a data center are not as common elsewhere.</p>
<p>Noise is becoming more of a problem, not just from fans on the IT equipment, but also on the HVAC systems. To maintain safe volumes, ear plugs or ear muffs are recommended for all personnel while inside the server room.</p>
<p>Procedures for removing floor tiles on raised floors should include cones or temporary barriers so that someone does not accidentally fall through an open hole. If the subfloor is deep or is superficial, workers should be harnessed, and the tiles should be tethered before removal.</p>
<p>Electrical safety is relevant because of the large amount of high-voltage circuits found in modern data centers. Any electrical maintenance or installations should be done by certified electricians. Large UPS cabinets have a potential for lethal electrical arcs, so arc-flash (“bunny”) suits must be worn during maintenance. All racks/cabinets, PDUs, and other electrical equipment must be properly grounded. An emergency power cutoff (aka “big red button”) should be installed, which when pressed will cut all power to the room (or module or bay) if someone is getting shocked.</p>
<p>For server rooms in older buildings, all locations of asbestos should be clearly marked, or technicians should be trained for awareness and the use appropriate safety precautions (respiratory masks) when running network cable (typically to/from other areas of the building).</p>
<p>Servers can be heavy, even 75 lb (35 kg) for a 4U server. Even some large network switches cannot be lifted and must be delivered by forklift. To reduce back strain and risk of injury, you should use teamwork to mount servers. There are also server lifts that can align a server to the appropriate height for installation and removal.</p>
<p>There should be an adequate number of well-marked emergency exits. This seems obvious but is more difficult in large, containerized data centers and where there a multiple floors in the same facility.</p>
<p><strong>References</strong></p>
<ol>
<li>The Green Grid (2008). Quantitative Efficiency Analysis of Power Distribution Configuration for Data Centers. www.thegreengrid.org/~/media/WhitePapers/White_Paper<em>16</em>-_Quantitative_Efficiency_Analysis_30DEC08.pdf?lang=en.</li>
</ol>
<h3 id="Geographic_Location_Criteria"><a href="#Geographic_Location_Criteria" class="headerlink" title="Geographic Location Criteria"></a>Geographic Location Criteria</h3><p><strong>Geographic Location Requirements</strong></p>
<p>For a large multinational corporation, there is a good chance that it already has a number of servers at each of its branch offices and would likely build traditional data centers in the countries in which it is already established. The reason for having equipment closer to the users is for better application responsiveness, which (hopefully) yields higher productivity.</p>
<p>A company whose product is Web based would need to build multiple data centers around the world to provide better service to customers in a wide variety of geographic locations. A common example of this is Web search—Google, Microsoft, and Yahoo! all have large data centers at many sites scattered around the globe. This variety in location provides faster search results to the user, which is a competitive advantage (as long as accuracy is maintained).</p>
<p>A third category of companies that require large data centers are Web hosting providers. They offer server(s) to host the website for companies of various sizes. Small companies might need to have their site hosted in only one location to serve local customers. Large companies might have several different regional sites but would prefer to deal with one hosting company rather than a different one in each region.</p>
<p>An organization in any of the above categories would have similar goals when it comes to picking the location of a new data center. In order to satisfy the facility’s needs, it is necessary to locate a data center where there are adequate electrical power suppliers. For the “green” conscious, one might look for an electricity supplier that uses hydroelectric, solar, wind, geothermal, or other renewable energy sources. Regardless of the source of the power, it is desirable to work with a utility company that is willing to negotiate bulk-purchasing agreements, which lock in a specific discounted price per kilowatt hour for several years.</p>
<p>In addition to power, if you want to take advantage of evaporative cooling techniques mentioned earlier in this unit, your data center should be located near an adequate water supply. There are also instances in which you can locate near a lake/reservoir that is naturally cold, use that water as your cold supply, and return it after tempering (lower the temperature by mixing again with cold water from the same source).</p>
<p>For a smaller scale data center, another factor to consider is the cost of real estate and taxes. For a large deployment, however, the cost of the IT and facilities equipment are much higher than the cost of land, and monthly energy costs far outweigh any taxes.</p>
<p>There are also basic logistics that are desirable to have available to support the data center and its life cycle. A supply chain is needed to procure the goods and services, not just servers but all equipment and building materials. There also should be adequate ports/rails/highways to deliver standard shipping containers (40’ × 8’ × 8’6”; or 12.19m × 2.43m x 2.59m) to your site. Small and medium sized data centers will typically buy servers from companies such as IBM, Dell, or HP, whereas big companies have custom servers, but none of these are manufactured on site and so have to be purchased and delivered. Containerized/modular solutions do more of the manufacturing off site so that installation is faster at the data center. Recycling of raw materials should occur at the end of the life span of the hardware or for failed components, such as hard drives, so recycling availability should also be considered.</p>
<p><strong>Weather</strong></p>
<p>Also mentioned in the previous module was economization, or mixing of colder outside air with the hot air expelled from the IT equipment. In Figure 2.16, you will notice different colored bands. In regions of yellow or orange, you could use outside cooling during the colder seasons, and in regions of green and blue, you could likely use outside cooling year round. Regions marked as red could still use outside cooling but for a few months of the year. For example, because of its cold average climate and renewable energy sources, Iceland has a growing data center industry.</p>
<p><img src="/images/14536687448435.jpg" alt="Figure 2.16: Global average temperature map (Wikipedia, 2014)."></p>
<p>As you will see in the power usage effectiveness (PUE) section later in this module, the energy utilization will be higher during hot months and lower during cold months. This is due to the efficiency gains from “free cooling” (either using air economizers or naturally cold water.)</p>
<p>The average amount of annual rainfall might be a factor if you are considering using rainwater storage/filtration as a water source for cooling your data center. However, the number of sunny days per year in a particular region might convince you to try solar.</p>
<p>Part of a risk assessment for a particular location would include the frequency of natural disasters in the region, such as floods, hurricanes, tornadoes/cyclones, tsunamis, and earthquakes. As you can see in Figure 2.17, different regions have varying levels of susceptibility to a potentially damaging event.</p>
<p><img src="/images/14536687773973.jpg" alt="(a) Earthquake risk map"></p>
<p><img src="/images/14536687993821.jpg" alt="(b) Flood risk map"></p>
<p><img src="/images/14536688116773.jpg" alt="(c) Hurricane risk map"></p>
<p><img src="/images/14536688215654.jpg" alt="(d) Lightening risk map"></p>
<p><img src="/images/14536688339101.jpg" alt="(e) Tornado risk map"></p>
<p><img src="/images/14536688504783.jpg" alt="(f) Thunderstorm risk map"></p>
<p><img src="/images/14536688631189.jpg" alt="(g) Volcano risk map"></p>
<p><img src="/images/14536688743833.jpg" alt="(h) Wildfire risk map"></p>
<p>Figure 2.17: Natural disaster threat maps (<a href="/Global Datavault">Global Datavault</a>, 2013). (Click on each figure for an enlarged view.)</p>
<p>As a cloud user, or cloud provider, it is beneficial to have two (or more) geographically distinct data centers for your services to mitigate risks of natural disaster, excluding large asteroid impacts, of course.</p>
<p><img src="/images/14536689245659.jpg" alt=""></p>
<p><strong>Connectivity</strong></p>
<p>As broadband adoption among consumers continues to grow, the effectiveness of cloud computing will increase. You can find trending graphs and current broadband adoption at Akamai.</p>
<p>The Internet relies on fiber optics to send and receive data over long distances. Figure 2.18 shows the relationship between multiple tiers of the Internet, in which tier 1 providers own the actual fiber cables, the network equipment, and the buildings they are housed in; tier 2 providers own large networks as well and peer with (have connections to) other tier 2 providers but also have to lease some connections from tier 1 providers in order to reach the whole Internet; and tier 3 providers are only resellers, which provide connections to end users. A single corporate entity, such as Verizon in the United States, may provide services at all tiers.</p>
<p><img src="/images/14536689450310.jpg" alt="Figure 2.18: Internet connectivity (Wikipedia, 2014)."></p>
<p>In order to support a large amount of users, a cloud provider should choose a data center location that is in a city/region that has a tier 2 or tier 1 provider. This will also decrease the latency to global users due to fewer hops (each router is a hop) between the client and the server. A data center’s requirement for uplink to the Internet ranges from a few megabits per second to several hundred gigabits per second, and that much bandwidth simply is not yet available everywhere.</p>
<h3 id="Costs"><a href="#Costs" class="headerlink" title="Costs"></a>Costs</h3><p><strong>Data Center Costs</strong></p>
<p>As you may recall from Unit 1, organizations have to deal with capital expenses and operating expenses for their IT projects. For new companies with an anticipated need for many servers, or for existing companies that have outgrown their existing infrastructure, a decision must be made to build a new data center, expand existing facilities, or migrate some (or all) of their IT services to a cloud provider. With rising energy costs, companies with existing (but outdated) data centers would also consider building a new data center or retrofitting the existing one with new power/cooling. Many scenarios, such a retrofit, might cause unacceptable downtime, so they would instead choose to go to a new location and migrate servers/services from the old location to a new data center. In any of the above scenarios, a detailed cost analysis would be helpful in making the decision (and likely required before any budgetary approvals).</p>
<p>Prices fluctuate often. What was valid in 2011 might not be valid in 2014. The intent of this section is not to give you the tools to do an entire cost analysis on your own but rather to help you understand the types of expenses that go into building a data center.</p>
<p><strong>Capital Expenditures (CapEx)</strong></p>
<p>Capital expenses (which occur only once) for data centers include upfront planning, cost of property and/or construction, facilities equipment (power, HVAC, safety, and security), as well as IT equipment (servers, network switches, initial network connectivity). A more detailed explanation of each follows:</p>
<p>Upfront planning: Upfront planning design costs account for a significant portion of CapEx. This typically includes feasibility and impact studies, architectural design, engineering design, project management costs, and contingency costs. A recent Forrester research study [1] estimates these costs to range from 20% to 25% of the total cost for a typical data center.</p>
<p>Property costs: A principal expense incurred in building data centers is the cost of property. This, of course, varies significantly depending on location. Organizations typically have the option of either constructing a building from scratch or to repurpose an existing building to be a data center.</p>
<p>When building from scratch, many costs, such as the real estate transaction, consultant, brokerage, and building permits, apply in addition to the actual cost of building the data center “shell,” which could include excavation, grading, roadways, utility connections, and physical security. Reed Construction Data has estimates for the United States and are typically between <code>$200</code> to <code>$300</code> per square foot, depending on the type of building and labor.</p>
<p>Many organizations opt to repurpose existing buildings, particularly those that have been abandoned, such as factories or mills, among others. Google’s Hamina data center in Finland was repurposed from an old paper mill. Barcelona Supercomputing Center is an example of an old chapel repurposed as a data center site.</p>
<p>Leasing a building or subleasing space in a building can also be done, shifting this capital expense to an operating expense.</p>
<p>Facilities equipment: Some heavy equipment is often installed during the construction phase, and once the building shell is complete, data center–specific facilities can be built, and equipment needs to be procured and commissioned. This includes most of the equipment outlined in the “Facilities” page in the “Data Center Components” module, such as cooling (CRACs, chillers, condensers, water tanks), electrical equipment (power distribution, generators, transformers, UPSs, automated transfer switch), and fire detection and suppression systems (FM-200).</p>
<p>Estimates for the cost of HVAC and electrical equipment are approximately $7,000 to $20,000 per kilowatt of IT load. [1] In addition, fire suppression systems costs can range from <code>$20,000</code> to <code>$60,000</code> for a typical data center spanning several thousand square feet.</p>
<p>IT equipment and connectivity: Once the data center facility is ready, equipment can be moved in and installed. Typically, this involves purchasing rack-mountable servers and networking and rack power distribution equipment. In larger data center environments, it is becoming increasingly popular to roll out fully containerized servers with integrated power, cooling, and network management. IT equipment costs can vary widely depending on the size and configuration of the hardware.</p>
<p>In addition to IT equipment, the data center needs to have a dedicated network connection in order for it to be accessible to the organization that is using it. Network service providers typically charge an upfront average of $10,000/mile to install and commission fiber to a data center. [1]</p>
<p><strong>Operating Expenditures</strong></p>
<p>Operational expenditures (periodically recurring) for data centers typically include electrical power, staffing, cooling/HVAC, Internet uplink, maintenance, taxes, and/or leasing.</p>
<p>One of the biggest operational expenses in running a data center is the power, and it can account for approximately 70% to 80% of the overall cost of running a data center. [1] Electrical utility charges to industrial sectors (which differs from residential pricing) can vary significantly from state to state and country to country. In the United States, the Energy Information Administration (EIA), an entity within the U.S. Department of Energy, publishes regular reports on power costs broken down by state and sector. The International Energy Agency (IEA) publishes energy data for other countries as well. It is also important to note that the cost of power has been rising over the past few years. [2] Because power forms a large percentage of operational expenses and total cost of ownership (TCO), large data centers tend to be placed in regions with cheaper sources of power, whenever possible.</p>
<p>Staffing: Even though data centers can house thousands of servers, they are relatively efficient in terms of number of personnel per server. However, data centers still need a number of operational staff such, as twenty-four seven security and an on-site engineering staff. In countries in which labor is expensive, this cost will end up being the second-largest recurring expense after power. These costs can run into hundreds of thousands of dollars per year, depending on the location and staffing levels.</p>
<p>Cooling: Cooling costs also amount to a significant portion of data center expenses. According to a study by the American Society of Heating, Refrigerating, and Air Conditioning Engineers (ASHRAE), data center cooling consumes 42% of the total power drawn by traditional, inefficient data centers. [3] Many organizations are now paying attention to new techniques to reduce the amount of power required to keep a data center cool. ASHRAE has also published thermal guidelines for data center designers to plan and correctly configure cooling systems to maximize efficiency.</p>
<p>Connectivity: In addition to upfront installation and commissioning costs, network connectivity is usually charged monthly or quarterly from a service provider. The fee is typically in the range of several hundred or thousand dollars per month, based on the bandwidth and reliability guarantees of the connection. [1]</p>
<p>Maintenance: Purpose-built data centers also incur significant maintenance overheads in order to keep the facility running smoothly and to minimize downtime. This cost could be 3% to 5% of the initial construction costs. [1] In addition, facilities equipment may need periodic replacement or repair. For example, UPS batteries are replaced once in 5 years, on average.</p>
<p>Taxes and/or leasing: Building permits are required when building new data center shells or if an existing building’s structure is altered significantly. Organizations also need to pay some form of annual property tax on the property owned. This cost, like others, can vary considerably from region to region. Average taxes are estimated to be $70 per square foot in the United States. [1] If you are leasing a property, many of these taxes are included in the monthly price of your lease.</p>
<p><strong>Reference</strong></p>
<ol>
<li>Rachel Dines, et al. (2011). “Build or Buy? The Economics of Data Center Facilities.” Forrester Research.</li>
<li>U.S. Energy Information Administration. (May 7, 2014). Annual Energy Outlook 2014. <a href="http://www.eia.gov/electricity/" target="_blank" rel="external">http://www.eia.gov/electricity/</a>.</li>
<li>Barroso, Luiz André, and Hölzle, Urs. (2009). The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines. Morgan &amp; Claypool Publishers. <a href="http://bnrg.eecs.berkeley.edu/~randy/Courses/CS294.F09/wharehousesizedcomputers.pdf" target="_blank" rel="external">http://bnrg.eecs.berkeley.edu/~randy/Courses/CS294.F09/wharehousesizedcomputers.pdf</a>.</li>
</ol>
<h3 id="Power_and_Efficiency"><a href="#Power_and_Efficiency" class="headerlink" title="Power and Efficiency"></a>Power and Efficiency</h3><p><strong>Server Utilization</strong></p>
<p>As you recall from the introduction unit, multiple servers are virtualized and consolidated onto fewer physical hosts to increase utilization and decrease energy costs. This improvement is helpful, but the demand for IT-related services in organizations continues to grow, as does the number of Web-based startup companies, which have a higher proportional demand for IT equipment.</p>
<p><a href="http://youtu.be/bkjdGO0jz3E" target="_blank" rel="external">Video 2.4: Data center efficency.</a></p>
<p>To see why server utilization is important, let us discuss power consumption when a server is idle (the CPUs are not doing anything, but the HDDs are spinning, and RAM and I/O devices still consume power) versus when the server is at maximum load (when all CPUs are at 100% utilization).</p>
<p>To estimate the power consumption (P) at a specific utilization (n%) use the following formula:</p>
<p><img src="/images/14536691108382.jpg" alt=""></p>
<p>Through empirical measurements, this approximation is accurate to within ±5% across utilization rates.</p>
<p><img src="/images/14536691310594.jpg" alt=""></p>
<p><img src="/images/14536691438298.jpg" alt=""></p>
<p>If an organization wants to decrease its overall monthly operating expenses, it has to do so both inside the rack and outside the rack, and to minimize the latter, you must first understand power usage effectiveness (PUE).</p>
<p><strong>Power Usage Effectiveness (PUE)</strong></p>
<p>As we have read earlier, a data center draws a significant amount of power, from kilowatts to several megawatts. The cost of power is a significant element of the operating expenses of a data center and hence contributes to the total cost of ownership (TCO). As companies are offering more Web-based services, which are housed at a data center, the cost of power becomes an important element in the cost of offering services on the Web. Furthermore, some projections claim that data center related emissions will triple by 2020. These economic and environmental factors have accelerated the interest in measuring and improving the energy efficiency of data centers.</p>
<p>The power drawn by a data center is shared between the IT equipment and the support equipment, such as the power distribution and cooling facilities. Data center energy efficiency can be thought of as the ratio of the energy delivered to the IT equipment to the total energy delivered to the data center. Clear and standardized efficiency metrics are needed to help data centers to understand their energy efficiency, identify areas for improvement, and perform comparisons over time.</p>
<p>The Green Grid consortium has developed the PUE and its inverse, the data center infrastructure efficiency (DCIE). The PUE is simply the ratio of the total power entering the data center divided by the power used by the IT equipment. PUE measures how efficiently the power is delivered to the IT equipment in the data center.</p>
<p><img src="/images/14536691932629.jpg" alt=""></p>
<p>If a data center’s PUE is 3.0, then the data center facilities (e.g., power distribution, cooling) utilizes 2 units of energy for every unit delivered to the IT equipment. The lower the PUE, the more efficient the data center facilities. An ideal PUE is 1.0, which would indicate 100% efficiency, meaning that all the power drawn by the data center was delivered to the IT equipment.</p>
<p><img src="/images/14536692227041.jpg" alt=""></p>
<p><img src="/images/14536692443849.jpg" alt=""></p>
<p>In 2007, the Lawrence Berkeley National Labs (LBNL) ran an energy study for 25 data centers (see Figure 2.19). The best PUE, 1.14, resulted in about 87% of the site energy reaching the IT equipment, while in the worst case (PUE 3.0), only 33% makes it to the IT equipment.</p>
<p><img src="/images/14536692548006.jpg" alt="Figure 2.19: PUE of 25 data centers studied by LBNL (Lawrence Berkeley National Labs, 2007)."></p>
<p>The PUE allows companies to identify areas for improvement, address these areas, and monitor the progress in PUE over time. Google publishes quarterly PUEs for their actual data centers, as shown in Figure 2.20. Because Google’s data centers are mostly in the northern hemisphere, the average PUE typically rises in the summer because they require increased use of the cooling equipment.</p>
<p><img src="/images/14536692713460.jpg" alt="Figure 2.20: PUE data for all large-scale Google data centers. "></p>
<p>Google improves the efficiency of its data centers using five methods. [1] First, they accurately measure and record the PUE as often as possible. Second, they design good air containment to limit the mixing of hot and cold air. Third, they increase the cold air temperatures because equipment manufacturers allow their equipment to run within aisles at higher temperatures. Fourth, they utilize free cooling whenever possible. This possibility is determined by the geographical location of their data centers and the number of hours per year that allow the use of cool ambient air, evaporating power, and large thermal reservoirs. Fifth, they minimize power distribution losses by eliminating several power conversion steps. The lowest recorded PUE to date at a Google data center is 1.08, which is around 92% efficiency.</p>
<p>PUE simply measures the efficient use of power. Understanding the energy contributors to PUE is important to improve data center design practices. However, PUE is not sufficient as the only measure because it does not account for the load on the IT equipment. If PUE is low, but the IT equipment is not doing useful work, then the data center is losing money. Some recent practices include utilizing a TCO metric that accounts for the cost of the server, which is the total cost of energy that it will consume while running a specific workload over its lifespan. Using this approach, data centers will utilize application-specific optimizations, which will lead to more effective use of the data center equipment.</p>
<p><strong>PDU Branches (Optional Reading)</strong></p>
<p>In an earlier module, you were introduced to a PDU, or power distribution unit. You also learned about inefficiencies related to multiple conversions. This section shows you the advantages of using three-phase power and some common pitfalls to avoid when choosing the size and number of PDUs. This section is optional reading as it goes into specific details concerning electrical engineering</p>
<p>There are several types of power distribution: rack level, row level, and room level. All of them have an input electrical feed and provide one or more branch circuits, with each branch protected by a circuit breaker (a safety device that will “trip” if there is an overload, stopping the flow of electricity). The difference with row-level PDUs is that they typically take a higher voltage and output to a lower voltage using a transformer (transformers generate heat, so you will also find cooling fans inside a row-level PDU).</p>
<p><img src="/images/14536692981877.jpg" alt="Figure 2.21: Row-level and rack-level PDUs."></p>
<p>Three-phase power (often denoted with Greek letter phi [3Φ]) is how AC electricity is generated and transmitted, with each phase a sine wave that is 120 degrees apart. It is not common to have three-phase power in homes, but it is common in industrial buildings and a requirement in any modern data center. It is important to keep each phase as evenly loaded as possible. You should not plug all servers into one branch before going on to the next—stagger them instead. The important thing to know is that the total power that can be handled by a three-phase circuit is greater than that of a single phase (1Φ) for each copper wire (same thickness/gauge).</p>
<p><img src="/images/14536693222840.jpg" alt=""></p>
<p>For example, given V = 120 volts and I = 15 amps, then single-phase power = 1800W per three wires; and given Vline = 208 volts and Iline = 15 amps, then three-phase power = 5410W per five wires. (Recall that it is only the current/amps that determines how thick the copper has to be.)</p>
<p>Knowing the maximum power of any branch is important for choosing the number and size of PDU(s) to power your IT equipment. U.S. electric code states that the line current should not exceed 80% of the rating of the breaker (or fuse).</p>
<p><img src="/images/14536693341164.jpg" alt=""></p>
<p>It is rare for a server to actually consume the number of watts that their power supply is rated. For example, a PSU is labeled as 975W, but the server might only consume 650W at maximum capacity. For this reason, it is useful to take power measurements of each new model of server at various loads (idle, 25%, 50%, 75%, 100%) before putting it into production.</p>
<p><img src="/images/14536693525163.jpg" alt=""></p>
<p>When designing a redundant system, it is common to have multiple independent paths from the power source to the IT equipment. You can think of redundant power (and cooling) systems like hard drives in RAID. Recall that in a RAID1 mirror, you withstand a drive failing, but you also lose half your capacity, which is analogous to 2N redundancy—you cannot load a single component (e.g., UPS, generator, PDU) greater than 50% of its original capacity. While in RAID5, you still withstand a single drive failure, but you get higher utilization of individual drives, which is what N+1 redundancy is like for power and HVAC equipment. The remaining (working) units have to absorb the workload of the failed unit.</p>
<p>For example, you have a 100kW load and desire to have redundant UPS. In a 2N system, you have two UPS, each capable of powering 100kW, but their normal load would be 50kW each. If you use three UPS units instead, each could be smaller and capable of handling 50kW each but having a normal load of 33kW. In this case, the utilization is higher (66% instead of 50%), so the UPS would also run more efficiently.</p>
<p>Although blade chassis and some larger 4U/5U servers have N+1 power supplies, most servers and network equipment are designed with 2N redundant power supplies. For this reason, in a traditional data center, you would run two independent power feeds to each rack. This way, if you lose power in one feed, the other can take over. You must be cautious, however, because when one of the two feeds fail, the other one gets double the load. Incorrect assumptions often lead to overloading a branch circuit, which goes unnoticed in normal conditions. Then when there is a single failure, it has a cascading effect, overloading the remaining branches and tripping the breakers. This single failure leads to some or all servers in a rack losing power completely.</p>
<p>Power and Efficiency: Redundancy</p>
<p>You have a 100kW load and are using an N+1 design for UPS, with 3 + 1 = 4 UPS total.</p>
<p><img src="/images/14536694765030.jpg" alt=""></p>
<p>You have 12 servers, each with dual-redundant PSUs, which are measured together to consume a maximum of 500W. You also have two power feeds (A and B) going to the rack. To each feed, a three-phase 208V PDU is attached. Each PDU has three branches (one per phase), each with a 20A breaker.</p>
<p><img src="/images/14536694916072.jpg" alt=""></p>
<p><strong>References</strong></p>
<ol>
<li>GoogleEfficiency: How Others Can Do It. <a href="http://www.google.com/about/datacenters/efficiency/external/" target="_blank" rel="external">http://www.google.com/about/datacenters/efficiency/external/</a>.</li>
</ol>
<h3 id="Redundancy"><a href="#Redundancy" class="headerlink" title="Redundancy"></a>Redundancy</h3><p><strong>Data Center Redundancy</strong></p>
<p>The following video (Video 2.5) covers some important aspects of data center redundancy:</p>
<p><a href="http://youtu.be/Vh6RfwiRxNY" target="_blank" rel="external">Video 2.5: Data center redundancy.</a></p>
<p><strong>Data Center Tier Classifications</strong></p>
<p>Data centers can be classified based on reliability. In order to understand the four different tiers, as specified in TIA-942 standard, you must first know what is meant by the word reliable. Reliability is most frequently measured in uptime, or availability. A service that is 100% reliable is extremely difficult to guarantee, and, therefore, no companies will make that claim in their service-level agreement (SLA) to a customer. But if they did, it would be available to users every second of every year.</p>
<p><img src="/images/14536695528250.jpg" alt=""></p>
<p>Tier 1 has non-redundant components, such as power, cooling, and network connections, which can lead to downtime for maintenance in addition to single points of failure (SPOFs) (service is disrupted if a single part of the overall system has a problem). Tier 2 still has a single path for power and cooling but adds redundancy, such as UPS and a generator. Tier 3 adds multiple paths for power (multiple UPS and PDUs from the source to the rack) and cooling (e.g., several CRACs feeding raised floors). Tier 3 allows for no interruptions from planned maintenance. Tier 4 is similar to tier 3, but all paths must be redundant and can continue operations at full capacity with at least one unplanned outage (e.g., losing main power or network provider, UPS failure, AC outage).</p>
<p>Even a tier 4 data center could incur downtime due to multiple simultaneous outages, as occurred for Amazon during a large thunderstorm in Virginia. [1]</p>
<p><img src="/images/14536695671871.jpg" alt=""></p>
<p>The percentage of availability can be measured in hours, minutes, or seconds. Downtime is calculated as:</p>
<p><img src="/images/14536695753393.jpg" alt=""></p>
<p>For example, the calculation for the downtime that a tier 4 data center should achieve is as follows:</p>
<p><img src="/images/14536695864344.jpg" alt=""></p>
<p><img src="/images/14536696069897.jpg" alt=""></p>
<p><strong>References</strong></p>
<ol>
<li>Malik, Om. (2012). Severe Storms Cause Amazon Web Services Outage. <a href="http://gigaom.com/2012/06/29/some-of-amazon-web-services-are-down-again/" target="_blank" rel="external">http://gigaom.com/2012/06/29/some-of-amazon-web-services-are-down-again/</a>.</li>
</ol>
<h3 id="Reliability_Metrics"><a href="#Reliability_Metrics" class="headerlink" title="Reliability Metrics"></a>Reliability Metrics</h3><p><strong>Mean Time Between Failures (MTBF)</strong></p>
<p>Sometimes, when you read material referencing availability and reliability, you will see the term nines used. Five nines or nine nines refers to the number of nines in a percentage of availability. Two nines is 99%, three nines is 99.9%, four is 99.99%, and so on.</p>
<p>You will also see the phrases mean time between failure (MTBF) and mean time to failure (MTTF) in the specifications for many individual components (e.g., hard drives, motherboards, power supplies). These are defined as the average number of hours that component is expected to last and are usually determined by the manufacturer using a sample of parts in more extreme conditions. However, reported failure rates in the field often are higher. For example, hard drives are rated 1 million hours or more, but they have been found to be 2 to 10 times higher, [1] and Google found drive failures rates to be 50% higher on average in their study. [2] The failure rate is 1/MTBF. For example, if the MTBF of a device is 100 hours, then the chances of that device failing in 1 hour is 1/100, 0.01, or 1%.</p>
<p>It is important to note that when determining the overall MTBF of a system that has non-redundant components, the MTBFs of each individual component add as a reciprocal. Formally,</p>
<p><img src="/images/14536696484378.jpg" alt=""></p>
<p>On the other hand, when a system consists of redundant components, required in both components simultaneously to have an overall system failure. The overall MTBF of the system is thus the product of the MTBFs of each individual redundant component of the system. Formally,</p>
<p><img src="/images/14536696577801.jpg" alt=""></p>
<p>Availability and Reliability</p>
<p>Assume you have 20,000 independent hard drives of a particular model in your data center, each with a manufacturer specified MTBF of 1 million hours. Assume you do not trust the manufacturer-specified MTBF, so divide by two to get 500,000 hours.</p>
<p><img src="/images/14536698176330.jpg" alt=""></p>
<p><img src="/images/14536698337164.jpg" alt=""></p>
<p>One factor that is often overlooked when considering uptime is human error. No matter how much redundancy is designed into the system, even if it is properly implemented and maintained, there is some likelihood of a mistake being made by a person. The result of which eventually leads to a service being unavailable (downtime). Some mistakes can be prevented through policy, specifying standard configurations, good documentation, and change management.</p>
<p>When it comes to large cloud deployments, there is little focus on the hardware resiliency of an individual server. When 10,000 or more servers are working together as part of a single application, the application itself builds in the fault tolerance (more on that in the next unit, “Resource Sharing”). In this situation, a single server failure, or even several, will not disrupt the application/service. Small and medium sized businesses, or even a large enterprise that has legacy applications, cannot afford to author these cloud-style, fully customized applications, so they rely on third-party software, most of which does not respond well to hardware failures. Instead, cloud providers will focus on server hardware that is inexpensive and as energy efficient as possible, removing unneeded parts.</p>
<p><strong>References</strong></p>
<ol>
<li>Schroeder, Bianca, and Gibson, Garth A. (2007). Disk Failures in the Real World: What Does an MTTF of 1,000,000 Hours Mean to You?. In Proceedings of the 5th USENIX Conference on File and Storage Technologies. 1–16 Pages.</li>
<li>Eduardo Pinheiro, Weber, Wolf-Dietrich, and Barroso, Luiz André. (2007). Failure Trends in a Large Disk Drive Population. In Proceedings of the 5th USENIX Conference on File and Storage Technologies.</li>
</ol>
<h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Servers mount into racks, come in incremental heights, called rack units (e.g., 1U, 2U, 3U), and have various depths. Larger cases allow for more expansion and/or redundant components.</li>
<li>Servers support more CPU sockets and DIMM slots than a desktop or laptop.</li>
<li>There are many types of PCI express cards available to expand servers, including RAID controllers, network cards (1, 10, or 40 gigabit Ethernet), Infiniband HCAs, storage HBAs, and coprocessors/accelerators, such as GPUs.</li>
<li>HDDs and SSDs can attach to a host through SATA or SAS and are used as building blocks for larger storage arrays, often employing RAID technology. These arrays can be a directly attached DAS, a remote block device SAN (using FC or iSCSI), or a remote file server NAS (using CIFS or NFS). Distributed file systems use many servers or arrays in parallel to increase performance and resiliency.</li>
<li>Data center networking topology is often multitier, with the lowest tier switches in the same rack (top of rack) or in the same row (end of row).</li>
<li>Ethernet remains the de-facto standard for data center networks, is available at a variety of speeds, and works over copper or fibre-optic cables.</li>
<li>Infiniband is a high-speed interconnect, now popular with HPC, and gaining acceptance in enterprise IT data centers.</li>
<li>Data centers and server rooms contain racks, raised floors, and cable trays and are designed with strong physical security.</li>
<li>Power distribution comes from the utility company or a backup generator and typically goes through a UPS, through PDUs, to the rack, and then server level. Some systems improve efficiency by converting AC to DC fewer times and/or decentralizing the UPS.</li>
<li>Servers are most often cooled through CRACs, which push cold air under a raised floor while taking in hot air and removing the heat through a chilled water loop. That loop is attached to a chiller, which itself removes heat by venting through the air or a secondary loop to a cooling tower. Higher densities/efficiencies can be achieved using in-row cooling with hot-aisle containment. Evaporative cooling is also becoming popular for data centers.</li>
<li>Safety is important for both equipment and personnel. FM-200 provides fast, safe fire suppression. All metal surfaces should be properly grounded, and safety equipment should be used appropriately.</li>
</ul>
<h2 id="Cloud_Management"><a href="#Cloud_Management" class="headerlink" title="Cloud Management"></a>Cloud Management</h2><p>You have learned the origins of a data center, from its roots in mainframe computers to the newest trends, and about what goes inside a data center (cooling, power, servers, network, and more), as well as some of the design criteria for various components that data center.</p>
<p>When designing large data centers, it is not possible to follow the same practices as that of a small data center. To truly leverage the economies of scale, it is important that the data center have a software layer that allows resources to be governed and managed easily. The cloud software stack is a platform to run a cloud given a pool of physical resources. Since most Cloud Service Providers (CSPs) are extremely wary of revealing their techniques (since it is their Intellectual Property) we have to rely on reverse engineering, rumors and the contribution of open-source cloud software stacks like OpenStack to understand the components involved.</p>
<p>In this module, we will start by learning about how a software middleware layer enables all the benefits of the cloud. We will look at the simplest use case, that of resource provisioning, and understand that a long series of steps are involved in handling a simple resource request. Of course, cloud providers charge for every quantifiable resource that a user utilizes. We must understand the details of the billing and monitoring systems that enables CSPs to profit from their data center. Automation and orchestration are some important techniques that we will look at the enable the low staff-to-resource ratio at CSPs, and drive down their effective costs.</p>
<p>Finally, we will look at OpenStack, an increasingly popular software platform that allows anyone with physical resources to create a cloud environment.</p>
<h3 id="Cloud_Middleware"><a href="#Cloud_Middleware" class="headerlink" title="Cloud Middleware"></a>Cloud Middleware</h3><p>Middleware is a general term for software that serves to “glue together” separate, often complex and already existing, programs. The term middleware is used in many contexts. For example, in the context of a single computer, middleware exists between the operating system kernel and application programs in the form of APIs, which manage access to system resources such as hardware devices.</p>
<p><img src="/images/14545511086121.jpg" alt="Figure 2.22: Cloud Middleware Features"></p>
<p>Cloud Middleware is a software platform that controls and coordinates different cloud services and makes it possible for users to issue service requests, and cloud providers to manage their infrastructure. Cloud Middleware consists of multiple abstraction layers that hides system complexity and enables communication between various applications, services, and devices that are part of a cloud service.</p>
<p><strong>Cloud Middleware Features</strong></p>
<p>There are a number of distinct and important features that cloud middleware provides, which come with several benefits. Some of the most important responsibilities of a cloud middleware stack are as follows:</p>
<p>Interoperability: Middleware is designed to connect distinct application services with different APIs to one another. Cloud service APIs act as middleware for cloud services by taking instructions from a program (written in languages such as Java, Python etc.), and translating them into service calls that the cloud service can understand. These instructions are further passed down the middleware stack at the cloud service provider’s end to perform actions (such as create virtual machines, allocate disk space, create a database table, etc.). Thus cloud middleware is the proverbial “glue” that enables multiple distinct applications and services to connect and communicate to each other.</p>
<p>Virtualization Management: Cloud middleware is also responsible for the configuration, allocation, creation, management and destruction of virtualized resources from physical resources. As an example, when a cloud service provider gets a request from a client to provision a virtual machine, it handles that request through multiple middleware layers until it reaches a hypervisor layer, which handles the configuration and allocation of a virtual machine for the client.</p>
<p>Resource Allocation and Scheduling: As discussed above, an important aspect of cloud middleware is the management of resources. As part of this responsibility middleware must manage resource allocation and scheduling of multiple resource types in order to achieve multiple goals such as performance, isolation, utilization, etc.</p>
<p>Load Balancing and Fault Tolerance: Cloud service providers must utilize adequate load balancing mechanisms in their middleware in order to optimize the distribution of load on multiple back-end services and physical infrastructure. The middleware should also coordinate with back-end resources in order to provide end-to-end fault-tolerance so that the availability of services to the client meets required SLOs.</p>
<p>Resource Monitoring: A crucial responsibility of middleware is the monitoring of resources. Monitoring provides a source of data which is valuable for the internal middleware features such as allocation, scheduling, load balancing and fault tolerance as discussed above. In addition, data from monitoring systems can be made available to clients, which gives them an additional visibility into the state of their applications and provisioned resources.</p>
<p>User Management and Security: Cloud middleware also must provide support for access control of users, and use standard security practices for the management of various types of credentials to control access to individual resources. The user management system in the middleware should provide features that allow cloud clients to create and destroy entities such as users and groups, and configure the Access Control Lists (ACLs) that define the resources that individual users and groups have access to.</p>
<p>User Interface and APIs: Finally, cloud middleware must make available a client-facing set of APIs. It is also typical of cloud middleware to provide user-friendly interfaces (typically in the form of Web interfaces), where clients can log in and manage their provisioned resources and make service requests.</p>
<p><img src="/images/14545512278768.jpg" alt=""></p>
<p><strong>References</strong></p>
<ol>
<li>Amrani C., Filali K., Ahmed K., Diallo A., Telolahy S. (2012). “A Comparative Study of Cloud Computing Middleware.” 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing.</li>
</ol>
<h3 id="Resource_Provisioning"><a href="#Resource_Provisioning" class="headerlink" title="Resource Provisioning"></a>Resource Provisioning</h3><p>The most basic role of a Cloud Service Provider is to package and isolate virtual parts of a physical data center and provide access for cloud users to provision resources on the cloud. Provisioning is the process of mapping abstract resource requests by cloud users to physical resources on servers within a data center.</p>
<p>In IAAS, this mainly involves launching virtual machines on top of hypervisors (these are special software applications that enable isolation of virtual machines on top of a physical machine- we will read more about them soon), as well as mounting storage volumes from a storage pool and creating a private network overlay for the user’s resources.</p>
<p>The techniques behind virtualizing compute, storage or networking resources is dealt with in future modules. Here, we focus on understanding some of the high level steps that a CSP must take to create and allocate a publicly accessible virtual machine with a fixed set of resources to an end user.</p>
<p><strong>Components of a Resource Provisioning System</strong></p>
<p>A resource provisioning system on the cloud generally has the following sub-parts:</p>
<ol>
<li>Access to a physical pool of resources- generally thousands or millions of servers, interconnected by a network (generally using a fat-tree topology as discussed in a previous module) and also a large pool of disks.</li>
<li>An identity management subsystem that maintains and validates the end user’s credentials for accessing many different types of resources; it also provides for role based access control.</li>
<li>A metering and monitoring system to detect utilization of physical resources</li>
<li>A billing and charge management system to map the metered resources to physical costs and take appropriate actions based on the user’s allowed privileges.</li>
<li>A resource manager that works with a hypervisor to map physical resources to virtual abstractions.</li>
<li>Often, the provisioning system will have a web front-end or an API.</li>
</ol>
<p><strong>Cloud End-to-End Service Provisioning Flow</strong></p>
<p>Figure 2.23 below shows the typical end-to-end steps for a customer provisioning a virtual machine from a Cloud Service Provider (CSP):</p>
<p><img src="/images/14545514768762.jpg" alt="Figure 2.23: Typical End-to-End IaaS provisioning steps"></p>
<p>The steps illustrated in Figure 2.23 are explained as follows:</p>
<ol>
<li>The customer logs on to the portal and is authenticated by the identity management system.</li>
<li>Based on the customer’s entitlement, the portal extracts a subset of services that the user can order from the service catalogue and constructs a ‘request catalog’.</li>
<li>The customer selects a service, e.g. a virtual server of a particular size. Associated to this service is a set of technical requirements such as the amount of vRAM, vCPU, etc. in addition to business requirements such as high availability or SLA requirements.</li>
<li>The orchestration tool extracts the technical service information from the service catalog and decomposes the service into individual parts, such as compute resource configuration, network configuration, and so on.</li>
<li>The provisioning process is initiated.</li>
<li>The virtual machine running on the server is provisioned using the server/compute domain manager.</li>
<li>The network, including firewalls and load balancers, as well the storage is provisioned by the network, network services and storage domain managers.</li>
<li>Charging is initiated for billing/chargeback and the change management case is closed and the customer is notified accordingly.</li>
</ol>
<p><img src="/images/14545515552417.jpg" alt=""></p>
<p><strong>Metering and Monitoring Cloud Services</strong></p>
<p>Cloud service providers (CSPs) charge their customers according to usage of their services. In order to do that, providers have to monitor the usage of their services carefully and have to include the cost of running these services in their infrastructure. This is mission critical for both CSPs and their customers.</p>
<p>The revenue stream of a CSP depends on the correctness of metering and monitoring their resources. Considering AWS revenues in 2014, losing metrics for a couple of hours can cost Amazon millions of dollars. On the other, overcharging customers for a couple of hours can highly affect their credibility.</p>
<p>From the customer’s point of view, the cost of cloud resources forms an important part of their expenses. In order for cloud clients to make budget plans, they must receive consistent bills every month. This poses important challenges to cloud providers for metering and monitoring.</p>
<p><strong>Challenges in Monitoring and Metering</strong></p>
<p>There are various costs included in cloud resources. Although fixed costs such as facilities, staff, servers are easy to calculate, variable costs require constant metering and monitoring. The advantage of using CSPs comes from paying only for the resources which are used. For example, provisioning an EC2 instance includes the cost of instance usage per hour, storage per GB-month for each storage type, and data transfer per GB-month. Even for this one resource, AWS has to keep track of these metrics for every instance and attached volume. In Figure 2.24, a possible break down of various cost for services can be found. If we imagine doing this for more than 1 million AWS customers for tens of different types of services, this will require the metering and monitoring of gigabytes of logs every minute and charging customers accordingly. The most popular model which is used to define such metrics is called the chargeback model.</p>
<p><img src="/images/14545516100780.jpg" alt="Figure 2.24: Metering in Different Types of Cloud Services"></p>
<p><strong>Chargeback Model</strong></p>
<p>Basically the chargeback model is the ability of an IT organization to measure the usage of resources and chargeback their customers accordingly. Traditionally the chargeback model is easy to implement since an IT department can easily divide its budget for the business units that it serves like software licences, stand-alone servers etc. This is challenging in cloud because the CSP needs to consider the rate and time of consumption.</p>
<p><strong>Validation of Metering and Monitoring</strong></p>
<p>From the customer’s point of view, verifiable metering is an important issue. There are some costs which are easy to measure like EC2 usage (hourly usage * cost per hour) but it is hard for customers to measure other resources such as data transitions or I/O requests. In order to verify metering and monitoring, users can work with certified cloud providers. For instance, IBM’s “Resilient Cloud Validation” program allows businesses who collaborate with IBM to perform a consistent program of benchmarking and validation of cloud services.</p>
<p><strong>Case Study: Ceilometer</strong></p>
<p>Although the underlying architecture of metering and measuring is hidden by corporate CSPs, Ceilometer is designed for OpenStack metering, billing and rating. The high-level architecture of OpenStack Ceilometer can be summarized as follows (Figure 2.25):</p>
<p><img src="/images/14545516558630.jpg" alt="Figure 2.25: Ceilometer Architecture"></p>
<p>Polling Agent: A daemon which polls OpenStack services for metering.</p>
<p>Notification Agent: A daemon which listens to notifications on the message queue and converts them into samples and events.</p>
<p>Collector: Daemon designed to gather metering data created by the notification and polling agents.</p>
<p>API: Service to query the data recorded by the collector.</p>
<p>Alarming: Daemons to evaluate and trigger notifications based on predefined rules.</p>
<p>Each of these services are designed to scale horizontally. Additional workers and nodes can be added according to expected load.</p>
<p><img src="/images/14545516976128.jpg" alt=""></p>
<h3 id="Cloud_Orchestration_and_Automation"><a href="#Cloud_Orchestration_and_Automation" class="headerlink" title="Cloud Orchestration and Automation"></a>Cloud Orchestration and Automation</h3><p>Cloud Orchestration is the process by which all of the provisioning, middleware and other complex systems in a cloud can be automatically arranged and coordinated. Today, cloud orchestration supports the specification and management of the following resources including but not limited to:</p>
<ul>
<li>Compute Servers (In the form of virtual machines)</li>
<li>Auto Scaling</li>
<li>Load Balancers</li>
<li>Databases</li>
<li>Block Storage</li>
<li>DNS and Virtual Network (VLANs)</li>
<li>Software configuration and setup (typically in the form of bootstrapping scripts)</li>
</ul>
<p><strong>Benefits of Orchestration</strong></p>
<p>Cloud orchestration is a method to fully realize the dynamic potential of cloud infrastructure, by allowing users to specify and configure a complete application encompassing multiple resource types. One of the most important aspects of the cloud is the rapid service delivery which is made possible by cloud orchestration. It also saves cost by eliminating manual intervention and management of IT services. Benefits of cloud orchestration can be summarized as follows:</p>
<ul>
<li>Integration, automation and optimization of service deployment across heterogeneous environments.</li>
<li>Self-service portals for selection of cloud services, including storage and networking, from a predefined menu of offerings.</li>
<li>Reduced need for intervention to allow lower ratio of administrators to physical and virtual servers.</li>
<li>Automated high-scale provisioning and de-provisioning of resources with policy-based tools to manage virtual machine sprawl by reclaiming resources automatically.</li>
<li>Ability to integrate workflows and approval chains across technology silos to improve collaboration and reduce delays.</li>
<li>Real-time monitoring of physical and virtual cloud resources, as well as usage and accounting chargeback capabilities to track and optimize system usage.</li>
<li>Making adoption of best practices easy by prepackaging automation templates and workflows for most common resource types.</li>
</ul>
<p><strong>Orchestrator Tools</strong></p>
<p>There are variety of tools which provides orchestration, Puppet and Chef are popular examples.</p>
<p><strong>Puppet</strong></p>
<p>Puppet is a tool that can be used to issue service commands to multiple client machines from a master machine. This allows developers and systems administrators to manage client machines from a single master machine, providing commands to individual clients based on code which describes the configuration actions that are to be performed on each machine:</p>
<p><img src="/images/14545518040764.jpg" alt="Figure 2.26: Puppet"></p>
<p><strong>Chef</strong></p>
<p>Chef uses the same concepts as Puppet, but differs in deployment. Chef operates using user-specified recipes, which describe the state of the resources in the system, such as the packages (and versions) to install, start up daemons or services to execute, or an data to be downloaded/created. This ensures an identical operating environment with the same resources and configurations across all systems. Using Chef, it is possible to automate the creation of a complex distributed system, stitching together various components and workflows.</p>
<p><img src="/images/14545518601244.jpg" alt=""></p>
<h3 id="Case_Study__3A_OpenStack"><a href="#Case_Study__3A_OpenStack" class="headerlink" title="Case Study : OpenStack"></a>Case Study : OpenStack</h3><p>Earlier, we described OpenStack as a popular solution that allows developers to build both public and private clouds. At the time of writing, OpenStack has mature offerings that cover a large part of the cloud software stack spectrum. We take a very quick conceptual overview of the various pieces of the OpenStack architecture, as the complete description of the OpenStack architecture is beyond the scope of this course. The purpose is to give you an overview of the different components that make up a cloud software stack.</p>
<p>OpenStack consists of multiple layers that can be used to configure, provision, manage, monitor and deprovision various types compute, storage and networking resources.</p>
<p>A high level view of the various services involved in the OpenStack middleware suite is represented in Figure 2.27 below:</p>
<p><img src="/images/14545518887840.jpg" alt="Figure 2.27: OpenStack Service Architecture"></p>
<p><strong>User Authentication Service (Keystone)</strong></p>
<p>The primary authentication service in OpenStack is called Keystone. Keystone is an OpenStack project that provides Identity, Token, Catalog and Policy services for use specifically by individual services in the OpenStack family.</p>
<p>The identity service provides authentication credential validation as well as data about users and groups. The token service validates and manages session tokens used for authenticating requests once a user’s credentials have already been verified. The catalog service provides an endpoint registry used for endpoint discovery. The policy service provides a rule-based authorization engine and the associated rule management interface which can be used to dynamically grant or deny resource privileges to specific applications and services.</p>
<p><strong>Monitoring Service (Ceilometer)</strong></p>
<p>The Ceilometer project aims to deliver a unique point of contact for billing systems to acquire all of the measurements needed to establish customer billing, across all current OpenStack core components with work underway to support future OpenStack components. Celiometer is designed to provide efficient collection of metering data, while ensuring metering messages are digitally signed and non-repudiable. Ceilometer was introduced in “Metering and Monitoring Cloud Services”.</p>
<p><strong>Orchestration Service (Heat)</strong></p>
<p>Heat is the primary orchestration system in OpenStack. Heat provides orchestration services for higher-level systems such as Sahara, OpenStack’s cluster provisioning system that is similar to AWS EMR. Heat can orchestrate individual cloud middleware services such as the compute service (nova), block storage service (Cinder) and networking services (Neutron).</p>
<p><strong>Virtual Machine Image Service (Glance)</strong></p>
<p>Glance image services include discovering, registering, and retrieving virtual machine images. Glance has a RESTful API that allows querying of VM image metadata as well as retrieval of actual virtual machine images.</p>
<p><strong>Object Storage Service (Swift)</strong></p>
<p>Swift is a highly available, distributed, eventually consistent object store, similar to Amazon’s S3. You create, modify, and get objects and metadata by using Swift’s Object Storage API, which is implemented as a set of REST web services. We will take a closer look at OpenStack’s Swift service in future modules.</p>
<p><strong>Block Storage Volume Management Service (Cinder)</strong></p>
<p>Cinder is an OpenStack project to provide “block storage as a service”, similar to Amazon’s Elastic Block Storage (EBS) service. Cinder allows users to define block storage devices and attach them as volumes to individual virtual machines. Cinder virtualizes pools of block storage devices and provides end users with a self service API to request and consume those resources without requiring any knowledge of where their storage is actually deployed or on what type of device. Cinder is also used by the Glance service to store virtual machine images as volumes. Cinder is designed to work with a growing number of storage systems and devices including Storage-Area-Network (SAN) appliances and distributed file systems.</p>
<p><strong>Cluster Provisioning and Management Service (Sahara)</strong></p>
<p>The Sahara project aims to provide users with a simple means to provision data processing frameworks (such as Hadoop, Spark and Storm) on OpenStack, much like the Elastic MapReduce (EMR) service on AWS. Sahara can be used to specify cluster configuration parameters such as the framework version, cluster topology, node hardware details and more. Sahara uses Nova to provision individual cluster nodes using framework specific images supplied by Glance. Sahara then runs special scripts to complete the configuration of each of the individual cluster nodes so that they are ready to execute jobs.</p>
<p><strong>Compute Service (Nova)</strong></p>
<p>Nova is an OpenStack project designed to provide massively scalable, on demand, self service access to compute resources. Nova is designed to provision, manage different virtual machines deployed using different virtualization platforms including Xen, VMWare, Hyper-V and can even deploy virtual machines onto EC2, with an aim to support the notion of “cloud bursting”. The Nova service is also used by other services such as Trove (to provision virtual machines that hold databases), and Sahara (to provision virtual machines for analytics clusters).</p>
<p><strong>Database-as-a-Service (Trove)</strong></p>
<p>Trove is a Database as a Service for OpenStack. It’s designed to run entirely on OpenStack, with the goal of allowing users to quickly and easily utilize the features of a relational database without the burden of handling complex administrative tasks. Cloud users and database administrators can provision and manage multiple database instances as needed. The service focuses on providing resource isolation at high performance while automating complex administrative tasks including deployment, configuration, patching, backups, restores, and monitoring.</p>
<p><strong>User Interface (Horizon)</strong></p>
<p>Horizon is the canonical implementation of OpenStack’s Dashboard, which provides a web-based user interface to OpenStack services including Nova, Swift, Keystone, etc. Users can log into Horizon to get interactive dashboard views of the status of their resources, and issue service requests to individual OpenStack services.</p>
<p><strong>Physical Hardware Management (Ironic)</strong></p>
<p>Ironic is a bare-metal provisioning system that can be used to control physical serves. Ironic has the ability to power on and power off individual servers using industry-standard protocols such as Intelligent Platform Management Interface (IPMI). Once machines are powered on, it can coordinate the machine boot up process using the Preboot eXecution Environment (PXE) boot, using specific boot images stored in the image management service (Cinder).</p>
<p>One may wonder why bare metal provisioning makes sense in a cloud environment where virtualized resources are the norm. There are situations where bare metal provisioning could be advantageous: A client that has a higher SLO requirement for compute performance for say, a high performance computing application, may require dedicated hardware without the overheads of a hypervisor. In such a case, it would be advantageous to provision and dedicate an entire physical server without virtualization.</p>
<p>As an example of one of the scenarios that Ironic can be used in, consider the case where there happens to be a physical server capacity crunch due to a large number of virtual machines that have been provisioned. In this case, Ironic can power on additional servers that are powered off and configure them to boot with a specific hypervisor image. Once the additional capacity is made available, virtual machines can be provisioned on the new servers, or existing virtual machines can be migrated to the new servers. In case of the reverse (where the physical server capacity is not being fully utilized), virtual machines can be consolidated to a smaller number of physical servers and the servers that have been freed up can be powered off to save on electric costs.</p>
<h3 id="Cloud_Software_Stack_Summary"><a href="#Cloud_Software_Stack_Summary" class="headerlink" title="Cloud Software Stack Summary"></a>Cloud Software Stack Summary</h3><ul>
<li>The cloud software stack enables the proivisioning, monitoring and metering of virtual user “resources” on top of a Cloud Service Provider’s infrastructure.</li>
<li>Cloud Middleware is the overarching platform that coordinates all of the different cloud services and allows them to be accessed as services by users.</li>
<li>Provisioning on the cloud refers to the creation of different resources on top of the physical infrastructure. A provisioning system deals with identity and cost management, scheduling resources.</li>
<li>Metering is an extremely challenging task on clouds. It requires tracking the utilization of network, storage IOPS, disk and compute capacity, by thousands or millions of concurrent users and mapping into different costs.</li>
<li>Orchestration and Automation are the processes that enable Cloud Service Providers to operate at scale, by running workflows and creating environments and configurations without manual intervention.</li>
<li>OpenStack is an open-source cloud stack implemenations that contains services for user authentication, provisioning, management, metering, compute, storage among others.</li>
</ul>
<h2 id="Cloud_Software_Deployment_Considerations"><a href="#Cloud_Software_Deployment_Considerations" class="headerlink" title="Cloud Software Deployment Considerations"></a>Cloud Software Deployment Considerations</h2><p>Now that you have seen how a cloud data center runs, you may feel that all of the complexity is handled by the Cloud Service Providers (CSPs), and it is trivial to build a cloud application. However, to truly fulfil the promise of the cloud, developers must design and deploy their applications following a few best practices.</p>
<p>In this module, we look at how applications are to be deployed on the cloud to ensure fault tolerance and achieve high performance. The global presence of cloud data centers simplifies the process of reaching many end users, but deployment patterns must support easy scaling and fault-tolerance.</p>
<p>A cloud application must be economical, reachable with low-latency, support a large number of simultaneous users (high throughput), without any service degradation (fault tolerance and elasticity). Despite the tools that CSPs provide, building such an application requires a lot of planning.</p>
<p>In the last module of Unit 2, we will look at some common patterns around load balancing and scaling, as well as how robust applications should be built.</p>
<p>Last, we explore some additional challenges faced by responsive, interactive applications that use a large cluster of cloud computing resources and look at some solutions.</p>
<h3 id="Programming_the_Cloud"><a href="#Programming_the_Cloud" class="headerlink" title="Programming the Cloud"></a>Programming the Cloud</h3><p><strong>Cloud Programming Considerations</strong></p>
<p>Designing programs that are destined for the cloud requires special considerations. Depending on the type of application and the expected load, developers can utilize some of the features provided by cloud providers to enhance the scalability and maintainability of programs. Use of automatic scaling systems and load balancers allow developers to dynamically grow or shrink infrastructure based on the utilization of hardware or a program-computed load factor.</p>
<p>There are multiple considerations that a developer must account for when developing or migrating an application to the cloud, particularly those that concern performance and security.</p>
<p><strong>Factors that Impact Application Performance on the Cloud</strong></p>
<p>The environment in a cloud-centric data center is different from what developers might be used to when designing and deploying applications on owned infrastructure. Some developers find it hard to fine-tune or enhance the performance of their applications because they do not have access to the physical hardware layout or specifications on public clouds. We will try to enumerate some of the top concerns, with specific emphasis on factors that affect application performance on the cloud:</p>
<p><strong>Resource Bandwidth and Latency</strong></p>
<p>A primary concern for developing and deploying cloud applications is latency. Developers must plan their applications with strict latency requirements in mind. One approach is to compile the distribution of client locations. This will allow developers to find the optimal set of data center locations which can be used to optimize end-user performance and responsiveness. This is particularly true in web applications, where individual HTTP requests for static web content can represent an important fraction of the web page load times.</p>
<p>Apart from latency, applications may also have strict bandwidth requirements, particularly with those that deal with rich multimedia content such as audio and video. Many cloud providers allow cloud developers to specify performance parameters during provisioning in the form of IOPS requirements for compute and storage resources. In addition, many cloud providers allow developers to set up virtual networks. The implementation and adoption of Software Defined Networking and Storage (covered in future modules) provide additional insights into newer techniques used by data centers to manage traffic from multiple clients, while managing individual requirements as specified in the client SLOs.</p>
<p>The techniques mentioned above are primarily targeted for static content. A far more difficult problem is to optimize the latency of access to distributed data storage systems, particularly those that have to handle writes and updates. We will learn a bit more of these concerns in future modules.</p>
<p><strong>Multi-tenancy</strong></p>
<p>Applications on public data centers typically run on shared infrastructure. This aspect of cloud services raised several important issues. While modern virtualization technologies provide an isolated environment in terms of application environment and security, they typically cannot ensure performance isolation. Therefore virtualized resources on clouds cannot guarantee consistent performance at all times, the performance of a resource at any given time is a function of the total load on the resources from all tenants, also known as the interference experienced from other tenants sharing the same hardware.</p>
<p>Some cloud providers such as AWS provide clients the ability to provision certain types of resources (such as EC2 instances) on dedicated hardware. This provides protection against wide fluctuations in resource performance, delivering fairly consistent performance for the resources. However, dedicated hardware instances cost considerably more than regular on-demand instances, as AWS needs to assign a server exclusively for your resources.</p>
<p>A related aspect of multi-tenancy is the issue of provisioning variation, wherein identical requests for virtual resources on public clouds are not mapped identically onto physical resources, thereby causing a variation in performance [1] . For example, two identical requests for virtual machines (VM1 and VM2) could be routed to two different physical machines (A and B). Physical machine A might have four other tenants competing for resources on the same machine, while machine B may have only two. However, the client is charged the same for virtual machines VM1 and VM2, but can potentially experience different performance on these machines.</p>
<p><strong>Security Settings</strong></p>
<p>Public clouds are subject to increased attack vectors, as we saw in Unit 1. Developers must be extremely cautious in ensuring that they follow best practices, protocols and procedures when deploying and maintaining applications on the cloud. As a result, additional performance overheads may be experienced due to the use of security protocols mandated by public clouds.</p>
<p>Since we have already discussed these protocols in a previous module, we will not discuss it in detail again. Any code deployed on a public cloud should go through a strict process of manual and automated source code reviews and static analysis, as well as dynamic vulnerability analysis and penetration testing. Guidelines for deploying applications securely are shown on the next page.</p>
<p><strong>References</strong></p>
<ol>
<li>Rehman, M.S and Sakr, M.F (2010). “Initial Findings for Provisioning Variation in Cloud Computing.” 2010 IEEE Second International Conference on Cloud Computing Technology and Science (CloudCom).</li>
</ol>
<h3 id="Deploying_Applications_on_the_Cloud"><a href="#Deploying_Applications_on_the_Cloud" class="headerlink" title="Deploying Applications on the Cloud"></a>Deploying Applications on the Cloud</h3><p>Once a cloud application has been designed and developed, it can be moved to the deployment phase for release to clients. Deployment can be a multi-stage process, each involving a series of checks to ensure that the goals of the application are met.</p>
<p>Before deploying a cloud application into production, it is useful to have a checklist to assist in evaluating your application against a list of essential and recommended best practices. Examples include the deployment checklist from <a href="https://media.amazonwebservices.com/AWS_Operational_Checklists.pdf" target="_blank" rel="external">AWS</a> and <a href="https://msdn.microsoft.com/en-us/library/azure/hh694044.aspx" target="_blank" rel="external">Azure</a>. Many cloud providers provide a comprehensive list of tools and services that assist in deployment, such as <a href="http://d0.awsstatic.com/whitepapers/overview-of-deployment-options-on-aws.pdf" target="_blank" rel="external">this document</a> from AWS.</p>
<p><strong>The Deployment Process</strong></p>
<p>The deployment of a cloud application is an iterative process which starts from the end of development right through to the release of the application on the production resources (Figure 2.28):</p>
<p><img src="/images/14545526245060.jpg" alt="Figure 2.28: Code deployment process"></p>
<p>It is typical for cloud developers to maintain multiple concurrently running versions of their applications to pipeline deployment of their application to into various stages:</p>
<ol>
<li>Testing</li>
<li>Staging</li>
<li>Production</li>
</ol>
<p>Each of the three stages mentioned above should ideally have identical resources and configuration which allows developers to test and deploy the application and minimize the chances of inconsistencies stemming from a change in the environment and configuration.</p>
<p><strong>Pipelining Application Changes</strong></p>
<p>In a typical agile application development scenario (illustrated in the figure above), applications are maintained by a set of engineers and developers who work on issues and bugs using some kind of issue tracking mechanism. The changes to the code are maintained through a code repository system (say, svn, mercurial or git), where separate branches are maintained for release of code. After passing through code changes, reviews and approvals, the code can be pipelined into the testing, staging and production phases. This can be done in multiple ways:</p>
<p>Custom Scripts: Developers can use custom scripts to pull the latest version of the code and run specific commands to build the application and bring it into production state.</p>
<p>Pre-Baked Virtual Machine Images: Developers can also provision and configure a virtual machine with all the required environment and software to deploy their application. Once configured, the virtual machine can be snapshotted and exported to a virtual machine image (such as an AMI in AWS), and this image can be provided to various cloud orchestration systems to be automatically deployed and configured for a production deployment.</p>
<p>Continuous Integration Systems: In order to simplify the various tasks that are involved in deployment, Continuous integration (CI) tools can be used to automate tasks (such as retrieval of the latest version from a repository, building application binaries and running test cases) that need to be completed in the various machines that make up the production infrastructure. Examples of popular CI tools include: Jenkins, Bamboo, Travis. AWS Code Pipeline is an AWS-specific CI tool designed to work with AWS deployments.</p>
<p><strong>Managing Downtime</strong></p>
<p>Certain changes to the application may require partial or full termination of the application services to incorporate a change in the application back-end. Developers have to typically schedule a specific time of day to minimize interruptions to customers of the application. Applications that are designed for continuous integration may be able to perform these changes live on production systems with minimal or no interruption to the application’s clients.</p>
<p><strong>Redundancy and Fault Tolerance</strong></p>
<p>Best practices in application deployment typically assume that cloud infrastructure is ephemeral and may be unavailable or change at any moment. For example, virtual machines deployed in an IaaS service may be scheduled for termination at the cloud provider’s discretion, depending on the type of SLA.</p>
<p>Applications must refrain from hard-coding or assuming static endpoints for various components, such as databases and storage endpoints. Well designed applications should ideally use service APIs to query and discover resources and connect to them in a dynamic fashion.</p>
<p>Catastrophic failures in resources or connectivity can happen at a moment’s’ notice. Critical applications must be designed in anticipation of such failures and must be designed for failover redundancy.</p>
<p>Many cloud providers design their data centers into regions and zones. A region is a specific geographic site which houses a complete data center, while zones are individual sections within a data center which are isolated for fault tolerance. For example two or more zones inside a data center may have separate power, cooling and connectivity infrastructure so that a fault in one zone will not affect the infrastructure in the other. Region and Zone information is typically made available by cloud service providers to clients and developers to design and develop applications that can utilize this isolation property.</p>
<p>Developers can therefore configure their application to use resources in multiple regions or zones in order to improve the availability of their application and tolerate failures that may happen across a zone or region. They will need to configure systems that can route and balance traffic across regions and zones. DNS servers can also be configured to reply to domain lookup requests to particular IP addresses in each zone, depending on where the request originated. This provides a method of load balancing based on the geographic proximity of clients.</p>
<p><strong>Security and Hardening in Production</strong></p>
<p>Running Internet applications on a public cloud must be done with care. Since cloud IP ranges are well-known locations for high-value targets, it is important to ensure that all applications deployed on the cloud follow best practices when it comes to securing and hardening endpoints and interfaces. Some very basic principles that should be followed include:</p>
<ol>
<li>All software should be switched to production mode. Most software supports “debug-mode” for local testing and “production-mode” for actual deployments. Debug-mode applications generally leak a large deal of information to attackers who send it malformed inputs and hence provide an easy source of reconnaissance for hackers. No matter if you are using a web-framework like Django and Rails or a database like Oracle, it is important to follow the relevant guidelines for deploying production applications.</li>
<li>Access to nonpublic services should be restricted to certain internal IP addresses for admin access. Make sure that administrators cannot directly log in to a critical resource from the Internet without visiting an internal launchpad. Configure firewalls with IP address and port-based rules to allow the minimal set of required accesses, especially over SSH and other remote connectivity tools.</li>
<li>Follow the principle of least privilege. Run all services as the least privileged user that can perform the required role. Restrict the use root credentials to specific manual logins by system administrators who need to debug or configure some critical problems in the system. This also applies to access to databases and administrative panels. Accesses should generally be protected using a long, random public-private key pair, and this key pair should be stored securely in a restricted and encrypted location. All passwords should have strict strength requirements.</li>
<li>Use well-known defensive techniques and tools for intrusion detection and prevention systems (IDS/IPS), security information and event management (SIEM), application layer firewalls, and anti-malware systems.</li>
<li>Set up a patching schedule that coincides with patch releases by the vendor of the systems that you use. Often, vendors like Microsoft have a fixed release cycle for patches.</li>
</ol>
<h3 id="Build_Fault-tolerant_Cloud_Services"><a href="#Build_Fault-tolerant_Cloud_Services" class="headerlink" title="Build Fault-tolerant Cloud Services"></a>Build Fault-tolerant Cloud Services</h3><p><strong>Failures and Fault Tolerance</strong></p>
<p>A large part of data center and cloud service management involves designing and maintaining a reliable service based on unreliable parts. The slide below (Figure 2.29) is a part of Google’s training for new hires, and should provide an idea of the large number (and types) of failures that are experienced regularly at a large data center.</p>
<p><img src="/images/14545528044106.jpg" alt="Figure 2.29: Reliability Issues from a Google Presentation"></p>
<p>A failure in a system occurs as a result of an invalid state introduced within the system due to a fault. Systems typically develop faults of one of the following types:</p>
<ol>
<li>Transient faults– Temporary fault in the system that corrects itself with time.</li>
<li>Permanent faults – Faults that cannot be recovered from and generally require replacement of resources.</li>
<li>Intermittent faults – Faults that occur periodically in a system.</li>
</ol>
<p>Faults may affect the availability of the system by bringing down the services or performance of the system functionalities. A fault-tolerant system has the ability to perform its function even in the presence of failures in the system. On the cloud, a fault-tolerant system is often thought of as one that provides services in a consistent manner with lower downtime than the agreed Service Level Agreements (SLAs) allow.</p>
<p><strong>Why is it Important?</strong></p>
<p>Failures in large mission critical systems can result in significant monetary losses to all parties concerned. By the very nature of cloud computing systems having a layered architecture, a fault in one layer of the cloud resources can trigger a failure in other layers above, or hide access to the layers below.</p>
<p>For example a fault in any hardware component of the system can affect normal execution of a Software as a Service application running on a virtual machine using the faulty resources. Faults in a system at any layer have a direct relation to the Service Level Agreements between the providers at each level.</p>
<p><strong>Proactive Measures</strong></p>
<p>Service providers take several measures in order to design the system in a specific way to avoid known issues, or predictable failures.</p>
<p><strong>Profiling and Testing</strong></p>
<p>Load and stress testing cloud resources in order to understand possible causes of failure is essential to ensure the availability of services. Profiling these metrics helps in designing a system that can successfully bear the expected load without any unpredictable behavior.</p>
<p><strong>Over-provisioning</strong></p>
<p>This is the practice of deploying resources in volumes that are larger than the general projected utilisation of the resource at a given time. In situations where the exact needs of the system cannot necessarily be predicted, over-provisioning resources can be an acceptable strategy in order to handle unexpected spikes in loads.</p>
<p>Consider as an example an e-commerce platform that has average consistent load on their servers year round, but during the holiday season the expectation is that the load pattern will spike up rapidly. At these peak times, it is advisable to provision extra resources based on the historical data for peak usage. A rapid rise in traffic is typically difficult to accommodate in a short period of time. As discussed in later sections, there is a time cost associated with scaling dynamically which involves the time consuming steps of detecting a change in the load pattern and provisioning extra resources to accommodate the new load, which will require time. This time delay in adjustment can be enough to overwhelm and at worst crash the system or at best degrade the Quality of Service.</p>
<p>Over-provisioning is also a tactic used to defend against DoS (Denial of Service) or DDoS (Distributed DoS) attacks, which is when attackers generate requests designed to overwhelm a system by throwing large volumes of traffic at them as an attempt to make the system fail. In any attack, it always takes some time for the system detect and take corrective measures. While such analysis of request patterns is being made, the system is already under attack and needs to be able to accommodate the increased traffic until a mitigation strategy can be implemented.</p>
<p><strong>Replication</strong></p>
<p>Critical systems components can be duplicated by using additional hardware and software components to silently handle failures in parts of the system without the entire system failing. Replication has two basic strategies:</p>
<ul>
<li>Active Replication, where all replicated resources are alive concurrently and respond to and process all requests. This means that for any client request, all resources receive the same request, all resources respond to the same request, and ensure that that the order of the requests is maintained to maintain state across all resources.</li>
<li>Passive Replication, where only the primary unit processes requests and secondary units merely maintain state and take over once the primary unit fails. The client is only in contact with the primary resource, which relays the state change to all secondary resources. The disadvantage of passive replication is that there may be either dropped requests or degraded QoS when switching from the primary to the secondary instance.</li>
</ul>
<p>There is also a hybrid strategy that is used, called semi-active, which is very similar to the active strategy with the difference that only the output of the primary resource is exposed to the client. The outputs of the secondary resources are suppressed and logged, and are ready to switch over as soon as a failure of the primary resource occurs. Figure 2.30 illustrates the differences between the replication strategies.</p>
<p><img src="/images/14545529419696.jpg" alt="Figure 2.30 : Replication Strategies"></p>
<p>An important factor to consider in replication is the number secondary resources to use. Although this differs from application to application based on the criticality of the system- there are 3 formal levels of replication:</p>
<ul>
<li>N+1 - This basically means that for an application that needs N nodes to function properly, one extra resource is provisioned as a fail-safe.</li>
<li>2N - At this level one extra node for each node required for normal function is provisioned as fail-safe.</li>
<li>2N+1 - At this level one extra node for each node required for normal function and one additional node overall is provisioned as a fail-safe.</li>
</ul>
<p><strong>Reactive Measures</strong></p>
<p>In addition to predictive measures, systems can take reactive measures and deal with failures as and when they happen:</p>
<p><strong>Checking and Monitoring</strong></p>
<p>All resources are constantly monitored in order to check for unpredictable behavior or loss of resources. Based on the monitoring information, recovery or reconfiguration strategies are designed in order to restart resources or bring up new resources. Monitoring can help in the identification of faults in the systems. Faults that cause a service to be unavailable are called crash faults and those that induce an irregular/incorrect behavior in the system are called byzantine faults.</p>
<p>There are several monitoring tactics that are used to check crash faults within a system. Two of them are:</p>
<ol>
<li>Ping-Echo, where the monitoring service asks each resource for its state and is given a time window to respond</li>
<li>Heartbeat, where each instance sends status to the monitoring service at regular intervals, without any trigger.</li>
</ol>
<p>Monitoring byzantine faults usually depends on the properties of the service being provided. Monitoring systems can check basic metrics like latency, CPU utilization, memory utilization, etc, and check against the expected values to see if the Quality of Service is being degraded. In addition application specific supervision logs are usually kept at each important service execution point and analysed periodically to see that the service is functioning properly at all times or whether there are injected failures in the system.</p>
<p>Checkpoint and Restart<br>Several programming models in the cloud implement checkpoint strategies, whereby state is saved at several stages of execution in order to enable recovery to a last saved checkpoint. In data analytics applications, there are often long running parallel distributed tasks that run on Terabytes of data sets to extract information. Since these tasks are executed in several small execution chunks, each step in the execution of the program can save the overall state of execution as a checkpoint. At points of failures where individual nodes are unable to complete their work, the execution can be restarted from a previous checkpoint. The biggest challenge while identifying valid checkpoints to roll back to is when parallel processes are sharing information. A failure in one of the processes may cause a cascading rollback in another process, as the checkpoints made in that process can be a result of fault in the data shared by the failing process. You will learn more about fault tolerance for programming models in future modules.</p>
<p><strong>Case Studies in Resiliency Testing</strong></p>
<p>Cloud services need to be built with redundancy and fault-tolerance in mind, as no single component of a large distributed system can guarantee 100% availability or uptime.</p>
<p>All failures including failures of dependencies in the same node, rack, data-center or regionally redundant deployments need to be handled gracefully without affecting the entirety of the system. Testing the ability of the system to handle catastrophic failures is important as sometimes even a few seconds of downtime or service degradation can cause hundreds of thousands, if not millions of dollars in revenue loss.</p>
<p>Testing for failures with real traffic needs to be done regularly so that the system is hardened and can cope when an unplanned outage occurs. There are various systems built to test resiliency, one such testing suite is Simian Army built by Netflix.</p>
<p>Simian Army consists of services (Monkeys) in the cloud for generating various kinds of failures, detecting abnormal conditions, and testing ability to survive them. The goal is to keep the cloud safe, secure, and highly available. Some of the Monkeys found in the Simian Army are:</p>
<ol>
<li>Chaos Monkey: A tool that randomly picks a production instance and disables it to make sure the cloud survives common types of failure without any customer impact. Netflix describes Chaos Monkey as “The idea of unleashing a wild monkey with a weapon in your data center (or cloud region) to randomly shoot down instances and chew through cables – all the while we continue serving our customers without interruption”. This kind of testing with detailed monitoring can expose various forms of weaknesses in the system and automatic recovery strategies can be built based on the results.</li>
<li>Latency Monkey: A service that induces delays in between RESTful communication of different clients and servers, simulating service degradation and downtime.</li>
<li>Doctor Monkey: A service that finds instances that are exhibiting unhealthy behaviors (eg: CPU load) and removes them from service. It allows the service owners some time to figure out the reason for the problem and eventually terminates the instance.</li>
<li>Chaos Gorilla: A service that can simulate the loss of an entire AWS availability zone. This is used to test that the services automatically rebalance the functionality among the remaining zones without user-visible impact or manual intervention.</li>
</ol>
<p><img src="/images/14545530896366.jpg" alt=""></p>
<h3 id="Load_Balancing"><a href="#Load_Balancing" class="headerlink" title="Load Balancing"></a>Load Balancing</h3><p>The need for load balancing in computing stems from two basic requirements, a system employs replication to provide high availability and improves performance through parallel processing. High availability is the property of a service that is available for near 100% of the time when any client tries to access the service. The Quality of Service of a particular service generally includes several considerations such as throughput, latency requirements among others.</p>
<p><strong>What is Load Balancing?</strong></p>
<p>The most well-known form of load balancing is “Round robin DNS” employed by many large web services to load balance requests among a number of servers. Specifically, multiple front-end web servers, each with a unique IP address share a DNS name. To balance the number of requests on each of these web servers, large companies like Google maintain and curate a pool of IP addresses associated with a single DNS entry. When a client makes a request (for e.g. to the domain www.google.com), Google’s DNS would select one of the available addresses from the pool and sends it to the client. The simplest strategy employed to dispatch IP addresses is to use a simple round-robin queue, where after each DNS response, the list of addresses are permuted.</p>
<p>Before the advent of the cloud, DNS load balancing was a simple way to tackle the latency of long-distance connections. The dispatcher at the DNS server was programmed to respond with the IP address of the server that was geographically nearest to the client. The simplest schemes to do this tried to respond with the IP address from the pool that was numerically the closest to the IP address of the client. This method, of course, was unreliable, as IP addresses are not distributed in a global hierarchy. Current techniques are more sophisticated and rely on a software mapping of IP addresses to locations based on physical maps of Internet Service Providers (ISPs). Since this is implemented as a costly software lookup, this method yields more accurate results, but is expensive to compute. However, the cost of a slow lookup is amortized since the DNS lookup occurs only when the first connection to a server is made by the client. All subsequent communications happen directly between the client and the server that owns the dispatched IP address. An example of a DNS load balancing scheme is shown in the figure below.</p>
<p><img src="/images/14545531403688.jpg" alt="Figure 2.31: Load Balancing in a Cloud Hosting Environment"></p>
<p>The downside of this method is in the case of a server failure, the switch over to a different IP address is dependent on the configuration of the Time-To-Live(TTL) of the DNS cache. DNS entries are known to be long-living and updates are known to take over a week to propagate over the Internet. Hence, it is difficult to quickly “hide” a server failure from the client. This can be improved by reducing the validity (TTL) of an IP address in the cache, but this occurs at the cost of performance and increasing the number of lookups.</p>
<p>Modern load balancing often refers to the use a dedicated instance (or a pair of instances) that directs incoming traffic to the backend servers. For each incoming request on a specified port, the load balancer redirects the traffic to one of the backend servers based on a distribution strategy. In doing so the load balancer maintains the request metadata including information like Application protocol headers (such as HTTP headers). In this situation, there is no problem of stale information as every request passes through the load balancer.</p>
<p>Though all types of network load balancers will simply forward the user’s information along with any context to the backend servers, when it comes to serving the response back to the client they may employ one of two basic strategies [1] :</p>
<ol>
<li>Proxying - In this approach the load balancer receives the response from the backend and relays it back to the client. The load balancer behaves as a standard web proxy and is involved in both halves of a network transaction, namely forwarding the request to the client and sending back the response.</li>
<li>TCP Handoff - In this approach the TCP connection with the client is handed off to the backend server and therefore the server sends the response directly to the client, without going through the load balancer.</li>
</ol>
<p><img src="/images/14545532403522.jpg" alt="Figure 2.32: TCP Handoff mechanism from the dispatcher to the backend server."></p>
<p><strong>How Does This Help With Availability and Performance?</strong></p>
<p>Load balancing is an important strategy to mask failures in a system. As long as the client of the system is exposed to a single endpoint that is balancing load across several resources, failures in individual resources can be masked from the client by simply servicing the request via a different resource. However it is important to note that the now the Load Balancer is the single point of failure of the service because if it fails for any reason, even if all backend servers are still functioning, no client request will be able to be served. Hence in order to achieve high availability, load balancers are often implemented in pairs.</p>
<p>Load balancing allows a service to distribute workloads across several compute resources in the cloud. Having a single compute instance in the cloud has several limitations. We have discussed earlier the physical limitation on performance, where more resources are required for increasing workloads. By using load balancing, larger volume of workloads can be distributed across multiple resources such that each resource can fulfil its requests independently and in parallel, thereby improving the throughput of the application. This also improves average service times since there are more servers to serve the workload.</p>
<p>Checking and monitoring services are key in enabling the success of load balancing strategies. A load balancer needs to ensure that every request is fulfilled by ensuring each resource node is available, if not then traffic is not directed that specific node. Ping-echo monitoring is one of the most popular tactic in order to check the health of a specific resource node. In addition to health of a node, some load balancing strategies require additional information such as throughput, latency, CPU utilization, etc., in order to evaluate the most appropriate resource to direct traffic.</p>
<p>Load Balancers must often guarantee high availability. The simplest way to do this is to create multiple load balancing instances (each with a unique IP address) and link it to a single DNS address. Whenever a load balancer instance fails for any reason, it is replaced with a new one, and all traffic is passed on to the failover instance with a small performance impact. Simultaneously, a new load balancer instance can be configured to replace the failed one, and the DNS records should be immediately updated.</p>
<p><strong>Strategies for Load Balancing</strong></p>
<p>There are several load balancing strategies in the cloud:</p>
<p><strong>Equitable Dispatching</strong></p>
<p>This is a static approach to load balancing where a simple round-robin algorithm is used to divide the traffic between all nodes evenly and does not take into consideration the utilisation of any individual resource node in the system or the execution time of any request. This approach tries to keep every node in the system busy and is one of the simplest approaches to implement. An important drawback to this approach is that heavy client requests may aggregate and hit the same data centers, causing a few nodes to get overwhelmed while others remain underutilised. However, this requires a very specific load pattern and has low probability of occurring in practice on a large number of clients and servers with fairly uniform connection distribution and capacity. This strategy also makes it difficult to implement caching strategies on the data center that take into account considerations like spatial locality (where you prefetch and cache data near the data that was currently fetched), since the next request made by the same client may end up on a different server.</p>
<p>AWS uses this approach in their ELB (Elastic Load Balancer) offering. AWS ELB provisions load balancers that balance the traffic across the attached EC2 instances. Load balancers are essentially EC2 instances themselves with a service to specifically route traffic. As the resources behind the load balancer are scaled out, the IP addresses of the new resources are updated on the DNS record of the load balancer. This process takes several minutes to complete as it requires both monitoring and provisioning time. This period of scaling in the load balancer to be able to reach a point where it can handle a higher load is referred to as “warming up” the load balancer.</p>
<p>AWS’ ELB load balancers also monitor each resource attached for workload distribution in order to maintain a health check. A ping-echo mechanism is used to ensure all resources are healthy. ELB users can configure the parameters of the health check by specifying the delays and the number of retries.</p>
<p>AWS also supports “sticky sessions”, whereby metadata is added to maintain a shared context between the client’s browser and the load balancer so that all requests from that connection session are passed to the same backend server. This allows a web server to maintain session state transparently. Without sticky sessions, traditional web applications would struggle to function since a login that was completed on one of the web servers would not be valid if the user’s next request would be sent to a different web server.</p>
<p><strong>Hash-based Distribution</strong></p>
<p>This approach tries to ensure that at any point the requests made by a client through the same connection always ends up on the same server. In addition, in order to balance the traffic distribution of requests, it is done in a random order (it is important to note that this is different from round-robin). This has several advantages over the round-robin approach as it helps in session-aware applications where state persistence and caching strategies can be much simpler. It is also less susceptible to traffic patterns that would result in clogging on a single server since the distribution is random, but the risk still exists. In addition since every request needs to be evaluated for connection metadata in order to route to a relevant server, it introduce a small amount of latency to every request.</p>
<p>Azure Load Balancing Services uses such a hash-based distribution mechanism in order to distribute load. This mechanism creates a hash for every request based on – Source IP, Source Port, Destination IP, Destination Port and Protocol Type, in order to ensure that every packet from the same connection always ends up on the same server. The hash function is chosen such that the distribution of connections to servers is fairly random.</p>
<p>Azure provides health-checks via three types of probes - Guest Agent probe (on PaaS VMs), HTTP custom probes and TCP custom probes. All three probes provide health-check for the resource nodes via a ping-echo mechanism.</p>
<p><strong>Other Popular Strategies</strong></p>
<p>There are several other strategies used to balance load across multiple resources. Each of them uses different metrics to gauge the most appropriate resource node for a particular request:</p>
<ol>
<li>Request execution time based strategies - These strategies use a priority scheduling algorithm, whereby request execution times are used in order to judge the most appropriate order of load distribution. The main challenge in using this approach is to predict the execution time of a particular request.</li>
<li>Resource utilization based strategies – These strategies use the CPU utilization on each resource node to balance the utilization across each node. The load balancers maintain an ordered list of resources based on their utilization and thus direct requests to the least loaded node.</li>
</ol>
<p><strong>Other Benefits of Employing a Load Balancer</strong></p>
<p>Having a centralized load balancer lends itself to several strategies that can used to increase the performance of the service. However it is important to note that this strategy only works as long as the load balancer is not under insurmountable load, otherwise the load balancer itself becomes the bottleneck. Some of these are listed below:</p>
<ul>
<li>SSL Offload - Network transactions via SSL have a an extra cost associated with them since they need to have processing for encryption and authentication. Instead of serving all requests via SSL, the client connection to the load balancer can be made via SSL, while redirect requests to each individual server can be made via HTTP. This reduces the load on the servers considerably. Additionally security is maintained as long as the redirect requests are not made over an open network.</li>
<li>TCP Buffering - This is a strategy to offload clients with slow connections on to the load balancer in order to relieve servers that are serving responses to these clients.</li>
<li>Caching - In certain scenarios, the load balancer can maintain a cache for the most popular requests (or request that can be handled without going to the servers, like static content) so that it reduces the load on the servers.</li>
<li>Traffic Shaping - For some applications a load balancer can be used to delay/reprioritize the flow of packets such that traffic can molded to suit the server configuration. This does affect the QoS for some requests, but makes sure that the incoming load can be served.</li>
</ul>
<p><img src="/images/14545534121194.jpg" alt=""></p>
<p><strong>References</strong></p>
<ol>
<li>Aron, Mohit and Sanders, Darren and Druschel, Peter and Zwaenepoel, Willy (2000). “Scalable content-aware request distribution in cluster-based network servers.” Proceedings of the 2000 Annual USENIX technical Conference.</li>
</ol>
<h3 id="Scaling_Resources"><a href="#Scaling_Resources" class="headerlink" title="Scaling Resources"></a>Scaling Resources</h3><p>One of the important advantages of the cloud is the ability scale resources into a system on-demand. Scaling up (provisioning larger resources) or scaling out (provisioning extra resources) can help in reducing the load on a single resources by decreasing utilization as a result of increased capacity or broader distribution of the workload.</p>
<p>Scaling can help in improving performance by increasing the throughput, since a larger number of requests can now be served. This also helps in improving latency since a reduced number of requests are queued during peak loads on a single resource. In addition this can also help in improving the reliability of the system by reducing the resource utilization to be farther away from the breaking point of the resource.</p>
<p>It is important to note that although the cloud enables us to easily provision newer or better resources, cost is always an opposing factor that needs to be considered. Therefore even though it is beneficial to scale up/out, it is also important to recognize when to scale in/down in order to save costs. In an n-tier application it is also essential to pinpoint where the bottlenecks are and which tier to scale, whether it is the data tier or server tier.</p>
<p>Scaling resources is facilitated by load balancing (we discussed this earlier), which helps in masking the scaling aspect of a system by hiding it behind a consistent endpoint.</p>
<p><strong>Scaling Strategies</strong></p>
<p><strong>Horizontal Scaling (Scale Out/In)</strong></p>
<p>Horizontal scaling is a strategy where additional resources can be added into the system or extraneous resources can be removed from the system. This type of scaling is beneficial for the server tier, when the load on the system is unpredictable and fluctuates inconsistently. The nature of the fluctuating load makes it essential to efficiently provision the correct amount of resources to handle the load at all times.</p>
<p>A few considerations that make this a challenging task is the spin up time of an instance, the pricing model of the cloud service provider and the potential loss in revenue from degrading Quality of Service by not scaling out in time. As an example let’s consider the following load pattern (Figure 2.33):</p>
<p><img src="/images/14545536485618.jpg" alt="Figure 2.33: Sample Request Load Pattern"></p>
<p>Let us imagine we are using Amazon’s Web Services, let us also imagine that each unit of time is equivalent to 3 hours of actual time and that we require one server to serve 5k requests. If you consider the load during the time units 16 to 22, there is an enormous fluctuation in the load. We can detect a fall in demand at right around time unit 16 and start to reduce the number of allocated resources. Since we are going from roughly 50k requests to almost 0 requests in the space of 3 hours, academically we can save the cost of 10 instances that would have been up at time 16.</p>
<p>Now let us imagine instead that each time unit is equal to 20 mins of actual time. In that case spinning down the all the resources at time unit 16 only to spin up new resources after 20 mins will actually increase the cost instead of saving, since AWS bills each compute instance on an hourly basis.</p>
<p>In addition to the above two considerations, a service provider will also need to evaluate the losses they will incur by providing degraded QoS during time unit 20, if they only have capacity for 90k requests instead of 100k requests.</p>
<p>The scaling depends on the characteristics of the traffic and its ensuing load generated at a web service. If the traffic follows a predictable pattern, for example based on human behavior such as streaming movies from a web service in the evening, then the scaling can be predictive in order to maintain the QoS. However, in many instances, the traffic cannot be predicted and the scaling systems need to be reactive based on different criteria as the examples above showed.</p>
<p><strong>Vertical Scaling (Scale Up/Down)</strong></p>
<p>There are certain kinds of loads for service providers that are more predictable than others. For example if you know from historical patterns that the number of requests will always be between 10k-15k, then you can comfortably assume that one server that can serve 20k requests will be good enough for the service provider’s purposes. These loads may increase in the future, but as long they increase in a consistent manner, the service can be moved to larger instance, that can serve more requests. This is suitable for small applications that experience a low amount of traffic.</p>
<p>The challenge in vertical scaling is that there is always some switch-over time that can be considered as down time. This is because in order to move all operations from the smaller instance to a larger instance, even if the switch-over time is mere minutes, the Quality of Service does degrade during that interval.</p>
<p>In addition, most cloud providers provide compute resources in increasing compute power by doubling the compute power of a resource. Therefore the granularity in scaling up is not as high as it is in horizontal scaling. So even if the load is predictable and steadily increasing as the popularity of the service increases, many service providers choose to do horizontal scaling instead of vertical scaling.</p>
<p><strong>Considerations for Scaling</strong></p>
<p><strong>Monitoring</strong></p>
<p>Monitoring is one of the most crucial element in scaling resources as it enables you to have metrics that can be used to interpret what parts of the system to scale and when to scale it. Monitoring enables analyzing traffic patterns or resource utilization in order to make an educated assessment to scale resources in order to maximize QoS along with profit.</p>
<p>There are several aspects of resources that are monitored in order to trigger scaling of resources. The most common metric is resource utilization. For example a monitoring service can track the CPU utilization of each resource node and scale resources if the usage is excessive or too low. If for instance the usage for each resource is higher than 95% then it is probably a good idea to add more resources since the system is under a heavy load. Service providers usually decide these trigger points by analyzing the breaking point of resource nodes, when they will start to fail, and mapping out their behavior under various levels of load. Though, for cost reasons, it is important to make maximum utilization out of each resource, it is advisable to leave some room for the Operating System to allow for overhead activities. Similarly if the utilization is significantly below say 50%, then it is possible that not all the resource nodes are required and can be un-provisioned.</p>
<p>In practice, service providers usually monitor a combination of several different metrics of a resource node to evaluate when to scale resources. Some of these include CPU utilization, Memory Consumptions, Throughput, Latency, etc. AWS provides CloudWatch as an additional service that can monitor any AWS resource and provide such metrics. There are other solutions available in the market as well like Nagios.</p>
<p><strong>Statelessness</strong></p>
<p>A stateless service design lends itself to a scalable architecture. A stateless service essentially means that the client request contains all the information necessary to serve a request by the server. The server does not store any client related information on the instance and does store any session related information on the server instance.</p>
<p>Having a stateless service helps in switching resources at will, without any configuration required to maintain the context (state) of the client connection for subsequent requests. If the service is stateful, then the scaling of resources needs to implement a strategy to transfer the context from the existing node configuration to the new node configuration. However there are techniques to implement stateful services like maintaining a network cache such as Memcached where the context is shared across the servers.</p>
<p><strong>Deciding What to Scale</strong></p>
<p>Depending on the nature of the service, different resources need to be scaled depending on the requirement. For the server tier, as the workloads increase, depending on the type of application, it may increase the resource contention for either CPU, memory, network bandwidth, or all of the above. Monitoring the traffic allows us to identify which resource is getting constrained and appropriately scale that specific resource. Cloud Service Providers do not necessarily provide scalability granularity to only scale compute or memory, but they do provide different types of compute instances that specifically cater to compute heavy or memory heavy loads. So for example for an application that has memory intensive workloads, it would be more advisable to scale up the resources to memory optimized instances, or for applications that need to serve a large number of requests that are not necessarily compute heavy or memory heavy, scaling out multiple standard compute instances might be a better strategy.</p>
<p>Increasing hardware resources may not always be the best solution for increasing the performance of a service. Increasing the efficiency of the algorithms used within the service can also help in reducing resource contention and improve utilization, removing the need to scale physical resources.</p>
<p><strong>Scaling the Data Tier</strong></p>
<p>In data-oriented applications, where there is a high number of reads and writes (or both) to a database or storage system, the round trip time for each request is often limited by the hard disk I/O read and write times. Larger instances allow for higher I/O performance for reads and writes which can improve seek times on the hard disk can in turn result in a large improvement in the latency of the service. Whereas having multiple data instances in the data tier can improve the reliability and availability of the application by providing failover redundancies. Replicating data across multiple instances has additional advantages in reducing network latency if the client is served by a data center physically closer to it. Sharding or partitioning of the data across multiple resources is another horizontal data scaling strategy where instead of simply replicating the data across multiple instances, data is partitioned into several partitions and stored across multiple data servers.</p>
<p>The additional challenge when it comes to scaling the data tier is that of maintaining all the three facets of Consistency (a read operation on all replicas is the same), Availability (reads and writes always succeed) and Partition Tolerance (guaranteed properties in the system are maintained when failures prevent communication across nodes) in the system. This is often referred to as the CAP theorem which in short states that within a distributed database system, it is very difficult to obtain all three properties completely and thus may at most exhibit a combination of two of the properties. You will learn more about database scaling strategies and the CAP theorem in future modules.</p>
<p><img src="/images/14545537883625.jpg" alt=""></p>
<h3 id="Dealing_with_Tail_Latency"><a href="#Dealing_with_Tail_Latency" class="headerlink" title="Dealing with Tail Latency"></a>Dealing with Tail Latency</h3><p>We have already discussed several optimization techniques used on the cloud to reduce latency. Some of the measures we studied include scaling resources horizontally or vertically and using a load balancer to route requests to the nearest available resources. This page delves more deeply into why, in a large data center or cloud application, it is important to minimize latency for all requests, and not just optimize the general case. We will study how even a few high-latency outliers can significantly degrade the observed performance of a large system. This page also covers various techniques to create services that provide predictable low-latency responses even if the individual components do not guarantee this. This is a problem that is especially significant for interactive applications where the desired latency for an interaction is below 100ms.</p>
<p><strong>What is Tail Latency?</strong></p>
<p>Most cloud applications are large, distributed systems often rely on parallelization as a source of reducing latency. A common technique is to fan-out a request received at a root node (for e.g. a front-end web server) to many leaf nodes (back-end compute servers). The performance improvement is driven both by the parallelism of the distributed computation, and also by the fact that extremely expensive data-moving costs are avoided. We simply move the computation to where the data is stored. Of course, each leaf node concurrently operates on hundreds or even thousands of parallel requests.</p>
<p><img src="/images/14545538640150.jpg" alt="Figure 2.34: Latency due to scale out"></p>
<p>Consider the example of searching for a movie on Netflix. As a user begins to type in the search box, this will generate several parallel events from the root web server, which include at least the following requests:</p>
<ol>
<li>to the autocomplete engine to actually predict the search being made based on past trends and the user’s profile,</li>
<li>to the correction engine which finds errors in the typed query based on a constantly adapting language model,</li>
<li>individual search results for each of the component words of a multi-word query, which must be combined together based on the rank and relevance of the movies,</li>
<li>additional post-processing and filtering of results to meet the user’s “safe-search” preferences</li>
</ol>
<p>Such examples are extremely common. A single Facebook request is known to contact thousands of memcached servers, whereas a single Bing search often contacts over ten thousand index servers.</p>
<p>Clearly, the need for scale has led to a large fan-out at the back-end for each individual request serviced by the front-end. For services that are expected to be “responsive” to retain their user base, heuristics show that responses are expected within 100 ms. As the number of servers required to resolve a query increases, the overall time often depends on the worst performing response from a leaf node to a root node. Assuming that all leaf nodes must complete executing before a result can be returned, the overall latency must always be greater than the latency of the single slowest component.</p>
<p>Like most stochastic processes, the response time of a single leaf node can be expressed as a distribution. Decades of experience have shown that in the general case, most (&gt;99%) requests of a well-configured cloud system will execute extremely quickly. But often, there are very few outliers on a system that execute extremely slowly.</p>
<p><img src="/images/14545540355724.jpg" alt="Figure 2.35: Tail Latency Example 5"></p>
<p>Consider a system where all leaf nodes have an average response time of 1 ms, but there is probability of a 1% that the response time is greater than 1000 ms (one second). If each query is handled by only a single leaf node, the probability of the query taking longer than one second is also 1%. However, as we increase the number of nodes to 100, the probability that the query will complete within one second drops to 36.6%, which means that there is a 63.4% chance that the query duration will be determined by the tail (lowest 1%) of the latency distribution.</p>
<p><img src="/images/14545540699473.jpg" alt=""></p>
<p>If we simulate this for a variety of cases, we see that as the number of servers increase, the impact of a single slow query is more pronounced (notice that the graph below is monotonically increasing). Also, as the probability of these outliers decreases from 1% to 0.01%, the system is substantially lower.</p>
<p><img src="/images/14545540831998.jpg" alt="Figure 2.36: Response time probability and the 50%ile, 95%ile and 99%ile latency of requests in a recent study."></p>
<p>Just like we designed our applications to be fault-tolerant to deal with resource reliability problems, it should be clear now why it is important for applications to be “tail-tolerant”. To be able to do this, we must understand the sources of these long performance variabilities and identify mitigations where possible and workarounds where not.</p>
<p><strong>Variability in the Cloud: Sources and Mitigations</strong></p>
<p>To resolve the response time variability that leads to this tail latency problem, we must understand the sources of performance variability [1] .</p>
<ol>
<li>The use of shared resources: Many different VMs (and applications within those VMs) contend for a shared pool of compute resources and in rare cases it is possible that this contention leads to low-latency for some requests. For critical tasks, it may make sense to use dedicated instances and periodically run benchmarks when idle, to ensure that it behaves correctly.</li>
<li>Background daemons and maintenance: We have already spoken about the need for background processes to create checkpoints, backups, update logs, collect garbage and handle resource clean up. However, these can degrade the performance of the system while executing. To mitigate this, it is important to synchronize disruptions due to maintenance threads to minimize the impact on the flow of traffic. This will cause all variation to occur in a short, well-known window rather than randomly over the lifetime of the application.</li>
<li>Queueing: Another common source of variability is the burstiness of traffic arrival patterns [1] . This variability is exacerbated if the OS uses a scheduling algorithm other than FIFO. Linux systems often schedule threads out-of-order to optimize the overall throughput and maximize utilization of the server. However, studies have found that using FIFO scheduling in the OS reduces tail latency but may also reduce the overall throughput of the system.</li>
<li>All-to-all incast: The pattern shown in Fig 2.34 above is known as all-to-all communication. Since most network communication is over TCP, this leads to thousands of simultaneous requests and responses between the front-end web server and all the back-end processing nodes. This is an extremely bursty pattern of communication and often leads to a special kind of congestion failure known TCP incast collapse [1] [2] . The intense sudden response from thousands of servers leads to many packet drops and retransmissions, eventually causing a network avalanche of traffic for packets of data that are very small. Large data centers and cloud applications would often need to use custom network drivers to dynamically adjust the TCP receiving window and the retransmission timer. Routers may also be configured to drop traffic that exceeds a specific rate and reduce the size of the sending.</li>
<li>Power and temperature management: Finally, variability is a byproduct of other cost reduction techniques like using idle states or CPU frequency down-scaling. A processor may often spend a non-trivial amount of time scaling up from an idle state. Turning off such cost optimizations lead to higher energy usage and costs, but lower variability. This is less of a problem in the public cloud as pricing models rarely consider internal utilization metrics of customers’ resources.</li>
</ol>
<p>Experiments conducted on AWS EC2 instances have found that the variability of such systems are much worse on the public cloud [3] , typically due to imperfect performance isolation of virtual resources and the shared processor. This is exacerbated if many latency-sensitive jobs are executed on the same physical node as CPU-intensive jobs.</p>
<p><strong>Living with Variability: Engineering Solutions</strong></p>
<p>Many of the sources of variability above have no fool-proof solution. Hence, instead of trying to resolve all of the sources that inflate the latency tail, cloud applications must be designed to be tail-tolerant. This, of course, is similar to the way that we design applications to be fault-tolerant since we cannot possibly hope to fix all possible faults. Some of the common techniques to deal with this variability are:</p>
<ol>
<li>“Good enough” results: Often, when waiting to receive results from 1000s of nodes, the importance of any single result may be assumed to be quite low. Hence, many applications may choose to simply respond to the users with results that arrive within a particular, short latency window and discard the rest.</li>
<li>Canaries: Another alternative that is often used for rare code paths is to test a request on a small subset of leaf nodes in order to test if it causes a crash or failure that can impact the entire system. The full fan-out query is only generated if the canary does not cause a failure. This is akin to sending a canary (bird) into a coal mine to test if it is safe for humans.</li>
<li>Latency-induced probation and health checks: Of course, a bulk of the requests to a system are too common to test using a canary. Such requests are more likely to have a long-tail if one of the leaf nodes is performing poorly. To counter this, the system must periodically monitor the health and latency of each leaf node and not route requests to nodes that demonstrate low performance (due to maintenance or failures).</li>
<li>Differential QoS: Separate service classes can be created for interactive requests, allowing them to take priority in any queue. Latency-insensitive applications can tolerate longer waiting time for their operations.</li>
<li>Request Hedging: This is a simple solution to reduce the impact of variability by forwarding the same request to multiple replicas and using the response that arrives first. Of course, this can double or triple the amount of resources required. To reduce the number of hedged requests, the second request may only be sent if the first response has been pending for greater than the 95th-percentile of the expected latency for that request. This causes the extra load to be only about 5%, but reduces the latency tail significantly (in the typical case shown in Fig 2.35, where the 95th-percentile latency is much lower than the 99th-percentile latency).</li>
<li>Speculative execution and selective replication: Tasks on nodes that are particularly busy can be speculatively launched on other underutilized leaf nodes. This is especially effective if a failure in a particular node causes it to be particularly overloaded.</li>
<li>UX-based solutions: Finally, the delay can be intelligently hidden from the user by having a well designed user interface which reduces the sensation of delay experienced by a human user. Techniques to do this may include the use of animations, showing early results or engaging the user by sending relevant messages.</li>
</ol>
<p>Using these techniques, it is possible to significantly improve the experience of the end-users of a cloud application to the peculiar problem of a long tail.</p>
<p><img src="/images/14545544252879.jpg" alt=""></p>
<p><strong>References</strong></p>
<ol>
<li>Li, J., Sharma, N. K., Ports, D. R., &amp; Gribble, S. D. (2014). “Tales of the Tail: Hardware, OS, and Application-Level Sources of Tail Latency..” Proceedings of the ACM Symposium on Cloud Computing. ACM.</li>
<li>Wu, Haitao and Feng, Zhenqian and Guo, Chuanxiong and Zhang, Yongguang (2013). “ICTCP: Incast Congestion Control for TCP in Data-Center Networks.” IEEE/ACM Transactions on Networking (TON). IEEE Press.</li>
<li>Xu, Yunjing and Musgrave, Zachary and Noble, Brian and Bailey, Michael (2013). “Bobtail: Avoiding Long Tails in the Cloud.” 10th USENIX Conference on Networked Systems Design and Implementation. USENIX Association.</li>
<li>Dean, Jeffrey and Barroso, Luiz Andr{\’e} (2013). “The tail at scale.” Communications of the ACM. ACM.</li>
<li>Tene, Gil (2014). “Understanding Latency - Some Key Lessons and Tools.” QCon London.</li>
</ol>
<h3 id="Economics_for_Cloud_Applications"><a href="#Economics_for_Cloud_Applications" class="headerlink" title="Economics for Cloud Applications"></a>Economics for Cloud Applications</h3><p>CSPs are taking great pains to attract users from their traditional deployments. Public IaaS cloud prices have been steadily and steeply falling ever since the launch of their services. On average, for most major CSPs, prices have fallen by 20-30% per year since 2013.</p>
<p><img src="/images/14545544739052.jpg" alt="Figure 2.37: Average cost reduction of CSP Services"></p>
<p>However, despite these decreasing prices, cloud adoption still must be done with care. To truly gain the cost benefits of the cloud, it is important to understand, budget, plan, monitor and carefully analyze your usage. Also, it is difficult to choose between CSPs for individual use cases, since there is no standard way for CSPs to package resources, nor do they always follow the same pricing models.</p>
<p><strong>Pricing Models</strong></p>
<p>Cloud providers generally charge for resources based on one of the following 3 types of parameters:</p>
<ol>
<li>Time Based: Resources are charged based on the amount of time they are provisioned to the user. For example, you pay a certain amount per hour/day/month/year to have a virtual machine running on an IaaS cloud. The granularity of the charging period varies from cloud provider to cloud provider. Amazon, for example charges users per hour on a non-prorated basis.</li>
<li>Capacity Based: Users are charged based on the amount of a particular resource that is utilized or consumed. This is a popular charging model for cloud storage systems. For example, users are charged a certain amount for storing a gigabyte on cloud object storage systems such as S3 and Azure Blobs.</li>
<li>Performance Based: In many cloud providers, users can select a higher performance level for resources by paying a higher rate. For virtual machines, larger, more powerful machines with more CPU, Memory and Disk capacity can be provisioned at a higher hourly rate.</li>
</ol>
<p>Based on these charging parameters, CSPs, such as AWS, use one of the following common pricing models:</p>
<ol>
<li>On-demand / pay-as-you-go pricing: This is generally the most expensive pricing model for long-term usage. Payments are made for a very short period of usage (generally metered in minutes or hours). The advantages are that there is no need for a long-term contract, making it very flexible to scale in and out based on the current need. Although not common, it is possible for CSPs to increase costs during high demand and decrease it during low demand. This is a great model for service providers, as well as for cloud users who are just beginning to use the cloud.</li>
<li>Reserved instances / Subscription-based pricing: Instead of paying an hourly or per-minute rate, a user can choose to pre-pay and reserve a resource for a fairly long period of time (weeks or months). This often leads to significantly high markdowns (20-50%) but requires a long-term commitment. Within reserved pricing models, the payment schemes can vary from prepaid to contractually obliged periodic payments.</li>
<li>Spot pricing: Spot pricing is a way for CSPs to deal with excess unutilized capacity by offering it for sale at significantly lower prices than on-demand resources. For example,AWS often offers markdowns of 80-90% on spot EC2 resources. The prices are determined by a user auction, where users bid the maximum amount that they are willing to pay for a resource. The biggest downside is that the resources can often be terminated at any time if the spot price increases beyond the actual bid price. Spot resources are ideal for short-running, non-critical jobs that can be executed speculatively.</li>
</ol>
<p>Generally reserved instances should be used to meet the base requirements of the system. If an application needs 2 instances 80% of the time, 3 instances 15% of the time and 4 instances 5% of the time, you would generally reserve 2 instances for the lifetime of the application and scale out either using on-demand or spot instances. As mentioned, on-demand instances should only be chosen while scaling out if the application is business critical or if the differential between the on-demand price and the current spot price is offset by the risk of sudden termination. This is often a business-case specific decision.</p>
<p><strong>How to Optimize Cost Utilization</strong></p>
<p>To use the cloud in a cost-effective manner, enterprises must develop a mature process for choosing the resources to deploy, monitor and visualize usage, as well as a clear mechanism to identify waste and optimize utilization.</p>
<p><img src="/images/14545545777962.jpg" alt="Figure 2.38: Cost Optimization Process"></p>
<p>Before considering cost requirements, an organization must plan the amount of work that it is capable of completing in a given period based on fixed resources like the amount of staff, while dealing with physical constraints like inventory management, overhead due to transportation, material handling etc. The provisioning of IT resources must be designed to meet or exceed the physical capacity of the organization. This is extremely important, because the elasticity provided by the cloud tempts development teams to simply add resources as needed, without considering the cost implications of their decisions.</p>
<p>The first step when attempting to reduce expenditure on the cloud is to match resource types with the actual requirement for the application. This may mean selecting between EC2 VMs with different memory configurations, or number of cores. There is no simple way to do this, apart from testing and benchmarking the application across different resource types.</p>
<p>Even if an application performs better on a more expensive resource class, it is important to verify if the performance improvement is proportional to the cost increase. For example, if there is a 1.2x improvement in an application using a VM that is 7.5x more expensive than the base, it might make more sense to horizontally scale out the base resource to improve the performance.</p>
<p>It is important to build a monitoring and visualization system to monitor the various resources being used. The monitoring system must be designed to trigger scaling events in response to observed patterns of overload or idleness. Often, infrastructure teams choose to scale up or out aggressively, while they scale down or in more conservatively. Though this approach is more expensive in terms of resource provisioning, it theoretically provides a higher Quality of Service than operating near peak capacity all of the time.</p>
<p>That being said, organizations often underestimate the need to scale down and terminate rarely used resources. When planning to run different components of an application, it is important to categorize the utilization into different bins, based on the approximate duration for which it will be used. For instance, any jobs that are run for a short time on a nightly or weekly basis should not use resources 24/7. Idle resources should also be flagged and terminated (based on certain rules) by the monitoring system.</p>
<p>An important technique that ties into cost optimization is that of tagging resources. Tagging is the process of assigning labels to resources that allow them to be identified by monitoring and analytics tools. Tagging also enables custom rules to be defined per-tag, including access control lists to resources, billing alerts and specific scaling policies. Commonly used tags specify the owner (user or group) of a particular resource, the environment to which it belongs (e.g. production, backup, staging, testing), the cost center in charge of paying the bill, etc. When analyzing expenditure, this enables the generation of grouped views based on particular applications, as well as on specific development or testing teams.</p>
<p><strong>Case Study: Netflix’s Ice System</strong></p>
<p>As one of the largest AWS customers in the world, it is crucial for Netflix to have clear visibility about their expenditure. To support this requirement, they use an internally designed tool known as Ice. Ice relies on tags and resource metadata to build a dynamic dashboard that allows resources to be grouped by user, team, region, type, pricing model or any custom tag. It also supports the amortization of one time costs such as reserved instances over the lifetime of the resource. All of the data is periodically generated using AWS Billing APIs.</p>
<p><img src="/images/14545546158885.jpg" alt="Figure 2.39: Netflix Ice"></p>
<p>This tool has been released as a part of their Open Source initiative. Many companies (as well as CMU’s cloud computing course staff) use Ice or variants to gain insights into AWS usage and expenditure. It helps influence if any future resources should be reserved for the long-term (at cheaper prices), and identifies users or teams who overspend. All large cloud deployments should follow a similar process of planning &amp; budgeting, monitoring &amp; visualization, and forecasting &amp; optimization.</p>
<h3 id="Programming_the_Cloud_Summary"><a href="#Programming_the_Cloud_Summary" class="headerlink" title="Programming the Cloud Summary"></a>Programming the Cloud Summary</h3><ul>
<li>Cloud applications must take precautions to ensure that they use resources that help them meet their bandwidth and latency requirements, as well as follow security best practices.</li>
<li>Applications deployed on the cloud are often subject to performance variance due to the shared nature of the cloud.</li>
<li>The cloud makes it easy to maintain several different environments apart from production. Applicaton pipelines are maintained using code repository and version control systems and automated using Continuous Integration tools.</li>
<li>Planning for failure is crucial. Redundancy is the key technique used to ensure resilience- often ensure using replicas deployed across availability zones and regions.</li>
<li>Redundant resources are generally monitored and accessed using a central Highly Available load balancer. High Availability is ensure by switching over to a backup instance when one fails.</li>
<li>Companies like Netflix and Facebook inject large random (or planned) failures in their data centers and cloud operations to test for fault tolerance.</li>
<li>Load Balancing also supports horizontal scaling, whereby more identical resources can be thrown at a problem. The other type of scaling is vertical- where the size or capacity of existing resources is increased.</li>
<li>Horizontal scaling across too many nodes leads to the problem of Tail Latency, where the performance of the application is determined by it’s slowest component. This is both due to variability of performance on the cloud and also because applications with a large fan-out trigger bursts of activity at each stage.</li>
<li>Finally, the lack of standardization and high competitiveness of the cloud market leads to interesting opportunties and challenges to minimize costs.</li>
</ul>
<hr>
<h1 id="Virtualizing_Resources_for_the_Cloud"><a href="#Virtualizing_Resources_for_the_Cloud" class="headerlink" title="Virtualizing Resources for the Cloud"></a>Virtualizing Resources for the Cloud</h1><h2 id="Introduction_and_Motivation"><a href="#Introduction_and_Motivation" class="headerlink" title="Introduction and Motivation"></a>Introduction and Motivation</h2><p>In the last unit, we delved into depth about data centers, the components as well as design decisions for data centers. To build a cloud though, creating a data center is not enough. We need some mechanism to manage and share data center resources over multiple clients (or tenants). This sharing of resources must be done in a safe, efficient and isolated manner. This is where resource sharing technologies come in.</p>
<p>In this unit, we will go into some depth about various resource sharing techniques, but we will focus on the big one: Virtualization. Recall in Unit 1, you learnt about the evolution of cloud computing and surprising fact that the ideas of utility computing date actually back to the 1960s. You learnt also the evolution and combination of multiple technologies led to the emergence of the cloud today. Virtualization is arguably one of the most important of those technologies, acting like a kind of glue that holds everything together.</p>
<p>You will find this unit to be more systems focused than the previous units, but a thorough understanding of virtualization will help those who intend on taking systems-focused roles in the industry and will help those with research inclinations in this space.</p>
<p>In this module of this unit, we will look at virtualization in general and understand some of the key motivations behind virtual machines, the limitations of general purpose operating systems that led to the emergence of virtualization, and a whirlwind tour of the interfaces and abstractions in a modern computer system. We will then look at the bigger picture of resource sharing and what it means to share a resource in time and space.</p>
<p>The material presented in this Unit should contain sufficient detail for the course, but interested readers can refer to this popular book on virtual machines [1] to be a fairly comprehensive reference for all virtualization-related concepts.</p>
<p><strong>References</strong></p>
<ol>
<li>JE Smith and Nair (2005). “Virtual Machines: Versatile Platforms For Systems and Processes.” Morgan Kaufmann.</li>
</ol>
<h3 id="Background_and_Motivation"><a href="#Background_and_Motivation" class="headerlink" title="Background and Motivation"></a>Background and Motivation</h3><p><strong>Virtualization</strong></p>
<p>Virtualization is at the core of the cloud computing paradigm. It lies on top of the cloud infrastructure (or the data center) whereby virtual resources (e.g., virtual CPUs, memories, disks, networks) are constructed from the underlying physical resources and act as proxies to them. As is the case with the idea of cloud computing, which was first introduced in the 1960s, virtualization can be traced back to the 1970s. Forty years ago, the mainframe computer systems were extremely large and expensive. To address expanding user needs and costly machine ownerships, the IBM 370 architecture, announced in 1970, offered complete virtual machines (VMs) (virtual hardware images) to different programs running on the same computer hardware. Over time, computer hardware became less expensive, and users started migrating to low-priced desktop machines. This migration drove the adoption of the virtualization technology to fade for a period of time. Today, virtualization is enjoying a resurgence in popularity, with a number of research projects and commercial systems providing virtualization solutions for commodity PCs, servers, and the cloud. The following video (Video 3.1) is a brief introduction to virtualization.</p>
<p><a href="http://youtu.be/x2rzUi9fIHU" target="_blank" rel="external">Video 3.1: Introduction to Virtualization.</a></p>
<p>In this unit, we study various ingredients of the virtualization technology and the crucial role it plays in enabling cloud computing. First, we identify major reasons for why virtualization is becoming essential, especially for the cloud. Second, we learn how multiple hardware and software images can run side-by-side on a single resource while provided with security, resource, and failure isolation, which are critical requirements for cloud computing. Prior to delving into more details about virtualization, we present a brief background requisite for understanding how physical resources can be virtualized. In particular, we indicate how system complexity can be managed in terms of levels of abstractions and well-defined interfaces. Next, we describe resource sharing, in space and in time, as a general technique for improving system utilization. To this end, we formally define virtualization and examine two main VM types: process and system VMs.</p>
<p>After introducing and motivating virtualization for the cloud, we describe in detail CPU, memory, and I/O virtualization. Specifically, we first identify conditions for virtualizing CPUs, recognize the difference between virtualization and paravirtualization, explain emulation as a major technique for CPU virtualization, and examine Xen’s virtual CPU scheduling. Xen is a popular virtualization platform that Amazon utilizes in its AWS public cloud. Next, we outline the difference between conventional operating system’s virtual memory and system memory virtualization, explain the multiple levels of page mapping as imposed by memory virtualization, define memory overcommitment, and illustrate memory ballooning, a reclamation technique for memory overcommitment in VMware, which is another common virtualization platform. Finally, we explain how CPU and I/O devices can communicate with and without virtualization, identify the three main interfaces (system call, device driver, and operation level interfaces at which I/O virtualization can be applied), and study I/O virtualization pertaining to Xen. We close with two main case studies, Amazon EC2, which applies virtualization to provide IaaS, and Google App Engine, which also applies virtualization to deliver PaaS.</p>
<p><strong>References</strong></p>
<ol>
<li>Barham et al. (2003). “Xen and The Art of Virtualization.” In Proceedings of the nineteenth ACM symposium on operating systems principles (SOSP ‘03). ACM, New York, NY, USA.</li>
<li>Michelle Bailey (2009). “The Economics of Virtualization: Moving Toward an Application-Based Cost Model.” VMware Sponsored Whitepaper.</li>
<li>Chen and Noble (2001). “When Virtual Is Better Than Real.” IEEE Computer Society, Washington, DC, USA.</li>
<li>JE Smith and Nair (2005). “Virtual Machines: Versatile Platforms For Systems and Processes.” Morgan Kaufmann.</li>
<li>A. Beloglazov and R. Buyya (2010). “Energy Efficient Allocation of Virtual Machines in Cloud Data Centers.” CCGrid.</li>
<li>A. Beloglazov, J. Abawajy, and R. Buyya (2012). “Energy-Aware Resource Allocation Heuristics for Efficient Management of Data Centers for Cloud Computing.” Future Generation Computer Systems.</li>
<li>Y. Jin, Y. Wen, and Q. Chen (2012). “Energy Efficiency and Server Virtualization in Data Centers: An Empirical Investigation.” Computer Communications Workshops (INFOCOM WKSHPS).</li>
<li>Silicon Valley Leadership Group (2008). “Accenture, Data Centre Energy Forecast Report.” Technical Report.</li>
</ol>
<h3 id="Virtualization_and_Cloud_Computing"><a href="#Virtualization_and_Cloud_Computing" class="headerlink" title="Virtualization and Cloud Computing"></a>Virtualization and Cloud Computing</h3><p><strong>Why Virtualization?</strong></p>
<p>Virtualization is predominantly used by programmers to ease software development and testing. It is used by IT data centers to consolidate dedicated servers into more cost effective hardware and by the cloud (e.g., Amazon EC2) to isolate users sharing a single hardware layer and offer elasticity, among others. Next, we discuss six areas that virtualization enables on the cloud.</p>
<p><img src="/images/14551090208318.jpg" alt="Figure 3.1: Provisioning a VM on a physical system."></p>
<p>1.Enabling the cloud computing system model: A major use case of virtualization is cloud computing. As discussed in Unit 1, cloud computing adopts a model whereby software, computation, and storage are offered as services. These services range from arbitrary applications (called software as a service, or SaaS) such as Google Apps through platforms (called platform as a service, or PaaS) such as Google App Engine to physical infrastructures (called infrastructure as a service, or IaaS) such as Amazon EC2. For example, IaaS allows cloud users to provision VMs for their own use. As shown in Figure 3.1, provisioning a VM entails obtaining virtual versions of every physical machine component, including CPU, memory, I/O, and storage. Virtualization makes this possible via a virtualization intermediary called the hypervisor or the virtual machine monitor (VMM). Examples of leading hypervisors are Xen [1] and VMware. Amazon EC2 uses Xen for provisioning VMs to users.</p>
<p>2.Elasticity: A major property of the cloud is elasticity, or the ability to respond quickly to user demands by including or excluding resources for SaaS, PaaS, and/or IaaS, either manually or automatically. As shown in Figure 3.1, virtualization enhances elasticity by allowing providers/users to expand or contract services on the cloud. For instance, Google App Engine automatically expands servers during demand spikes and contracts them during demand lulls. On the other hand, Amazon EC2 allows users to expand and contract their own virtual clusters either manually (by default) or automatically (by using Amazon Auto Scaling). In short, virtualization is the key technology to attain elasticity on the cloud.</p>
<p>3.Resource sandboxing: A system VM provides a sandbox that can isolate one environment from others, ensuring a level of security that may not be applicable with conventional operating systems (OSs). First, a user running an application on a private machine might be reluctant to move her or his applications to the cloud unless guarantees are provided that her or his applications and activities cannot be accessed and monitored by any other user on the cloud. Virtualization can serve in offering a safe environment for every user by making it impossible for one user to observe or alter another’s data and/or activity. Second, because the cloud can also execute user applications concurrently, a software failure of one application cannot generally propagate to others, if all are running on different VMs. Such a property usually is called fault containment. Clearly, this protection increases the robustness of the system. In a nonvirtualized environment, however, erratic behavior of one application can bring down the whole system.</p>
<p><img src="/images/14551113294948.jpg" alt="Figure 3.2: Using virtual sandboxes to develop defenses against attacks and to monitor incoming data."></p>
<p>Sandboxing, as provided by virtualization, opens up interesting possibilities as well. As illustrated in Figure 3.2, a specific VM can be used as a sandbox whereby security attacks (e.g., denial-of-service attacks and inserting a malicious packet into a legitimate IP communication stream) can be safely permitted and monitored. This can allow the inspecting of the effects of such attacks, gathering information on their specific behaviors, and replaying them if necessary to design a defense against future attacks (by learning how to detect and quarantine them before they can cause any harm). Furthermore, suspicious network packets or input can be sent to a clone (a specific VM) before it is forwarded to the intended VM to preclude any potential ill effect. A VM can be thrown away after it has served its purpose.</p>
<p>4.Improved system utilization and reduced costs and energy consumption: It was observed very early that computer hardware resources are typically underutilized. The concept of resource sharing has been successfully applied in multiprogramming OSs to improve system utilization. Resource sharing in multiprogramming OSs, however, provides only process abstractions (not systems) that can access resources in parallel. Virtualization takes this step forward by creating an illusion of complete systems whereby multiple VMs can be supported simultaneously, each running its own system image (e.g., OS) and associated applications. For instance, in virtualized data centers (e.g., Amazon EC2), seven or more VMs can be provisioned on a single server, providing resource utilization of approximately 60% to 80%. [2] In contrast, approximately only 5% to 10% average resource utilization is accomplished in nonvirtualized data centers. [2] By enabling multiple VMs on a single physical server, virtualization allows consolidating physical servers into virtual servers that run on many fewer physical servers (a concept called server consolidation). Clearly, this consolidation can lead not only to improved system utilization but to reduced costs.</p>
<p>Server consolidation as provided by virtualization leads not only to improved system utilization and reduced costs but to optimized energy consumption in cloud data centers. Data centers hosting cloud applications consume tremendous amounts of energy, resulting in high operational costs and carbon dioxide emissions. [5] Server consolidation is considered an effective way to improve the energy efficiency of data centers by consolidating applications running on multiple physical servers into fewer virtual servers. Idle physical servers can subsequently be switched off to decrease energy consumption. Studies show that server consolidation can save up to 20% of data center energy consumption. [7] [8] A large body of research work illustrates the promise of virtualization in reducing energy consumption in cloud data centers. [5] [6] [7] Indeed, mitigating the explosive energy consumption of cloud data centers is currently deemed as one of the key challenges in cloud computing.</p>
<p>5.Mixed-OS environment: As shown in Figure 3.3 and pointed out in the previous subsection, a single hardware platform can support multiple OSs simultaneously. This provides great flexibility for users in which they can install their own OSs, libraries, and applications. For instance, a user can install one OS for office productivity tools and another OS for application development and testing, all on a single desktop computer or on the cloud (e.g., Amazon EC2).</p>
<p><img src="/images/14551114141438.jpg" alt="Figure 3.3: Mixed-OS environment offered by system virtualization."></p>
<p>6.Facilitating research: Running an OS on a VM allows the hypervisor to instrument accesses to hardware resources and count specific event types (e.g., page faults) or even log detailed information about events’ nature, events’ origins, and how operations are satisfied. Moreover, traces of executions and dumps of machine states at points of interests can be taken at the VM level, an action that cannot be performed on native systems. Last, system execution can be replayed on VMs from some saved state for analyzing system behavior under various scenarios. Indeed, the complete state of a VM can be saved, cloned, encrypted, moved, and/or restored; actions that are not so easy to perform with physical machines. [3] As such, it has become common for OS researchers to conduct most of their experiments using VMs rather than native hardware platforms. [4]</p>
<p><strong>References</strong></p>
<ol>
<li>Barham et al. (2003). “Xen and The Art of Virtualization.” In Proceedings of the nineteenth ACM symposium on operating systems principles (SOSP ‘03). ACM, New York, NY, USA.</li>
<li>Michelle Bailey (2009). “The Economics of Virtualization: Moving Toward an Application-Based Cost Model.” VMware Sponsored Whitepaper.</li>
<li>Chen and Noble (2001). “When Virtual Is Better Than Real.” IEEE Computer Society, Washington, DC, USA.</li>
<li>JE Smith and Nair (2005). “Virtual Machines: Versatile Platforms For Systems and Processes.” Morgan Kaufmann.</li>
<li>A. Beloglazov and R. Buyya (2010). “Energy Efficient Allocation of Virtual Machines in Cloud Data Centers.” CCGrid.</li>
<li>A. Beloglazov, J. Abawajy, and R. Buyya (2012). “Energy-Aware Resource Allocation Heuristics for Efficient Management of Data Centers for Cloud Computing.” Future Generation Computer Systems.</li>
<li>Y. Jin, Y. Wen, and Q. Chen (2012). “Energy Efficiency and Server Virtualization in Data Centers: An Empirical Investigation.” Computer Communications Workshops (INFOCOM WKSHPS).</li>
<li>Silicon Valley Leadership Group (2008). “Accenture, Data Centre Energy Forecast Report.” Technical Report.</li>
</ol>
<h3 id="Limitations_of_General-Purpose_Operating_Systems"><a href="#Limitations_of_General-Purpose_Operating_Systems" class="headerlink" title="Limitations of General-Purpose Operating Systems"></a>Limitations of General-Purpose Operating Systems</h3><p>The operating system (OS) binds all hardware resources to a single entity, which limits the flexibility of the system, not only in terms of applications that can run concurrently and share resources, but also in terms of isolation. Isolation is crucial in cloud computing, with many users sharing cloud infrastructure. A system is considered to provide full isolation when it supports a combination of fault isolation, resource isolation, and security isolation. [1] Fault isolation reflects the ability to limit a buggy program from affecting another program. Complete fault isolation requires no sharing of code or data. Resource isolation corresponds to the ability of enforcing/controlling resource usages of programs. This requires careful allocation and scheduling of resources. Last, security isolation refers to the extent at which accesses to logical objects or information (e.g., files, memory addresses, port numbers) are limited. Security isolation promotes safety by which one application cannot reveal information (e.g., names of files or process IDs) to any other application. General-purpose OSs provide a weak form of isolation (only the process abstraction), not full isolation.</p>
<p>On the other hand, virtualization relaxes physical constraints and enables optimized system flexibility and isolation. Hypervisors allow running multiple OSs side by side yet provide full isolation (i.e., security, resource, and failure isolations). To mention a few, first, hypervisors can effectively authorize multiplex accesses to physical resources. Second, undesired interactions between VMs are sometimes called cross-talk, and hypervisors can incorporate sophisticated resource schedulers and allocators to circumvent cross-talk. Third, hypervisors offer no sharing among OS distributions. The only code and data shared among VMs is indeed the hypervisor itself. One exception is page-sharing mechanisms. Performance of VMs can be improved by sharing of memory pages between VMs in hypervisors such as VMware. However, this is done in a transparent, isolated fashion and only with data that is binary identical. This topic is covered in detail in the “Memory Virtualization” module.</p>
<p>Nonetheless, the unique benefits offered by virtualization have some side effects. For instance, the degree of isolation comes at the cost of efficiency. Efficiency can be measured in terms of overall execution time. In general, VMs provide inferior performance as opposed to equivalent physical machines. This is mainly due to: (1) the overhead of context switching between VMs and the hypervisor and (2) the duplication of efforts by the hypervisor and the OSs running in VMs (e.g., all might be running schedulers, managing virtual memories, and interpreting I/O requests).</p>
<p>Figure 3.4 demonstrates approximate logical locations of the two leading examples in virtualization, Xen and VMware ESX (more on these later in the module), vis-à-vis some traditional OSs along the efficiency and isolation dimensions. The x axis indicates the kind of isolation supported by the shown hypervisors and OSs, and the y axis exhibits qualitative efficiency. A basic observation is that, to date (March 2013), there is no VM technology that has reached the ideal case of maximizing both efficiency and isolation.</p>
<p><img src="/images/14551115218604.jpg" alt="Figure 3.4: Traditional OSs and popular hypervisors along the efficiency and isolation dimensions."></p>
<p><strong>References</strong></p>
<ol>
<li>Soltesz et al. (2007). “Container-Based Operating System Virtualization: A Scalable, High-Performance Alternative to Hypervisors.” In ACM SIGOPS Operating Systems Review (Vol. 41, No. 3, pp. 275-287), ACM.</li>
</ol>
<h3 id="Managing_System_Complexity__3A_Abstractions"><a href="#Managing_System_Complexity__3A_Abstractions" class="headerlink" title="Managing System Complexity : Abstractions"></a>Managing System Complexity : Abstractions</h3><p>Modern computers are among the most advanced human-engineered structures. These structures are typically very complex. Such complexity stems from incorporating various silicon chips, such as processors, memories, disks, displays, keyboards, mice, network interfaces, on which programs are operated and services are offered. The key to managing complexity in computer systems is dividing system components into levels of abstractions separated by well-defined interfaces.</p>
<p><strong>Levels of Abstractions</strong></p>
<p>The first aspect of managing computer complexity is by abstracting computer components. For instance, gates are built on electronic circuits, binary on gates, machine languages on binary, programming languages on machine languages, and operating systems along with associated applications on programming languages. Another abstraction that almost every computer user understands and uses is the file. Files are abstractions of disks. As demonstrated in Figure 3.5, the details of a hard disk are abstracted by the OS so that disk storage appears to applications as a set of variable-size files. Consequently, programmers need not worry about locations and sizes of cylinders, sectors, and tracks or bandwidth allocations at disk controllers. They can simply create, read, and write files without knowledge of the way the hard disk is constructed or organized.</p>
<p><img src="/images/14551116022300.jpg" alt="Figure 3.5: Files are abstractions of disks."></p>
<p>Another common example of abstraction is the process. Processes are abstractions of CPUs and memories. Hence, programmers need not worry whether their processes will monopolize CPUs or consume full memory capacities. The OS creates and manages all users’ processes without any involvement from users. Abstractions for displays are also provided by drawing and windowing packages. Mouse clicks are abstracted as invocations to program functions. Keydown events at keyboards are abstracted as inputs of characters. Finally, abstractions for networks include layers/protocols, such as IP, UDP, and TCP. As shown in Figure 3.6, distributed systems (e.g., the cloud) build on network layers and involve extra layers, such as sockets, RMIs, and RPCs. In short, the ability of abstracting system components enables the simplified design, programming, and use of computer systems.</p>
<p><img src="/images/14551116203827.jpg" alt="Figure 3.6: Abstraction layers in distributed systems."></p>
<p>To this end, we note that abstractions can be applied at the hardware or software levels. At the hardware level, components are physical (e.g., CPU and RAM). Conversely, at the software level, components are logical (e.g., RMI and RPC). In this unit, we are most concerned with abstractions at the software or near the hardware/software levels.</p>
<h3 id="Managing_System_Complexity__3A_Interfaces"><a href="#Managing_System_Complexity__3A_Interfaces" class="headerlink" title="Managing System Complexity : Interfaces"></a>Managing System Complexity : Interfaces</h3><p><strong>Well-Defined Interfaces</strong></p>
<p>A system (or subsystem) interface is defined as a set of function calls that allow leveraging the underlying system’s functionalities without needing to know any of its details. The two most popular interfaces in systems are the Application Programming Interface (API) and the Instruction Set Architecture (ISA) interface. Another interface that is less popular, yet important (especially in virtualization), is the Application Binary Interface (ABI). The following video (Video 3.2) describes API, ABI, and ISA.</p>
<p><a href="http://youtu.be/BFvKY7GpWbM" target="_blank" rel="external">Video 3.2: Computer System Architecture and Interfaces</a></p>
<p>As explained in Video 3.1, an API is used by high-level language (HLL) programmers to invoke some library or OS features. An API includes data types, data structures, functions, and object classes, to mention a few. An API enables compliant applications to be ported easily (via recompilation) to any system that supports the same API. Although the API deals with software source codes, the ABI is a binary interface. The ABI is essentially a compiled version of the API. Hence, it lies at the machine language level. With ABI, system functionalities are accessed through OS system calls. OS system calls provide a specific set of operations that the OS can perform on behalf of user programs. A source code compiled to a specific ABI can run unchanged only on a system with the same OS and ISA. Finally, the ISA defines a set of storage resources (e.g., registers and memory) and a set of instructions that perform arithmetic, control program execution, and allow manipulating data held at storage resources. ISA lies at the boundary between hardware and software. As discussed later in the unit, ABI and ISA are important in defining virtual machine types.</p>
<p>Platform as a service (PaaS) is at a higher level of abstraction than infrastructure as a service (IaaS).</p>
<p>Developers can build Web application developments using either IaaS (as provided by Amazon EC2) or PaaS (as provided by Google App Engine).</p>
<p>PaaS places restrictions on the libraries, the programming languages, and customized software that can be used for Web application development.</p>
<p>IaaS allows users to install any software necessary for Web application development.</p>
<p>FaceID, a Web services company, wants to offer a Web authentication tool as a service. FaceID has a legacy system for developing such tools. It cannot afford to develop its system from scratch.</p>
<h3 id="Resource_Sharing_Basics"><a href="#Resource_Sharing_Basics" class="headerlink" title="Resource Sharing Basics"></a>Resource Sharing Basics</h3><p><strong>Effective Resource Sharing</strong></p>
<p>The underlying infrastructure of the cloud (or the data center) might consist of thousands of processors connected to terabytes of memory and petabytes of disk capacity. Often, there is a mismatch between the ideal number of processors a certain application requires and the actual number of physical processors available. It is more frequently the case that an application cannot exploit more than a fraction of the processors available due to two main limitations:</p>
<ul>
<li>Availability of parallelism in the application</li>
<li>Amenability of the application to scale well with more processors</li>
</ul>
<p>These limitations led to the examination of techniques that can help utilize hardware resources more effectively. Among these techniques is resource sharing.</p>
<p><strong>Resource Sharing in Space and in Time</strong></p>
<p>Resource sharing refers to multiplexing or partitioning system resources (e.g., CPUs, memory) among application processes. Resource sharing is not a new idea, and it has been applied traditionally to uniprocessor and multiprocessor systems via the operating system (OS). Typically, there are two ways to implement resource sharing, either in space or in time. Sharing in time (or timesharing) allows processes to take turns in using a resource component, while sharing in space enables each process to have exclusive access to a specific portion of a component. For instance, with a single CPU and multiple processes, the OS can allocate the CPU to each process for a certain time interval under the rule that only one process can run at a time on the CPU, which can be achieved by using a specific CPU scheduling mechanism. The part of the OS that performs scheduling, or more precisely decides which process runs next on the CPU, is called the scheduler. The strategy that the scheduler uses for multiplexing between processes is called the scheduling algorithm. One popular strategy for scheduling processes on a CPU is the round-robin algorithm (see Fig. 3.7). With round-robin, each process is assigned a time interval (or a quantum) during which it is allowed to execute. The OS can maintain a queue of processes, as shown in Figure 3.7 (a). When a process is scheduled on a CPU, and it completes execution for its time quantum, it gets preempted and added to the tail of waiting processes. The process at the head of the queue is then granted to run on the CPU, as depicted in Figure 3.7 (b). This is also called context switching.</p>
<p><img src="/images/14551118044926.jpg" alt="Figure 3.7 Round-robin scheduling. (a) A queue of processes, with the process at the head, A, being currently scheduled at a CPU and the one next to it, B, being ready to get running once A is context switched. (b) The queue when A uses up its quantum, gets context switched, and B gets scheduled."></p>
<p>As mentioned previously, resource sharing can be achieved in space as well. A common example of sharing in space is the sharing of the main memory. The main memory usually is partitioned into multiple partitions, each allocated to a process. Thus, the data of all processes will be resident in the same memory at the same time. Sharing memory in space makes the system more efficient than allocating the whole memory to a single process. In particular, it allows each process to take turns on the CPU in a much faster way (because it is faster to load the process state from the main memory than from disk). Another common example of sharing in space is the sharing of disk storage in which a disk can hold files from multiple users at the same time. Partitioning memory and disk spaces as well as keeping track of who is using which memory portion and/or which disk blocks are typical OS tasks. The OS abstracts system components, so resource sharing is eased. For instance, rather than having to worry about sharing the tracks, sectors, cylinders, and bandwidth of a disk drive, we simply deal with files.</p>
<p><strong>References</strong></p>
<ol>
<li>Raj Vaswani and John Zahorjan (1991). “The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed, Shared Memory Multiprocessors.” SIGOPS Oper. Syst. Rev. 25, 5.</li>
</ol>
<h3 id="Resource_Sharing_in_Multiprocessor_Systems"><a href="#Resource_Sharing_in_Multiprocessor_Systems" class="headerlink" title="Resource Sharing in Multiprocessor Systems"></a>Resource Sharing in Multiprocessor Systems</h3><p><strong>Sharing in Multiprocessor Systems</strong></p>
<p>What we have discussed so far has to do with sharing in space and in time of a single system component (e.g., CPU, memory, and disk). In contrary, sharing a whole system is accomplished via sharing all of its components. In principle, this can be applied to uniprocessor and multiprocessor systems, with the caveat that sharing a multiprocessor system is always more involved. In this section, we focus mainly on sharing multiprocessor systems. Figure 3.8 depicts a multiprocessor system partitioned in space into three partitions. Each partition is assigned resources that are physically distinct from the resources used by the other partitions. Clearly, sharing in space allows a partition to own its resources physically. Hence, the number of partitions that can be supported in a multiprocessor system shared in space is limited to the number of available physical processors. In addition, it is often the case that each of the physical partitions is underutilized. Thus, sharing multiprocessor systems in space usually is a suboptimal solution for system utilization. Nonetheless, sharing multiprocessor systems in space has its own advantages. First, it protects each partition from the possibility of intentional or unintentional denial-of-service attacks by other partitions. It does not require sophisticated algorithms for scheduling and management of resources.</p>
<p><img src="/images/14551118912067.jpg" alt="Figure 3.8: Sharing in space of a multiprocessor system."></p>
<p>As compared to sharing in space, sharing multiprocessor systems in time makes it possible to partition an n-way system into a system with more than n partitions (i.e., we are no longer limited by the number of available physical processors). Figure 3.9 demonstrates a multiprocessor system with physical resources being shared in a time-multiplexed manner. Clearly, sharing in time allows better system utilization because it is permissible for two partitions to share the resources of a single system board. This, however, comes at the cost of requiring mechanisms to provide safe and efficient ways of sharing resources.</p>
<p><img src="/images/14551119093032.jpg" alt="Figure 3.9: Sharing in time of a multiprocessor system."></p>
<p>As pointed out previously, sharing resources in multiprocessor systems is more involved than in uniprocessor systems. For instance, in a uniprocessor system, scheduling is one dimensional. In particular, after the scheduler defines a specific quantum, it mainly decides which process should go next. In a multiprocessor system, however, after defining the quantum, the scheduler has to decide which process should go next and on which CPU. This latter extra dimension introduces some complexity, as illustrated in Figure 3.10. The figure reveals a way to deal with scheduling on multiprocessors in which a single, systemwide data structure (e.g., a list or a queue) is adopted. In Figure 3.10 (a), the four CPUs are busy, and multiple processes are waiting on a shared queue (i.e., seen and accessed by all CPUs), assuming a round-robin algorithm. In Figure 3.10 (b), CPU2 finishes its work and polls for the shared queue. When CPU2 gets access to the queue, it locks it and selects the next ready process as dictated by the round-robin algorithm. Locking the shared queue is done to avoid inconsistent accesses by multiple, idle CPUs.</p>
<p>Scheduling, as demonstrated by Figure 3.10, is easy to implement and provides some efficiency with respect to automatic load balancing. Specifically, load balancing is automatically offered because one CPU will never be idle while others are overloaded. In contrast, such a scheduling strategy exhibits a critical deficiency with regards to scalability. Particularly, as the number of CPUs is increased, contention on the queue as well as context switching overhead will increase.</p>
<p>Another complexity that has to do with scheduling on multiprocessor systems is the potential inefficiency of exploiting caching. Specifically, after process A in Figure 3.10 uses its quantum at CPU2, it gets context switched and later resumed (when its turn comes again), probably at a different CPU. When A has executed on CPU2, it is likely that the cache of CPU2 became full of A’s cache blocks. Consequently, when A is resumed at a different CPU, it will suffer from a high cache miss rate that will affect its overall performance. Some multiprocessor systems address this problem by applying what is called affinity scheduling. The basic idea of affinity scheduling is to attempt to schedule a process on a CPU that it used in its previous quantum. More on affinity scheduling can be found in Vaswani and Zahorjan’s “The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed, Shared Memory Multiprocessors.”</p>
<p><img src="/images/14551119310482.jpg" alt="Figure 3.10: Using a single, system-wide data structure (a queue in this case) to pursue CPU scheduling in a multiprocessor system. The queue is shared across all the CPUs and exemplifies a round-robin algorithm. (a) All CPUs are assumed to be busy. (b) CPU2 finishes its work and gets the next process, B, from the queue. CPU2 has to lock the queue due to being shared by all CPUs before getting B."></p>
<p>To this end, we note that resource sharing as described in this page (i.e., in the context of the OS) also applies to virtualization. Indeed, a core task of the hypervisor is to share/multiplex the underlying system components among various virtual machines. Similar to traditional OSs, the hypervisor can apply sharing strategies in space and in time. We discuss later in this unit how the hypervisor can pursue such sharing and provide examples of CPU scheduling algorithms from the Xen platform.</p>
<p><strong>References</strong></p>
<ol>
<li>Raj Vaswani and John Zahorjan (1991). “The Implications of Cache Affinity on Processor Scheduling for Multiprogrammed, Shared Memory Multiprocessors.” SIGOPS Oper. Syst. Rev. 25, 5.</li>
</ol>
<h3 id="Introduction_and_Motivation_Summary"><a href="#Introduction_and_Motivation_Summary" class="headerlink" title="Introduction and Motivation Summary"></a>Introduction and Motivation Summary</h3><ul>
<li>Virtualization is at the core of cloud computing. It allows the construction and provision of virtual hardware images (called virtual machines) from underlying physical machines.</li>
<li>Virtualization enables the cloud computing model by making it possible to offer a range of cloud services, including IaaS, PaaS, and SaaS.</li>
<li>Virtualization enhances the cloud elasticity by allowing providers/users to expand or contract services either manually or automatically.</li>
<li>Virtualization provides resource sandboxing by making it difficult for one cloud user to observe or alter another’s data and/or activity.</li>
<li>Virtualization provides fault containment by preventing a software failure at one virtual machine (VM) to propagate to another VM, even if both exist at the same physical machine (PM).</li>
<li>Virtualization provides server consolidation by allowing multiple VMs to run on a single PM, thereby improving system utilization and reducing costs.</li>
<li>Although virtualization allows running multiple VMs side by side, it provides full isolation (i.e., security, resource, and failure isolations).</li>
<li>Virtualized systems are complex. The key to managing complexity in virtualized systems is by dividing system components into levels of abstractions separated by well-defined interfaces.</li>
<li>In general, system abstractions hide details and ease software development and manageability (e.g., files abstract disks, whereby programmers can simply create, read, and write files without worrying about locations and sizes of cylinders, sectors, and tracks or bandwidth allocations at disk controllers).</li>
<li>Abstractions can be applied at the hardware and software levels.</li>
<li>An interface of a system component is defined as set of function calls that allows leveraging the component’s functionalities.</li>
<li>The three most common system interfaces are the Application Programming Interface (API), the Instruction Set Architecture (ISA) interface, and the Application Binary Interface (ABI).</li>
<li>ISA and ABI are of a special interest in virtualization, wherein they serve in defining VM types.</li>
<li>A core task in virtualization is to share (or multiplex) the underlying system components (e.g., CPUs, memory) among various VMs.</li>
<li>In general, resource sharing can be achieved in time (also called timesharing) and/or in space.</li>
<li>Sharing in time allows VMs to take turns in using a resource component (e.g., sharing a physical CPU among the virtual CPUs of a VM), while sharing in space enables each VM to have an exclusive access to a specific portion of a component (e.g., sharing a physical memory among VMs).</li>
<li>Timesharing and space sharing in multiprocessor systems are typically more involved than their counterparts in uniprocessor systems.</li>
<li>Timesharing improves system utilization but requires sophisticated scheduling and management mechanisms.</li>
<li>Space sharing is usually a sub-optimal solution for system utilization but a near-optimal solution for system complexity.</li>
</ul>
<h2 id="Virtualization"><a href="#Virtualization" class="headerlink" title="Virtualization"></a>Virtualization</h2><p>Now that we have covered the motivation and benefits of virtualization, in this module, we will cover the formal definition of virtualization.</p>
<p>You may have also noticed that there are different types of virtual machines. For example, Java programs run inside a sandbox environment known as a Java Virtual Machine (JVM). You may also have seen or used virtualization software on your desktop or laptop, such as VirtualBox or Parallels, which allow you to run multiple Operating systems on your computer simultaneously. Also, you should have provisioned and used virtualized instances on a cloud provider in the programming projects for this course. Each of these virtualization systems are different; you will learn about the various virtualization types in detail in this module.</p>
<h3 id="What_Is_Virtualization_3F"><a href="#What_Is_Virtualization_3F" class="headerlink" title="What Is Virtualization?"></a>What Is Virtualization?</h3><p><strong>Virtualization</strong></p>
<p>Formally, virtualization involves the construction of an isomorphism that maps a virtual guest system to a real host system. [1] Video 3.3 and Figure 3.11 illustrates the virtualization process.</p>
<p><a href="http://youtu.be/U6HwG9dc03A" target="_blank" rel="external">Video 3.3: Definition of Virtualization.</a></p>
<p>The function V in the figure maps guest state to host state. For a sequence of operations, e, that modifies a guest state, there is a corresponding sequence of operations, e’, in the host that performs equivalent modifications. Informally, virtualization creates virtual resources and maps them to physical resources. Virtual resources are created from physical resources and essentially act as proxies to them.</p>
<p><img src="/images/14551121054860.jpg" alt="Figure 3.11: Virtualization isomorphism."></p>
<p>The concept of virtualization can be applied to either a system component or an entire machine. Traditionally, virtualization has been applied to only the memory component in general-purpose operating systems (OSs), providing what is known as the virtual memory. In a revisit to the hard disk example in Figure 3.5, some applications might desire multiple hard drives. To satisfy such a requirement, the physical hard drive can be partitioned into multiple virtual disks, as shown in Figure 3.12. Each virtual disk is offered logical cylinders, sectors, and tracks. This keeps the level of detail analogous to what is offered by general-purpose OSs, yet at a different interface and without being abstracted. The hypervisor can map (the function V in the isomorphism) a virtual disk to a single, large file on the physical disk. Afterward, to carry a read/write operation on a virtual disk (the function e in the isomorphism), the hypervisor reflects the operation as a file read/write followed by an actual disk read/write (the function e’ in the isomorphism).</p>
<p><img src="/images/14551121252577.jpg" alt="Figure 3.12: Constructing a virtual disk by mapping its content to large files."></p>
<p>On the other hand, when virtualization is applied to an entire machine, it provides what is called a virtual machine (VM). Specifically, a full set of hardware resources, including processors, memory, and I/O devices, are virtualized to provide the VM. As shown in Figure 3.13, an underlying hardware machine usually is called a host, and an OS running on a VM is called a guest OS. A VM can run only at a single host at a time. As compared to a host, a VM can have resources different in quantity and in type. For instance, a VM can obtain more processors than what a host offers and can run an ISA that is different than that of the host. Last, every VM can be booted, shut down, and rebooted just like a regular host. Further details on VMs and their different types are provided on the next page.</p>
<p><img src="/images/14551121504766.jpg" alt="Figure 3.13: Virtualization as applied to an entire physical system. An OS running on a VM is called a guest OS, and every physical machine is called a host. Compared to a host, a VM can have virtual resources different in quantity and type."></p>
<p><strong>References</strong></p>
<ol>
<li>Popek, J., and Goldberg, R. (1974). “Formal Requirements for Virtualizable Third Generation Architectures.” Commun. ACM, Vol. 17, No. 7.</li>
</ol>
<h3 id="Virtual_Machine_Types"><a href="#Virtual_Machine_Types" class="headerlink" title="Virtual Machine Types"></a>Virtual Machine Types</h3><p>There are two main implementations of virtual machines (VMs): process VMs and system VMs. The following video (Video 3.4) covers the taxonomy of Virtual Machine Types:</p>
<p><a href="http://youtu.be/15DGinFJwFg" target="_blank" rel="external">Video 3.4: Virtual Machine Types.</a></p>
<p>We first cover process VMs and then system VMs.</p>
<p><strong>Process Virtual Machines</strong></p>
<p><img src="/images/14551122397465.jpg" alt="Figure 3.14: Process virtual machine (VM)."></p>
<p>A process VM is a VM capable of supporting an individual process as long as the process is alive. Figure 3.14 demonstrates process VMs. A process VM terminates when the hosted process ceases. From a process VM perspective, a machine consists of a virtual memory address space, user-level registers, and instructions assigned to a process to execute a user program. According to this definition, a process in a general-purpose OS can also be called a machine. However, a regular process in an OS can only support user program binaries compiled for the ISA of the host machine. That is, executing binaries compiled for an ISA different than that of the host machine is not supported by regular processes. Conversely, a process VM allows that to happen. Process VMs can support ISAs that differ from host ISAs via emulation. As shown in Figure 3.15, emulation is the process of allowing the interfaces and functionalities of one system (the source) to be implemented on a system with different interfaces and functionalities (the target). Emulation is discussed in detail later. The abstraction of the process VM is provided by a piece of virtualizing software called the runtime (see Fig. 3.14). The runtime is placed at the Application Binary Interface (ABI) on top of the host OS and the underlying hardware. It is this runtime that emulates the VM instructions and/or system calls when guest and host ISAs are different.</p>
<p><img src="/images/14551122593382.jpg" alt="Figure 3.15: The emulation process."></p>
<p>Finally, a process VM may not directly correspond to any physical platform; it is employed mainly to offer cross-platform portability. Such kinds of process VMs are called high-level language VMs (HLL VMs). An HLL VM abstracts away details of the underlying hardware resources and the OS and allows programs to run the same way on any platform. Java VM (JVM) and Microsoft Common Language Infrastructure (CLI) are examples of HLL VMs. In summary, a process VM is similar to a regular process running on an OS. However, a process VM allows, via emulation, the execution of an application compiled for an ISA different than that of the host machine.</p>
<p><strong>System Virtual Machines</strong></p>
<p>Contrary to process VMs, a system VM is a VM capable of virtualizing a full set of hardware resources, including processors, memories, and I/O devices, thus providing a complete system environment. A system VM can support an OS along with its associated processes as long as the system environment is alive. Figure 3.16 illustrates system VMs. As defined previously, the hypervisor (or the VM monitor [VMM]) is a piece of software that provides abstraction for the system VM. It can be placed at the ISA level directly on top of the raw hardware and below system images (e.g., OSs). The hardware resources of the host platform can be shared among multiple guest VMs. The hypervisor manages the allocation of and access to the hardware resources by the guest VMs. In practice, the hypervisor provides an elegant way to logically isolate multiple guest VMs sharing a single physical system. Each guest VM is given the illusion of acquiring all of the underlying hardware resources.</p>
<p><img src="/images/14551122850438.jpg" alt="Figure 3.16: System virtual machine (VM)."></p>
<p>There are different classes of system VMs. Figure 3.17 exhibits three of these classes as well as traditional systems. In a conventional time-shared system, the OS runs in a privileged mode (system mode), while the applications associated with it run in unprivileged mode (user mode) (more details on system privileges are discussed later). With system virtualization, however, the guest OS(s) might run in unprivileged mode, while the hypervisor operates in a privileged mode. Such a system is called a native system VM. In a native system VM, every privileged instruction issued by a user program at any guest OS has to trap to the hypervisor. In addition, the hypervisor needs to specify and implement every function required for managing hardware resources. In contrary, if the hypervisor operates in unprivileged mode on top of a host OS, the guest OS(s) will also operate in unprivileged mode. This system is called a user-mode hosted system VM. In this case, privileged instructions from guest OS(s) still need to trap to the hypervisor. In return, the hypervisor needs also to trap to the host OS. Clearly, this requirement increases the overhead by adding one more trap per every privileged instruction. Nonetheless, the hypervisor can utilize the functions already available on the host OS to manage hardware resources. Finally, the hypervisor can operate partly in privileged mode and partly in user mode in a system called a dual-mode hosted system VM. This way, the hypervisor can make use of the host OS’s resource-management functions and preclude the one more trap per each privileged instruction imposed in user-mode hosted system VMs.</p>
<p><img src="/images/14551123005959.jpg" alt="Figure 3.17: Different system VM classes."></p>
<h3 id="Virtualization_Summary"><a href="#Virtualization_Summary" class="headerlink" title="Virtualization Summary"></a>Virtualization Summary</h3><ul>
<li>Virtualization involves the construction of an isomorphism that maps a virtual guest system to a real (or physical) host system.</li>
<li>An underlying physical machine (PM) usually is called a host, and an OS running on a VM is called a guest OS.</li>
<li>As compared to a host PM, a VM can have resources different in quantity and in type (e.g., a host can contain one Intel IA-32 physical CPU, while a VM can include eight PowerPC virtual CPUs that all map to the single physical CPU).</li>
<li>A VM can run only at a single host at a certain point in time, yet can be migrated to a different host (and run at that host) at a different point in time.</li>
<li>There are two types of VMs, process VMs and system VMs.</li>
<li>A process VM (e.g., JVM) consists of a virtual memory address space, user-level registers, and instructions assigned to an OS process to execute a user program (i.e., no OS can run within a process VM).</li>
<li>Process VMs can support ISAs that differ from host ISAs.</li>
<li>The abstraction of a process VM is provided by a piece of virtualizing software denoted as the runtime.</li>
<li>The runtime of a process VM is placed at the ABI interface, on top of a host OS.</li>
<li>As opposed to process VMs, a system VM provides a complete system environment (i.e., an OS image can be run in a system VM).</li>
<li>System VMs can support ISAs that differ from host ISAs.</li>
<li>The abstraction of a system VM is provided by a piece of a virtualizing software called the hypervisor (or the virtual machine monitor [VMM]).</li>
<li>There are three main classes of system VMs, which are defined according to where in the system the hypervisor is placed.</li>
<li>A system VM is called a native system VM when its hypervisor is placed on bare metal (i.e., the raw hardware).</li>
<li>In native system VMs, the hypervisor is run in system mode, and the VMs (alongside their associated OSs) are run in user mode.</li>
<li>Hypervisors in native system VMs should specify and implement every function required for managing hardware resources.</li>
<li>With native system VMs, every privileged instruction issued by a user program at any guest OS has to trap to the hypervisor.</li>
<li>A system VM is called a user-mode hosted VM when its hypervisor is placed on top of a host OS.</li>
<li>In user-mode hosted VMs, the hypervisor and all its managed VMs run in user mode, while the underlying host OS runs in system mode.</li>
<li>With user-mode hosted VMs, privileged instructions from guest OS(s) need to trap to the hypervisor, but the hypervisor needs not implement every function required for managing hardware resources.</li>
<li>A system VM is called a dual-mode hosted VM when its hypervisor is placed partly on bare metal and partly on a host OS.</li>
<li>In dual-mode hosted VMs, the hypervisor can partly operate in system mode and partly in user mode, hence, using the best of native system VMs and user-mode hosted VMs.</li>
</ul>
<h2 id="Resource_Virtualization_-_CPU"><a href="#Resource_Virtualization_-_CPU" class="headerlink" title="Resource Virtualization - CPU"></a>Resource Virtualization - CPU</h2><p>A key element, which is also one of the more complex parts of virtualization, is CPU virtualization. Virtualizing a CPU entails two major steps:</p>
<ul>
<li>Multiplexing a physical CPU (pCPU) among virtual CPUs (vCPUs) associated with virtual machines (VMs); this is called vCPU scheduling.</li>
<li>Virtualizing the ISA of a pCPU in order to make vCPUs with different ISAs run on this pCPU.</li>
</ul>
<p>In this module, we present some conditions for virtualizing ISAs. Second, we describe ISA virtualization. Third, we make a distinction between two types of ISA virtualization: full virtualization and paravirtualization. Fourth, we discuss emulation, a major technique for virtualizing CPUs. Fifth, we compare two kinds of VMs: simultaneous multiprocessing (SMP) and uniprocessors (UP) VMs. Finally, we close with a discussion on vCPU scheduling. As examples, we present two popular Xen vCPU schedulers.</p>
<h3 id="The_Conditions_for_Virtualizing_ISAs"><a href="#The_Conditions_for_Virtualizing_ISAs" class="headerlink" title="The Conditions for Virtualizing ISAs"></a>The Conditions for Virtualizing ISAs</h3><p><strong>Virtualizing an ISA</strong></p>
<p>The key to virtualizing a CPU lies in the execution of both privileged and unprivileged instructions issued by guest virtual processors. The set of any processor instructions is documented and provided in the ISA. The following video (Video 3.5) outlines the issues regarding virtualizing an ISA:</p>
<p><a href="http://youtu.be/L55ut9i10O8" target="_blank" rel="external">Video 3.5: Virtualizing an ISA.</a></p>
<p>Special privileges to system resources are permitted by defining modes of operations (or rings) in the ISA. Each CPU ISA usually specifies two modes of operations, system (or supervisor/kernel/privileged) mode and user mode (see Fig. 3.18(a)). System mode allows a wide accessibility to system components, while user mode restricts such accessibility. In an attempt to provide security and resource isolations, OSs in traditional systems are executed in system mode, while associated applications are run in user mode. Some ISAs, however, support more than two rings. For instance, the Intel IA-32 ISA supports four rings (see Fig. 3.18(b)). In traditional systems, when Linux is implemented on an IA-32 ISA, the OS is executed in Ring 0 and application processes are executed in Ring 3.</p>
<p><img src="/images/14551131967470.jpg" alt="Figure 3.18: System modes of operations (or rings)."></p>
<p>A privileged instruction is defined as one that traps in user mode and does not trap in system mode. A trap is a transfer of control to system mode wherein the hypervisor (as in virtualization) or the OS (as in traditional OSs) performs some action before switching control back to the originating process. Traps occur as side effects of executing instructions. Overall, instructions can be classified into two different categories: sensitive and innocuous. Sensitive instructions can be either control sensitive or behavior sensitive. Control-sensitive instructions are those that attempt to modify the configuration of resources in a system, such as changing the mode of operation or CPU timer. An example of control-sensitive instructions is load processor status word (LPSW) (IBM System/370). LPSW loads the processor status word from a location in memory if the CPU is in system mode and, otherwise it traps. LPSW contains bits that determine the state of the CPU. For instance, one of these bits is the P bit, which specifies whether the CPU is in user mode or system mode. If executing this instruction is allowed in user mode, a malicious program can easily change the mode of operation to privileged and obtain control over the system. Hence, to protect the system, such an instruction can only be executed in system mode. Behavior-sensitive instructions are those whose behaviors are determined by the current configuration of resources in a system. An example of behavior-sensitive instructions is Pop Stack into Flags Register (POPF) (Intel IA-32). POPF pops the flag registers from a stack held in memory. One of these flags, called the interrupt enable flag, can be altered only in system mode. If POPF is executed in user mode by a program that attempts to pop the interrupt enable flag, POPF will act as a no op (i.e., no operation). Therefore, the behavior of POPF depends on the mode of operation, rendering as behavior sensitive. Finally, if the instruction is neither control sensitive nor behavior sensitive, it is innocuous.</p>
<p>According to Popek and Goldberg (1974), a hypervisor can be constructed if it satisfies three properties: efficiency, resource control, and equivalence. Efficiency entails executing all innocuous instructions directly on hardware without any interference by the hypervisor. Resource control suggests that it is not possible for any guest software to change the configuration of resources in a system. Equivalence requires identical behavior of a program running on a VM or on a traditional OS with no virtualization. One exception is a difference in performance. Popek and Goldberg’s proposal (or theorem) implies that a hypervisor can be constructed only if the set of sensitive instructions is a subset of the set of privileged instructions. That is to say, instructions that interfere with the correct functioning of the system (i.e., sensitive instructions, such as LPSW) should always trap in the user mode. Figure 3.19 illustrates Popek and Goldberg’s theorem. [1]</p>
<p><img src="/images/14551132129213.jpg" alt="Figure 3.19: Demonstrating Popek and Goldberg&#39;s theorem."></p>
<p>Finally, let us discuss how a trap can be handled in a system. Specifically, we describe traps in the context of CPU virtualization. Figure 3.20 demonstrates how a hypervisor can handle an instruction trap. The hypervisor’s trap-handling functions can be divided into three main parts: dispatcher, allocator, and a set of interpreter routines. First, a privileged instruction traps to the hypervisor’s dispatcher. If the hypervisor recognizes that the instruction is attempting to alter system resources, it directs it to the allocator; otherwise, it sends it to a corresponding interpreter routine. The allocator decides how system resources are to be allocated in a nonconflicting manner and satisfies the instruction’s request accordingly. The interpreter routines emulate (more on emulation shortly) the effects of the instruction when operating on virtual resources. When the instruction is handled fully (i.e., done), control is passed back to the guest software at the instruction that comes immediately after the one that caused the trap.</p>
<p><img src="/images/14551132303380.jpg" alt="Figure 3.20: Demonstration of a trap to a hypervisor. The hypervisor includes three main components: the dispatcher, the allocator, and the interpreter routines."></p>
<p><strong>References</strong></p>
<ol>
<li>Popek, J., and Goldberg, R. (1974). “Formal Requirements for Virtualizable Third Generation Architectures.” Commun. ACM, Vol. 17, No. 7.</li>
</ol>
<h3 id="Full_Virtualization_and_Paravirtualization"><a href="#Full_Virtualization_and_Paravirtualization" class="headerlink" title="Full Virtualization and Paravirtualization"></a>Full Virtualization and Paravirtualization</h3><p>A problem arises when an instruction that is sensitive but unprivileged is issued by a process running on a VM in user mode. According to Popek and Goldberg (1974), sensitive instructions have to trap to the hypervisor if executed in user mode. However, as explained earlier, sensitive instructions can be privileged (e.g., LPSW) and unprivileged (e.g., POPF). Unprivileged instructions do not trap to the hypervisor. Instructions that are sensitive and unprivileged are called critical (see Fig. 3.21). ISAs that contain critical instructions do not satisfy Popek and Goldberg’s theorem [1] . Video 3.6 covers this concept and ways around it:</p>
<p><a href="http://youtu.be/ay0ZpkMbw_8" target="_blank" rel="external">Video 3.6: ISA virtualization techniques.</a></p>
<p>The challenge becomes, can a hypervisor be constructed in the presence of critical instructions? The answer is yes. Nonetheless, Smith and Nair [2] distinguish between a hypervisor that complies with Popek and Goldberg’s theorem and one that does not comply by referring to the former as a true or an efficient hypervisor and to the latter simply as a hypervisor.</p>
<p><img src="/images/14551133049722.jpg" alt="Figure 3.21: Instructions that do not satisfy Popek and Goldberg’&#39; theorem. They are called critical instructions."></p>
<p>If a processor does not satisfy Popek and Goldberg’s virtualization requirement, a hypervisor can be constructed by using code patching, full virtualization, and/or paravirtualization. As illustrated in Figure 3.22, code patching requires the hypervisor to scan the guest code before execution, discover all critical instructions, and replace them with traps (system calls) to the hypervisor. Full virtualization emulates all instructions in the ISA. Emulation degrades performance because it implies reproducing the behavior of every source instruction by first translating it to a target instruction, then running it on a target ISA (more on emulation shortly). Paravirtualization deals with critical instructions by modifying guest OSs. Specifically, it entails rewriting every critical instruction as a hypercall that traps to the Xen hypervisor. Accordingly, paravirtualization improves performance due to totally avoiding emulation but at the expense of modifying guest OSs. In reverse, full virtualization avoids modifying guest OSs but at the expense of degrading system performance. As examples, VMware uses full virtualization, while Xen employs paravirtualization. Xen supports most major OSs, including Windows, Linux, Solaris, and NetBSD.</p>
<p><img src="/images/14551133248973.jpg" alt="Figure 3.22: Code scanning and patching to enforce critical instructions to trap to the hypervisor. The code is shown in a format close to a control flow diagram."></p>
<p><strong>References</strong></p>
<ol>
<li>Popek, J., and Goldberg, R. (1974). “Formal Requirements for Virtualizable Third Generation Architectures.” Commun. ACM, Vol. 17, No. 7.</li>
<li>Smith, J. E., and Nair, R. (2005). “The Architecture of Virtual Machines.” Computer, 38(5), 32-38.</li>
</ol>
<h3 id="Emulation"><a href="#Emulation" class="headerlink" title="Emulation"></a>Emulation</h3><p>Now that we understand the conditions for virtualizing ISAs and the two main classes of CPU virtualization, full virtualization and paravirtualization, we move to discussing emulation as a technique used to implement full virtualization and process VMs. Emulation has been introduced previously, but to recap, emulation is the process of allowing the interfaces and functionalities of one system (the source) to be implemented on a system with different interfaces and functionalities (the target). Emulation is the only CPU virtualization mechanism available when the guest and host ISAs are different. If the guest and host ISAs are identical, direct native execution can (possibly) be applied.</p>
<p>Emulation is carried out either via interpretation or binary translation. With interpretation, source instructions are converted, one instruction at a time, to relevant target instructions. Interpretation is relatively slow because of the one-by-one emulation of instructions and the lack of any optimization technique (e.g., precluding the interpretation of an already encountered and interpreted instruction). Binary translation optimizes interpretation via converting blocks of source instructions to target instructions and caching generated blocks for repeated use. Typically, blocks of instructions are more amenable to optimizations than single instructions. Compared to interpretation, binary translation is much faster because of the application of block caching as well as code optimizations over blocks. In the following video, we discuss three major interpretation schemes: decode and dispatch, indirect threaded, and direct threaded.</p>
<p><a href="http://youtu.be/TutPj22Dq94" target="_blank" rel="external">Video 3.7: Interpretation Techniques in Virtualization</a></p>
<p>As explained in Video 3.7, a basic interpreter should read through the source code instruction by instruction, analyze each instruction, and call relevant routines to generate the target code. An interpreter called decode and dispatch applies basic interpretation but results in a number of branch (or jump) instructions, both direct and indirect, which leads to poor execution times. As an optimization for decode and dispatch, an interpreter called indirect threaded attempts to release some of the decode-and-dispatch branches via appending (or threading) a portion of the dispatch code to the end of each interpreter routine. Last, a more advanced interpreter, called direct threaded, improves indirect threaded by attempting to interpret a repeated operation only once. Although the direct-threaded interpreter improves the indirect threaded and the decode-and-dispatch branches, it still suffers from major drawbacks, such as vast memory image and limited portability. In the next video, we discuss binary translation, which essentially targets the limitations of direct-threaded interpretation.</p>
<p><a href="http://youtu.be/wnbYwUm6KgY" target="_blank" rel="external">Video 3.8: Binary Translation.</a></p>
<p>As presented in Video 3.8, binary translation tries to amortize the fetch and analysis costs caused by the direct-threaded interpreter through: (1) translating a block of source instructions (rather than a single instruction) to a block of target instructions and (2) caching the translated code in an attempt to save interpreting source instructions more than once. The following table compares binary translation, decode-and-dispatch, indirect-threaded, and direct-threaded emulation techniques in terms of four metrics: memory requirements, startup performance, steady-state performance, and code portability. For instance, the decode-and-dispatch interpreter row reads as follows: first, with decode and dispatch, memory requirements remain low because of having only one interpreter routine for each instruction type in the target ISA. Furthermore, decode and dispatch avoids threading the dispatch code at the end of each routine. Second, startup performance is fast because neither predecoding nor translation of the source binary is applied. Third, steady-state performance (i.e., the performance after starting up the interpreter) is slow because of the high number of branches and the interpretation of every instruction per each appearance. Finally, code portability is good because predecoding with addresses of interpreter routines is not applied by decode-and-dispatch interpreters (in contrast to direct-threaded interpreters).</p>
<p><img src="/images/14551136108303.jpg" alt=""></p>
<h3 id="Virtual_CPU"><a href="#Virtual_CPU" class="headerlink" title="Virtual CPU"></a>Virtual CPU</h3><p><strong>Uniprocessor and Multiprocessor VMs</strong></p>
<p>As described earlier in the unit, a virtual CPU (vCPU) acts as a proxy to a physical CPU (pCPU). In other words, a vCPU is a representation of a pCPU to a guest OS (i.e., an OS that runs on a VM). A vCPU can be initiated in a VM and mapped to an underlying pCPU by the hypervisor. In principle, a VM can have one or many vCPUs. For instance, a VM in VMware ESX 4 can have up to eight vCPUs. The amount of vCPUs represents the width of a VM. A VM with a width greater than one is called a symmetric multiprocessing (SMP) VM. In reverse, a VM with a width equal to one is called a uniprocessor (UP) VM. Video 3.9 discusses Multiprocesseor VMs in detail:</p>
<p><a href="http://youtu.be/0iZrCvccTdY" target="_blank" rel="external">Video 3.9: Multiprocessor VMs.</a></p>
<p>Figure 3.23 demonstrates an SMP native system VM with a width of four and a UP native system VM, both running on the same hardware.</p>
<p><img src="/images/14551137270984.jpg" alt="Figure 3.23: An SMP native system VM with a width of four and a UP native system VM, both running on the same hardware."></p>
<p>Similar to a process on a general-purpose OS, a vCPU can be in different states, such as running, ready, and wait states. At a certain point in time, a vCPU can be scheduled by the hypervisor at only a single core (akin to scheduling an OS process on a core). For instance, a UP VM running on a host machine equipped with two Xeon 5405 (i.e., a total of eight pCPUs) will run only on one of the eight available cores. Inherently parallel workloads, such as MapReduce applications, prefer SMP VMs. We next discuss how the hypervisor schedules vCPUs on pCPUs.</p>
<p><strong>Virtual CPU Scheduling and Xen’s Schedulers</strong></p>
<p>General-purpose OSs support two levels of scheduling, process and thread scheduling. With a hypervisor, one extra level of scheduling is added, vCPU scheduling. The hypervisor schedules vCPUs on the underlying pCPU(s), thereby providing each guest VM a portion of the underlying physical processing time.</p>
<p>We (briefly) discuss two popular schedulers from Xen, Simple Earliest Deadline First (SEDF) and Credit Scheduler (CS). As its name suggests, SEDF is simple, whereby only two parameters, n (the slice) and m (the period), are involved. A VM (or domain Ui in Xen’s parlance) can request n every m. SEDF specifies a deadline for each vCPU computed in terms of n and m. The deadline is defined as the latest time a domain can be run to meet its deadline. For instance, a domain Ui can request n = 10ms and m = 100ms. Accordingly, a vCPU at this domain can be scheduled by SEDF as late as 90ms into the 100ms period, yet still meet its deadline. SEDF operates by searching across the set of all runnable vCPUs, held in a queue, and selecting the one with the earliest deadline.</p>
<p>Xen’s Credit Scheduler (CS) is more involved than SEDF. First, when configuring a VM, each vCPU in the VM is set with two properties: the weight and the cap. The weight determines the share of a pCPU’s capacity that should be provided to a vCPU. For instance, if two vCPUs, vCPU-1 and vCPU-2, are specified with weights of 256 (the default) and 128, respectively, vCPU-1 will obtain double the share of vCPU-2. Weights can range from 1 to 65,535. The cap determines the total percentage of a pCPU that should be given to a vCPU. The cap can modify the behavior of the weight. Nonetheless, a vCPU can be kept uncapped.</p>
<p>CS converts each vCPU’s weight to credits. Credits from a vCPU will be deducted as long as it is running. A vCPU is marked as over once it runs out of credits and is otherwise marked under. CS maintains a queue per each pCPU (assuming a chip multiprocessors architecture) and stores all under vCPUs first, followed by all over vCPUs. CS operates by picking the first under vCPU in the queue to go next. CS keeps track of each vCPU’s credits, and when switching a vCPU out, it places it in the queue at the end of the appropriate category. Finally, CS applies load balancing by allowing a pCPU with no under vCPUs to pull under vCPUs from queues of other pCPUs. More details on SEDF and CS can be found in Chisnall’s The Definitive Guide to the Xen Hypervisor. [1]</p>
<p>Xen is an open source hypervisor. Hence, it is possible to devise and add your own scheduler. Chisnall provides an excellent and comprehensive coverage of Xen’s internals as well as step-by-step instructions for adding a new scheduler to Xen.</p>
<p><strong>References</strong></p>
<ol>
<li>David Chisnall (2007). “The Definitive Guide to the Xen Hypervisor.” Prentice Hall.</li>
</ol>
<h3 id="Resource_Virtualization_3A_CPU_Summary"><a href="#Resource_Virtualization_3A_CPU_Summary" class="headerlink" title="Resource Virtualization: CPU Summary"></a>Resource Virtualization: CPU Summary</h3><ul>
<li>Virtualizing a physical CPU (pCPU) involves: (1) timesharing the pCPU among virtual CPUs (vCPUs) contained and executed in VMs (called vCPU scheduling) and (2) virtualizing the ISA of the pCPU to make it amenable to host vCPUs with different ISAs.</li>
<li>A vCPU acts as a proxy to a pCPU.</li>
<li>In principle, a VM can have one or many vCPUs.</li>
<li>A VM that includes more than one vCPU is called a symmetric multiprocessing (SMP) VM, while a VM with one vCPU is called a uniprocessor (UP) VM.</li>
<li>In principle, hypervisors can support three levels of scheduling, process, thread, and vCPU scheduling.</li>
<li>Examples of vCPU schedulers are Simple Earliest Deadline First (SEDF) and Credit Scheduler (CS) from Xen.</li>
<li>In addition to vCPU scheduling, virtualizing a pCPU requires virtualizing the instructions defined in its ISA.</li>
<li>Instructions in ISAs can generally be classified into two types: privileged and unprivileged instructions.</li>
<li>A privileged instruction is defined as one that traps in user mode and does not trap in system mode.</li>
<li>In addition, instructions can be further classified into two different categories: sensitive and innocuous.</li>
<li>Sensitive instructions can be either control sensitive or behavior sensitive.</li>
<li>Control-sensitive instructions are those that attempt to modify the configuration of resources in a system (e.g., LPSW from IBM System/370).</li>
<li>Behavior-sensitive instructions are those whose behaviors are determined by the current configuration of resources in a system (e.g., POPF from Intel IA-32).</li>
<li>When the instruction is neither control sensitive nor behavior sensitive, it is innocuous.</li>
<li>Sensitive instructions can be privileged (e.g., LPSW) and unprivileged (e.g., POPF).</li>
<li>Popek and Goldberg (1974) suggested that a hypervisor can only be constructed if the set of sensitive instructions is a subset of the set of privileged instructions (i.e., sensitive instructions always trap in the user mode).</li>
<li>A problem arises when instructions that are sensitive but unprivileged are issued in VMs running in user mode (i.e., they will not trap as such).</li>
<li>The instructions that are sensitive and unprivileged are called critical instructions.</li>
<li>A hypervisor can still be constructed for ISAs that contain critical instructions.</li>
<li>Constructing a hypervisor with the presence of critical instructions can be achieved using either code patching, full virtualization, and/or paravirtualization.</li>
<li>Code patching replaces all critical instructions with system calls to the hypervisor, thus enforcing them to trap.</li>
<li>Full virtualization emulates all instructions in the ISA.</li>
<li>Emulation is a popular technique in virtualizing CPUs. It allows the interfaces and functionalities of one system (the source) to be implemented on a system with different interfaces and functionalities (the target).</li>
<li>Emulation can be implemented using either interpretation or binary translation.</li>
<li>Interpretation techniques (e.g., decode and dispatch, indirect threaded, and direct threaded) translate source instructions to target instructions one at a time, while binary translation converts blocks of source instructions to target instructions and caches them for repeated use.</li>
<li>Paravirtualization rewrites every critical instruction as a hypercall that traps to the hypervisor (which typically requires modifying guest OSs).</li>
<li>As concrete examples, VMware uses full virtualization, while Xen employs paravirtualization.</li>
</ul>
<h2 id="Resource_Virtualization_-_Memory"><a href="#Resource_Virtualization_-_Memory" class="headerlink" title="Resource Virtualization - Memory"></a>Resource Virtualization - Memory</h2><p>The next resource we will examine, with respect to virtualization, is memory. Memory virtualization should ring a bell; specifically, it is very closely related to the operating systems concept of virtual memory! As such, we will begin our discussion by recalling virtual memory concepts and then discuss memory virtualization as an extension of these techniques. VMWare has pioneered some interesting and clever techniques in the realm of memory reclamation from Guest OSes, which will also be covered in this module.</p>
<h3 id="One-Level_Page_Mapping"><a href="#One-Level_Page_Mapping" class="headerlink" title="One-Level Page Mapping"></a>One-Level Page Mapping</h3><p><strong>Virtual Memory and One-Level Page Mapping</strong></p>
<p>Virtual memory is a well-known virtualization technique supported in most general-purpose OSs. The basic idea of virtual memory is that each process is provided with its own virtual address space, broken up into chunks called virtual pages. A page is a contiguous range of addresses. As shown in Figure 3.24, virtual memory maps virtual pages to physical pages in what is called a page table. We call this one-level page mapping between two types of addresses: the virtual and the physical. Each process in the OS has its own page table. A main observation pertaining to page tables is that not all virtual pages of a process need to be mapped to respective physical pages in order for the process to execute. When a process references a page that exists in the physical memory (i.e., there is a mapping between the requested virtual page and the corresponding physical page), a page hit is attained. On a page hit, the hardware obtains the required virtual to physical mapping with no further actions. In contrary, when a process references a page that does not exist in the physical memory, a page miss is incurred. On a page miss, the OS is alerted to handle the miss. Subsequently, the OS fetches the missed page from disk storage and updates the relevant entry in the page table.</p>
<p><img src="/images/14556001462899.jpg" alt="Figure 3.24: Mapping a process&#39; virtual address space to physical address space. This is captured in what is called a page table. Each process has its own page table."></p>
<h3 id="Two-Level_Page_Mapping"><a href="#Two-Level_Page_Mapping" class="headerlink" title="Two-Level Page Mapping"></a>Two-Level Page Mapping</h3><p>Contrary to OSs in traditional systems, with system virtualization, the hypervisor allocates a contiguous addressable memory space for each created VM (not process). This memory space per a VM is called real memory. In return, each guest OS running in a VM allocates a contiguous, addressable memory space for each process in its real memory. This memory space per a process is called virtual memory (same name as in traditional systems). Each guest OS maps the virtual memories of its processes to the real memory of the underlying VM, while the hypervisor maps the real memories of its VMs to the system physical memory. Clearly, in contrast to traditional OSs, this entails two levels of mappings between three types of addresses: virtual, real, and physical. In fact, these virtual-to-real and real-to-physical mappings define system memory virtualization. This basic idea of memory virtualization via two-level page mapping is summarized in Video 3.10:</p>
<p><a href="http://youtu.be/Gl0Dw7G9V5U" target="_blank" rel="external">Video 3.10: Memory Virtualization.</a></p>
<p><img src="/images/14556002072284.jpg" alt="Figure 3.25: Memory virtualization in a native system VM."></p>
<p>Similar to any general-purpose OS, a guest OS would still own its set of page tables. In addition, the hypervisor would own another set of page tables for mapping real-to-physical addresses. The page tables in the hypervisor are called real map tables. Figure 3.25 demonstrates system memory virtualization in a native system VM. It shows page tables maintained by guest VMs and real map tables maintained by the hypervisor. Each entry in a page table maps a virtual page of a program to a real page in the respective VM. Likewise, each entry in a real map table maps a real page in a VM to a physical page in the physical memory. When a guest OS attempts to establish a valid mapping entry in its page table, it traps to the hypervisor. Subsequently, the hypervisor establishes a corresponding mapping in the relevant VM’s real map table.</p>
<h3 id="Memory_Overcommitment"><a href="#Memory_Overcommitment" class="headerlink" title="Memory Overcommitment"></a>Memory Overcommitment</h3><p>In memory virtualization, the combined total size of real memories can grow beyond the actual size of physical memory. This concept is typically called memory overcommitment. Memory overcommitment ensures that physical memory is highly utilized by active, real memories (assuming multiple VMs running simultaneously). Indeed, without memory overcommitment, the hypervisor can only run VMs with a total size of real memories less than that of the physical memory. For instance, Figure 3.26 shows a hypervisor with 4GB of physical memory and three VMs, each with 2GB of real memory. Without memory overcommitment, the hypervisor can only run one VM because of not having enough physical memory to assign to two VMs at once. Although each VM would require only 2GB of memory, wherein the hypervisor has 4GB of physical memory, this memory cannot be afforded because hypervisors generally require overhead memories (e.g., to maintain various virtualization data structures).</p>
<p><img src="/images/14556002486593.jpg" alt="Figure 3.26: A hypervisor with 4GB of physical memory, enabling three VMs at once with a total of 6GB of real memory."></p>
<p>To this end, in practical situations, some VMs might be lightly loaded, while others might be heavily loaded. Lightly loaded VMs can cause some pages to sit idle, while heavily loaded VMs can result in memory page thrashing. To deal with such a situation, the hypervisor can take (or steal) the inactive physical memory pages away from idle VMs and provide them to heavily loaded VMs. As a side effect, hypervisors usually write zeros to the stolen/reclaimed, inactive physical memory pages in order to avert information leaking among VMs.</p>
<h3 id="Reclamation_Techniques_and_VMware_Memory_Ballooning"><a href="#Reclamation_Techniques_and_VMware_Memory_Ballooning" class="headerlink" title="Reclamation Techniques and VMware Memory Ballooning"></a>Reclamation Techniques and VMware Memory Ballooning</h3><p><strong>Memory Reclamation</strong></p>
<p>To maintain full isolation, guest OSs are kept unaware that they are running inside VMs. VMs are also kept unaware of the states of other VMs running on the same physical host. Furthermore, with multiple levels of page mapping, VMs remain oblivious of any physical memory shortage. Therefore, when the hypervisor runs multiple VMs at a physical host, and the physical memory turns stressed, none of the VMs can automatically help in freeing up memory.</p>
<p>The hypervisor deals with the situation by applying a reclamation technique. As its name suggests, a reclamation technique attempts to reclaim inactive real memory pages at VMs and make them available for the hypervisor when experiencing a memory shortage. Video 3.11 describes a couple of techniques which can be used to reclaim memory from guest operating systems:</p>
<p><a href="http://youtu.be/fAb6DeSNUv8" target="_blank" rel="external">Video 3.11: Advanced Memory Management.</a></p>
<p>One of the popular reclamation techniques is the ballooning process introduced in VMware ESX, which has been the basis for similar techniques in other hypervisors.</p>
<p><img src="/images/14556003375691.jpg" alt="Figure 3.27: The ballooning process in VMware ESX."></p>
<p>In VMware ESX, a balloon driver must be installed and enabled in each guest OS as a pseudo-device driver. The balloon driver regularly polls the hypervisor through a private channel to obtain a target balloon size. As illustrated in Figure 3.27, when the hypervisor experiences memory shortage, it inflates the balloon by setting a proper target balloon size. Figure 3.27(a) shows four real memory pages mapped to four physical pages, of which only two pages are actually active (the red and the yellow ones). Without involving the ballooning process, the hypervisor is unaware of the other two inactive pages (the green and the dark-blue ones) because they are still mapped to physical pages. Consequently, the hypervisor will not be able to reclaim inactive pages unless getting informed. With memory ballooning, however, the hypervisor can set the balloon target size to an integer number (say 2 or 3). When recognized by the balloon driver at the guest OS, the driver checks out the pages, locates the two inactive pages, and pins them (see Figure 3.27(b)). The pinning process is carried out by the guest OS via ensuring that the pinned pages cannot be read/written by any process during memory reclamation. After pinning the inactive pages, the balloon driver transmits to the hypervisor the addresses of the pinned pages. Subsequently, the hypervisor proceeds safely with reclaiming the respective physical pages and allocating them to needy VMs. Last, to unpin pinned pages, the hypervisor deflates the balloon by setting a smaller target balloon size and communicates that to the balloon driver. When received by the balloon driver, it unpins the pinned pages so the guest OS can utilize them. More information about the ballooning process can be found in VMware ESX’s documentation.</p>
<h3 id="Resource_Virtualization_3A_Memory_Summary"><a href="#Resource_Virtualization_3A_Memory_Summary" class="headerlink" title="Resource Virtualization: Memory Summary"></a>Resource Virtualization: Memory Summary</h3><ul>
<li>In the earliest days, either a process fit a memory or it could not be run.</li>
<li>Virtual memory changed the status quo by allowing a process that cannot fit a physical memory to run as if it essentially fits the memory.</li>
<li>An indirect inference of virtual memory is that multiple processes that cannot collectively fit a certain physical memory can now run altogether on this same physical memory.</li>
<li>The basic idea of virtual memory is that each process is provided with its own virtual address space.</li>
<li>The virtual address space of each process is translated to the physical address space that the physical memory uses.</li>
<li>The translation of virtual addresses to physical addresses is maintained in a software data structure called the page table.</li>
<li>In traditional systems (i.e., nonvirtualized environments), the virtual-to-physical translation is called one-level page mapping.</li>
<li>In virtualized environments (i.e., when a hypervisor is involved), the virtual-to-physical translation is extended at least one more level and called two-level page mapping.</li>
<li>The two-level page mapping entails two consecutive translations, virtual-to-real then real-to-physical translations. In this case, the real address space refers to the memory space of a VM, while the virtual and physical address spaces relate to the traditional memory spaces of processes and the physical memory.</li>
<li>As a result, memory virtualization in virtualized environments typically is perceived as an extension to the classic virtual memory concept supported in most general-purpose OSs.</li>
<li>When the combined total size of real memories grows beyond the actual size of the underlying physical memory, memory overcommitment is attained.</li>
<li>Memory overcommitment improves memory utilization via allowing VMs with aggregate real memories larger than the physical memory to run simultaneously.</li>
<li>Memory overcommitment, however, necessitates reclaiming inactive real memory pages at VMs and relocating them to the hypervisor when experiencing a physical memory shortage. This is called reclamation technique.</li>
<li>One of the popular reclamation techniques is the ballooning process incorporated in VMware ESX.</li>
</ul>
<h2 id="Resource_Virtualization_-_I/O"><a href="#Resource_Virtualization_-_I/O" class="headerlink" title="Resource Virtualization - I/O"></a>Resource Virtualization - I/O</h2><p>Our final segment in resource virtualization is that of IO devices. In this respect, we can consider the VMM or hypervisor the arbiter of communication between multiple guests and the physical hardware, multiplexing the usage (in time/space or both), depending on the actual device being shared.</p>
<p>The virtualization strategy for a given I/O device type consists of:</p>
<ol>
<li>Constructing a virtual version of that device, and</li>
<li>Virtualizing the I/O activity routed to the device.</li>
</ol>
<p>Typical I/O devices include disks, network cards, displays, and keyboards. As discussed previously, the hypervisor might create a virtual display as a window on a physical display. In addition, a virtual disk can be created by assigning to it a fraction of the physical disk’s storage capacity. After constructing virtual devices, the hypervisor ensures that each I/O operation is carried out within the bounds of the requested virtual device. For instance, if a virtual disk is allocated 100 cylinders from among 1,000 cylinders provided by a physical disk, the hypervisor guarantees that no I/O request intended for that virtual disk can access any cylinder other than the 100 assigned to it. More precisely, the disk location in the issued I/O request will be mapped by the hypervisor to only the area where the virtual disk has been allocated on the physical disk. Next, we cover some I/O basics and then move on to the details of I/O virtualization. After covering the basics, we will examine the case of Xen, and how it handles IO virtualization.</p>
<h3 id="I/O_Basics"><a href="#I/O_Basics" class="headerlink" title="I/O Basics"></a>I/O Basics</h3><p><strong>Basics of I/O</strong></p>
<p>To begin, each I/O device has a device controller. A device controller can typically be signaled either by a privileged I/O instruction or memory-mapped I/O. I/O instructions are provided by ISAs. Intel-32 is an example of a processor that provides I/O instructions in its ISA. Many recent processors, however, allow performing I/O between the CPU and the device controllers through memory-mapped I/O (e.g., RISC processors). As shown in Figure 3.28, with memory-mapped I/O, a specific region of the physical memory address space is reserved for accessing I/O devices. These addresses are recognized by the memory controller as commands to I/O devices and do not correspond to memory physical locations. Different memory-mapped addresses are used for different I/O devices. Finally, in order to protect I/O devices, both I/O instructions and memory-mapped addresses are handled in system mode, thus becoming privileged.</p>
<p><img src="/images/14556004831082.jpg" alt="Figure 3.28: Memory mapped I/O with a specific region in the RAM address space for accessing I/O devices."></p>
<p>Because I/O operations are executed in system mode, user programs can only invoke them through OS system calls (assuming traditional systems). The OS abstracts most of the details of I/O devices and makes them accessible through only well-defined interfaces. Figure 3.29 shows the three major interfaces that come into play when a user program places an I/O request. These are the system call interface, the device driver interface, and the operation-level interface. Starting an I/O operation, a user I/O request causes an OS system call that transfers control to the OS. Next, the OS calls device drivers (a set of software routines) via the device driver interface. A relevant device driver routine converts the I/O request to an operation specific to the requested physical device. The converted operation is subsequently carried through the operation-level interface to the corresponding physical device.</p>
<p><img src="/images/14556005015461.jpg" alt="Figure 3.29: The three major interfaces involved in I/O operations: system call, device driver, and operation-level interfaces."></p>
<h3 id="Virtualizing_I/O_Devices"><a href="#Virtualizing_I/O_Devices" class="headerlink" title="Virtualizing I/O Devices"></a>Virtualizing I/O Devices</h3><p><strong>I/O Virtualization</strong></p>
<p>I/O virtualization allows a single physical I/O device to be shared by more than one guest OS. Figure 3.30 demonstrates multiple guest OSs in native system VMs sharing a single hardware machine. As shown, the hypervisor constructs virtual devices from physical devices. A main observation is that both the guest OSs and the hypervisor must have device drivers encapsulating the interfaces to the devices. This means that with virtualization, two different device drivers must be supported per each device versus only one without virtualization. In reality, this is a problem because vendors of devices usually supply drivers for only the major OSs but not for hypervisors (though this could change in the near future). One way to circumvent such a problem is to collocate the hypervisor with a major OS (e.g., Linux) on the same machine. This way, I/O requests can be handled by the OS which holds all requisite I/O drivers. This is the approach adopted by Xen and discussed on the next page.</p>
<p><img src="/images/14556006679296.jpg" alt="Figure 3.30: Logical locations of device drivers in multiple guest OSs in native system VMs sharing a single hardware machine."></p>
<p>Moreover, with I/O virtualization, every I/O request issued by a user program at a guest VM should be intercepted by the hypervisor because I/O requests are all privileged and thus need to be controlled by the hypervisor. Clearly, this would entail a trap to the guest OS for every I/O request. All I/O requests are privileged, whether issued using I/O instructions or memory-mapped I/O; hence, they are not critical instructions, and they all trap to the hypervisor. As such, the hypervisor can easily intercept every I/O request simply when trapping. In principle, the hypervisor can intercept I/O requests at any of the three interfaces: the system call interface, the device driver interface, or the operation-level interface.</p>
<p>If the hypervisor intercepts an I/O request at the operation-level interface, some essential information about the I/O action might be lost. The hypervisor needs that information to handle I/O requests correctly. When an I/O request arrives at the device driver interface, it might get transformed into a sequence of instructions. When the sequence of instructions is received at the operation-level interface, it becomes difficult for the hypervisor to identify them as instructions for a single I/O request. For example, a disk write becomes multiple store instructions in case of memory-mapped I/O or multiple ISA I/O instructions. Hence, intercepting I/O requests at the operation-level interface typically is avoided. In contrast, intercepting an I/O request at the device driver interface allows the hypervisor to efficiently map the request to the respective physical device and transmit it through the operation-level interface. Clearly, this process is a natural point for I/O virtualization; yet it would oblige hypervisor developers to learn about the different device driver interfaces of various guest OSs in order to be able to intercept I/O requests. Last, intercepting I/O requests at the system call interface (i.e., the application binary interface [ABI]) might theoretically make the I/O virtualization process easier, whereby the entire I/O operation could be handled for each request by the hypervisor (the solo controller in this case). To achieve that goal, however, the hypervisor has to emulate the ABI routines of every guest OS (different OSs have different ABI routines). Consequently, hypervisor developers need also to learn about the internals of every potential guest OS. Furthermore, emulating ABI routines can degrade system performance due to the overhead imposed by the emulation process. In practice, intercepting I/O requests at the device driver interface can be more efficient. In the next video, we discuss the overall network virtualization process as applied to a physical network adapter.</p>
<p><a href="http://youtu.be/tMOAx08Ws3w" target="_blank" rel="external">Video 3.12: Shared Devices in Virtualization. </a></p>
<p>As explained in Video 3.12, one physical adapter card can appear as multiple virtual network interface cards (vNICs), each with a separate MAC address and on the same network as the physical one. To network infrastructures, such as LANs and SANs, vNICs appear as regular physical cards.</p>
<h3 id="Xen_u2019s_Approach_to_I/O_Virtualization"><a href="#Xen_u2019s_Approach_to_I/O_Virtualization" class="headerlink" title="Xen’s Approach to I/O Virtualization"></a>Xen’s Approach to I/O Virtualization</h3><p><strong>I/O Virtualization in Xen</strong></p>
<p>As a concrete example, we discuss Xen’s approach to I/O virtualization. As we pointed out earlier, to get around the problem of having device drivers for the hypervisor as well as the guest OSs, Xen collocates its hypervisor with a traditional general-purpose OS. Figure 3.31 shows a host OS and the Xen hypervisor executing in full privileges at ring 0. Guest OSs run unprivileged at ring 1, while all processes at all domains (i.e., virtual machines) run unprivileged at ring 3. Clearly, the figure assumes a system with four rings (e.g., Intel-32). On systems with only two levels of privileges, the hypervisor and the host OS can execute in system mode, while domains and processes can execute in user mode. As illustrated in the figure, Xen eliminates the device drivers entirely from guest OSs and provides a direct communication between guest OSs at domain U and the host OS at domain 0. More precisely, every domain Ui in Xen will not hold any virtual I/O devices or relevant drivers. Rather, every I/O request is now transferred directly to domain 0, which by default hosts all the required device drivers necessary to satisfy all I/O requests. For instance, rather than using a device driver to control a virtual network card interface (vNIC), with Xen network, frames/packets are transferred through event channels directly to and from domain 0. This is done using NIC frontend and backend interfaces at domain Uj (in which j &gt; 0) and U0, respectively. Likewise, no virtual disk is exposed to any guest OS, and all disk data blocks imposed by file reads and writes are delegated by Xen to domain 0.</p>
<p><img src="/images/14556007538437.jpg" alt="Figure 3.31: Xen&#39;s approach to I/O virtualization assuming a system with four rings (e.g., Intel-32). Xen collocates an OS, at a VM0 called domain 0, with the hypervisor on the physical platform to &quot;borrow&quot; its device drivers and avoid coding them in the hypervisor. This makes the hypervisor &quot;thinner&quot; and accordingly more reliable. Also, it makes it easier on the hypervisor developers."></p>
<h3 id="Resource_Virtualization_3A_I/O_Summary"><a href="#Resource_Virtualization_3A_I/O_Summary" class="headerlink" title="Resource Virtualization: I/O Summary"></a>Resource Virtualization: I/O Summary</h3><ul>
<li>To virtualize an I/O device, we ought to follow two main steps: (1) construct a virtual version of the device and (2) virtualize the I/O activity routed to the device.</li>
<li>Constructing a virtual version of an I/O device entails sharing the device across multiple guest OSs.</li>
<li>Virtualizing the I/O activity to an I/O device passes through the device’s controller (each I/O device has a device controller).</li>
<li>A device controller can be signaled via either a privileged I/O instruction (defined in ISA) or memory-mapped I/O.</li>
<li>I/O instructions and memory-mapped addresses are handled in system mode to protect the called I/O devices.</li>
<li>I/O instructions and memory-mapped addresses are not critical, thus can be easily handled by the hypervisor after naturally trapping to it (i.e., because they are privileged and not critical, they will naturally trap to the hypervisor when run in user mode).</li>
<li>General-purpose OSs abstract most of the details of I/O devices and make them accessible only through well-defined interfaces, such as the system call interface, the device driver interface, and the operation-level interface.</li>
<li>In the presence of a hypervisor, two different device drivers must be supported per each I/O device versus only one in traditional nonvirtualized systems.</li>
<li>The redundancy of device drivers in the presence of a hypervisor is usually circumvented by collocating the hypervisor with a major OS (e.g., Linux) on the same machine. Subsequently, the hypervisor leverages the device drivers of the major OS without requiring special device drivers (Xen applies this approach).</li>
<li>Because all I/O instructions are privileged, they need to be intercepted by the hypervisor.</li>
<li>In principle, the hypervisor can intercept I/O requests at any of the three interfaces: the system call interface, the device driver interface, and the operation-level interface.</li>
<li>Intercepting I/O requests at the operation-level interface might lead to the loss of some essential information about I/O actions.</li>
<li>Intercepting I/O requests at the system call interface (i.e., the ABI) entails emulating the ABI routines of every guest OS (different OSs have different ABI routines).</li>
<li>In practice, intercepting I/O requests at the device driver interface is typically the most efficient approach because it avoids emulating the ABI routines of every guest OS and losing some necessary information about I/O actions.</li>
</ul>
<h2 id="Case_Study"><a href="#Case_Study" class="headerlink" title="Case Study"></a>Case Study</h2><p>Now that we have covered some of the theory behind virtualization, we can look at a few case studies. Specifically, we start out with an overview of current virtualization suites in the market, which includes a feature-wise comparison of VMWare, XenServer, HyperV and RHEV. Next, we will take a closer look at Amazon EC2 as a public IaaS provider and some of the design decisions made with respect to providing a general, broad-based public IaaS cloud service.</p>
<h3 id="A_Taxonomy_of_Virtualization_Suites"><a href="#A_Taxonomy_of_Virtualization_Suites" class="headerlink" title="A Taxonomy of Virtualization Suites"></a>A Taxonomy of Virtualization Suites</h3><p><strong>Virtualization Suites</strong></p>
<p>We briefly survey some of the current and common virtualization software suites and distinguish between virtualization suites and hypervisors. Many vendors often use hypervisor and virtualization suite interchangeably. As discussed throughout this chapter, a hypervisor is primarily responsible for running multiple virtual machines (VMs) on a single physical host. A virtualization suite comprises various software components and individual hypervisors that enable the management of many physical hosts and VMs. A management component typically issues commands to the hypervisor to create, destroy, manage, and migrate VMs across multiple physical hosts.</p>
<p>The table below shows our taxonomy of four virtualization suites, <a href="http://www.vmware.com/" target="_blank" rel="external">vSphere 5.1</a>, <a href="http://www.microsoft.com/en-us/server-cloud/hyper-v-server/default.aspx" target="_blank" rel="external">Hyper-V</a> , <a href="http://support.citrix.com/product/xens/v6.0/" target="_blank" rel="external">XenServer 6</a>, and <a href="http://www.redhat.com/promo/rhev3/" target="_blank" rel="external">RHEV 3</a>. We compare the suites in terms of multiple features, including the involved hypervisor, the virtualization type, the allowable maximum number of vCPUs per VM, the allowable maximum memory size per VM, and whether memory overcommitment, page sharing, and live migration are supported. In addition, we indicate whether the involved hypervisors contain device drivers, and we list some of the popular cloud vendors that utilize such hypervisors.</p>
<p>To elaborate on some of the features, live migration allows running VMs to be seamlessly shifted from one physical machine to another. It enables many management features, such as maintenance, power-efficient dynamic server consolidation, and workload balancing, among others. Page Sharing refers to sharing identical memory pages across VMs. This renders effective when VMs use similar OS instances. Finally, some hypervisors eliminate device drivers entirely at guest OSs and provide direct communications between guest OSs and host OSs collocated with hypervisors (similar to what we discussed in the Section “Xen’s Approach to I/O Virtualization”).</p>
<p><img src="/images/14556011023821.jpg" alt=""></p>
<h3 id="Amazon_u2019s_Elastic_Compute_Cloud"><a href="#Amazon_u2019s_Elastic_Compute_Cloud" class="headerlink" title="Amazon’s Elastic Compute Cloud"></a>Amazon’s Elastic Compute Cloud</h3><p><strong>Amazon EC2</strong></p>
<p>Amazon Elastic Compute Cloud (Amazon EC2) is a vital part of Amazon’s cloud computing platform, Amazon Web Services (AWS). On August 25, 2006, Amazon launched EC2, which together with Amazon Simple Storage Service (Amazon S3) marked a change in the way IT was done. Amazon EC2 is a highly reliable and scalable infrastructure as a service (IaaS) with a utility payment model. It allows users to rent virtual machines (VMs) and pay for the resources that they actually consume. Users can set up and configure everything in their VMs, ranging from the operating system to any application. Specifically, a user can boot an Amazon Machine Image (AMI) to create a VM, referred to in Amazon’s parlance as an instance. AMI is a virtual appliance (or a VM image) that contains the user’s operating system, applications, libraries, data, and associated configuration settings.</p>
<p>Users can create EC2 instances either by using default AMIs prepackaged by Amazon or by developing their own AMIs using Amazon’s bundling tools. Default AMIs are preconfigured with an ever-growing list of operating systems, including Red Hat Enterprise Linux, Windows Server, and Ubuntu. A wide selection of free software provided by Amazon can also be directly incorporated into AMIs and executed over EC2 instances. For example, Amazon provides software for databases (e.g., Microsoft SQL), application servers (e.g., Tomcat Java Web Application), content management (e.g., MediaWiki), and business intelligence (e.g., Jasper Reports). Added to the wide assortment of free software, Amazon services (e.g., Amazon Relational Database Service, which supports MySQL, Oracle, and Microsoft SQL databases) can be further employed in conjunction with EC2 instances. Finally, users can always configure, install and run at any time any compatible software on EC2 instances, exactly as is the case with regular physical machines.</p>
<p><strong>Amazon EC2 Virtualization Technology</strong></p>
<p>Amazon EC2 demonstrates the power of cloud computing. Under the hood, it is a marvel of technology. As of March 2012, it was hosting around 7,100 server racks with a total of 454,400 blade servers, assuming 64 blade servers per rack. [1] Above its data centers, Amazon EC2 presents a true virtualization environment using the Xen hypervisor. Xen is a leading example of system virtualization, initially developed as part of the Xenoserver project at the Computer Laboratory, Cambridge University. [2] Currently, Xen is maintained by an open source community. [3] The goal of Xen is to provide IaaS with full isolation and minimal performance overhead on conventional hardware. As discussed previously in this unit, Xen is a native hypervisor that runs on bare metal at the most privileged CPU state. Amazon EC2 uses a highly customized version of Xen [4] to provision and isolate user instances rapidly, consolidate instances to improve system utilization, tolerate software and hardware failures by saving and migrating instances, apply system load balancing through live and seamless migration of instances, and more.</p>
<p>Amazon EC2 instances can be created, launched, suspended, resumed, and terminated as needed. The instances are system VMs composed of virtualized (or paravirtualized) sets of physical resources, including CPU, memory, and I/O components. To create instances, Xen starts a highly privileged instance (domain 0) at a host OS out of which other user instances (domain U) can be instantiated with guest OSs (see Fig. 1.25). As host OSs, Novell’s SUSE Linux Enterprise Server, Solaris and Open-Solaris, NetBSD, Debian, and Ubuntu, among others, can be used. It is not known, however, which among these host OSs Amazon EC2 supports, but Linux, Solaris and OpenSolaris, FreeBSD, NetBSD, and others can be employed as guest OSs. Among the guest OSs that Amazon EC2 supports are Linux, OpenSolaris, and Windows Server 2003, [5] Amazon EC2’s guest OSs are run at a lesser privileged ring than the host OS and the hypervisor. Clearly, this helps isolate the hypervisor from guest OSs and guest OSs from each other, a key requirement on Amazon AWS’s cloud platform. Nonetheless, running guest OSs in unprivileged mode violates the usual assumption that OSs must run in system mode. To circumvent consequent ramifications, Xen applies a paravirtualized approach whereby guest OSs are modified to run at a downgraded privileged level. As a result, sensitive instructions are enforced to trap to the hypervisor for verification and execution. Linux instances (and most likely OpenSolaris) on Amazon EC2 use Xen’s paravirtualized mode, and it is conjectured also that Windows instances do so. [5] [6] Upon provisioning instances, Xen provides each instance with its own vCPU(s) and associated ISA. The complexity of this step depends entirely on the architecture of the underlying pCPU(s). To this end, it is not clear what vCPU scheduler Amazon EC2 applies, but Xen’s default scheduler is the Credit Scheduler (as discussed in the Resource Virtualization: CPU module). Amazon EC2 is thought to have a modified version of the Credit Scheduler. [5]</p>
<p>Memory and I/O resources are virtualized in Xen in a way similar to that described in the Resource Virtualization: Memory module. First, Xen uses a two-level page mapping method. Second, the hardware page tables are allocated and managed by guest OSs, with a minimal involvement from the hypervisor. [5] Speciﬁcally, guest OSs can read directly from hardware page tables, but writes are intercepted and validated by the Xen hypervisor to ensure safety and isolation. For performance reasons, however, the guest OSs can batch write requests to amortize the overhead of passing by the hypervisor per every write request. Finally, with Xen, existing hardware I/O devices are not emulated as is typically done in fully virtualized environments. In contrast, I/O requests are always transferred from user instances to domain 0, and vice versa, using a shared memory communication paradigm as demonstrated in Fig. 1.25. At domain 0, device drivers of the host OS are borrowed to handle the I/O requests.</p>
<p><strong>References</strong></p>
<ol>
<li>H. Liu (2012). “Amazon Data Center Size.” <a href="http://huanliu.wordpress.com/2012/03/13/amazon-data-center-size/" target="_blank" rel="external">http://huanliu.wordpress.com/2012/03/13/amazon-data-center-size/</a> March 2012.</li>
<li>G. Coulouris, J. Dollimore, T. Kindberg, and G. Blair (May 2011). “Distributed Systems: Concepts and Design.” Addison-Wesley, 5 Edition.</li>
<li>Xen Open Source Community. <a href="http://www.xen.org" target="_blank" rel="external">http://www.xen.org</a>.</li>
<li>Amazon Elastic Compute Cloud. <a href="http://aws.amazon.com/ec2/" target="_blank" rel="external">http://aws.amazon.com/ec2/</a>.</li>
<li>F. Zhou, M. Goel, P. Desnoyers, and R. Sundaram (March 2011). “Scheduler Vulnerabilities and Attacks in Cloud Computing.” arXiv:1103.0759v1 [cs.DC].</li>
<li>C. Boulton (Serverwatch, 2007). “Novell.” Microsoft Outline Virtual Collaboration.</li>
</ol>
<h3 id="Amazon_EC2_Properties"><a href="#Amazon_EC2_Properties" class="headerlink" title="Amazon EC2 Properties"></a>Amazon EC2 Properties</h3><p>Amazon EC2 is designed using virutalization technology in order to meet certain performance, scalability, ﬂexibility, security, and reliability criteria.</p>
<p><strong>Elasticity</strong></p>
<p>In leveraging a major beneﬁt offered by virtualization, Amazon EC2 allows users to statically and dynamically scale up and down their EC2 clusters. In particular, users can always provision and deprovision virtual EC2 instances by manually starting and stopping any number of them using the AWS management console, the Amazon command line tools and/or the Amazon EC2 API. In addition, users can employ Amazon’s CloudWatch to monitor EC2 instances in realtime and automatically respond to changes in computing requirements. CloudWatch is an Amazon service that allows users to collect statistics about their cluster resource utilization, operational performance, and overall resource demand patterns. Metrics, such as CPU utilization, disk operations, and network trafﬁc, can be aggregated and fed to the Amazon’s Auto Scaling process enabled by CloudWatch. The Auto Scaling process can subsequently add or remove instances so that performance is maintained and costs are saved. In essence, Auto Scaling allows users to closely follow the demand curve of their applications and synergistically alter their EC2 clusters according to conditions they deﬁne (e.g., add three more instances to the cluster when the average CPU utilization exceeds 80%).</p>
<p><strong>Scalability and Performance</strong></p>
<p><img src="/images/14556012979969.jpg" alt=""></p>
<p><img src="/images/14556013387645.jpg" alt=""></p>
<p>Amazon EC2 instances can scale to more than 255 pCPUs per host, [1] 128 vCPUs per guest, 1TB of RAM per host, up to 1TB of RAM per unmodified guest, and 512GB of RAM per paravirtualized guest. [2] In addition, Amazon EC2 reduces the time needed to boot a fresh instance to seconds, thus expediting scalability as the needs for computing varies. To optimize performance, Amazon EC2 instances are provided in various resource capacities that can suit different application types, including CPU-intensive, memory-intensive, and I/O-intensive applications (see Table 1.2). The vCPU capacity of an instance is expressed in terms of elastic compute units (ECU). Amazon EC2 uses ECU as an abstraction of vCPU capacity, whereby one ECU provides the equivalence of a 1.0-1.2 GHz 2007 Opteron or 2007 Xeon processor. [3] Different instances with different ECUs provide different application runtimes. The performances of instances with identical type and ECUs may also vary as a result of what is called performance variation [4] [8] in cloud computing.</p>
<p><strong>Flexibility</strong></p>
<p>Amazon EC2 users are provided with complete control over EC2 instances, with a root access to each instance. They can create AMIs with software of their choice and apply many of Amazon services, including Amazon Simple Storage Service (Amazon S3), Amazon Relational Database Service (Amazon RDS), Amazon SimpleDB, and Amazon Simple Queue Service (Amazon SQS). These services and the various available Amazon EC2 instance types can jointly deliver effective solutions for computing, query processing, and provide storage across a wide range of applications. For example, users running I/O-intensive applications, such as data warehousing and Hadoop MapReduce, can exploit high-storage instances. On the other hand, for tightly coupled, network-intensive applications, users can utilize high-performance computing (HPC) clusters.</p>
<p>In addition, users have the flexibility to choose among multiple storage types that can be associated with their EC2 instances. First, users can rent EC2 instances with local instance-store disks as root devices. Instance-store volumes are volatile storage and cannot survive stops and terminations. Second, elastic block storage (EBS) volumes can be attached to EC2 instances, which provide the instances with raw block devices. The block devices can then be formatted and mounted with any file system at EC2 instances. EBS volumes are persistent storage and can survive any EC2 instance state, including stops and terminations. EBS volumes of sizes from 1GB to 1TB can be defined, and RAID arrays can be created by combining two or more volumes. EBS volumes can even be attached or detached from instances while they are running. They can also be moved from one instance to another, thus remaining independent of any instance. Finally, applications running on EC2 instances can access Amazon S3 through a defined API. Amazon S3 is a storage that makes Web-scale computing easier for developers, whereby any amount of data can be stored and retrieved at any time and from anywhere on the Web. [5] [6]</p>
<p><img src="/images/14556013776551.jpg" alt="Figure 3.33: Regions and availability zones in AWS cloud platform. Regions are geographically dispersed to avoid disasters. Availability zones are engineered as autonomous failure zones within regions."></p>
<p>To this end, Amazon EC2 users do not only have the flexibility of choosing among many instance and storage types but have the capability of mapping elastic IP addresses to EC2 instances without a network administrator’s help or the need to wait for DNS to propagate new bindings. Elastic IP addresses are static IP addresses but tailored for the dynamicity of the cloud. For example, unlike a traditional static IP address, an elastic IP address enables tolerating an instance failure by programmatically remapping the address to any other healthy instance under the same user account. Thus, elastic IP addresses are associated with user accounts and not EC2 instances. Elastic IP addresses exist until explicitly removed and persist even when accounts have no current running instances.</p>
<p><strong>Fault Tolerance</strong></p>
<p>Amazon EC2 users are capable of placing instances and storing data at multiple locations represented as regions and availability zones. As shown in Figure 1.27, a region can consist of one or many availability zones, and an availability zone can consist of many blade servers. Regions are independent collections of AWS resources that are dispersed geographically to avoid catastrophic disasters. An availability zone is a distinct location in a region designed to act as an autonomous failure zone. Specifically, an availability zone does not share a physical infrastructure with other availability zones, thus limiting failures from transcending its own boundaries. Furthermore, when a failure occurs, automated AWS processes start moving customer traffic away from the affected zone. Consequently, applications that run in more than one availability zone across regions can inherently achieve higher availability and minimize downtime. Amazon EC2 guarantees 99.95% availability per each Region. [3]</p>
<p>Last, EC2 instances that are attached to Amazon EBS volumes can attain improved durability over EC2 instances with local stores (or the so-called “ephemeral storage”). Amazon EBS volumes are replicated automatically in the backend of a single availability zone. Moreover, with Amazon EBS, point-in-time consistent snapshots of EBS volumes can be created and reserved in Amazon S3. Amazon S3 storage is automatically replicated across multiple availability zones, not only in a single availability zone. Amazon S3 helps maintain the durability of users’ data by quickly detecting and repairing losses. Amazon S3 is designed to provide 99.999999999% durability and 99.99% availability of data over a given year. [3] A snapshot of an EBS volume can also serve as the starting point for a new EBS volume in case the current one fails. Therefore, with the availability of regions and availability zones, the virtualized environment provided by Xen, and Amazon’s EBS and S3 services, Amazon EC2 users can achieve long-term protection, failure isolation, and reliability.</p>
<p><strong>Security</strong></p>
<p>Security in Amazon EC2 is provided at multiple levels. First, as pointed out earlier, EC2 instances are controlled completely by users. Users have full root access, or administrative control, over their instances, accounts, services, and applications. AWS does not have any access rights to user instances and cannot log into their guest OSs. [9] Second, Amazon EC2 provides a complete firewall solution, whereby the default state is to deny all incoming traffic to any user instance. Users must explicitly open ports for specific inbound traffic. Third, API calls to start/stop/terminate instances, alter firewall configurations, and perform other related functions are all signed by the user’s Amazon Secret Access Key. Without the Amazon Secret Access Key, API calls on Amazon EC2 instances cannot be made. Fourth, the virtualized environment provided by Xen provides a clear security separation between EC2 instances and the hypervisor as they run at different privileged modes. Fifth, the AWS firewall is placed in the hypervisor between the physical network interface and the virtual interfaces of instances. Hence, because packet requests are all privileged, they must trap to the hypervisor and accordingly pass through the AWS firewall. Consequently, any two communicating instances are treated as separate virtual machines on the Internet, even if they are placed on the same physical machine. Finally, because Amazon EBS volumes can be associated with EC2 instances, their accesses are restricted to the AWS accounts that created the volumes. This indirectly denies all other AWS accounts (and corresponding users) from viewing and accessing the volumes. We note, however, that this does not impact the flexibility of sharing data on the AWS cloud platform. In particular, users can still create Amazon S3 snapshots of their Amazon EBS volumes and share them with other AWS accounts/users. Nevertheless, only the users who own the volumes are allowed to delete or alter EBS snapshots.</p>
<p><strong>References</strong></p>
<ol>
<li>Xen 4.1 Release Notes. <a href="http://wiki.xen.org/wiki/Xen" target="_blank" rel="external">http://wiki.xen.org/wiki/Xen</a> 4.1 Release Notes.</li>
<li>Xen 4.0 Release Notes. <a href="http://wiki.xen.org/wiki/Xen" target="_blank" rel="external">http://wiki.xen.org/wiki/Xen</a> 4.0 Release Notes.</li>
<li>Amazon Elastic Compute Cloud. <a href="http://aws.amazon.com/ec2/" target="_blank" rel="external">http://aws.amazon.com/ec2/</a>.</li>
<li>B. Farley, V. Varadarajan, K. Bowers, A. Juels, T. Ristenpart, and M. Swift (2012). “More for Your Money: Exploiting Performance Heterogeneity in Public Clouds.” Heterogeneity in Public Clouds.</li>
<li>S. Ghemawat, H. Gobioff, and S. T. Leung (Oct. 2003). “The Google File System.” SOSP.</li>
<li>Hadoop. <a href="http://hadoop.apache.org/" target="_blank" rel="external">http://hadoop.apache.org/</a>.</li>
<li>J. Dean and S. Ghemawat (Dec. 2004). “MapReduce: Simplified Data Processing On Large Clusters.” OSDI.</li>
<li>M. S. Rehman and M. F. Sakr (Nov. 2010). “Initial Findings for Provisioning Variation in Cloud Computing.” CloudCom.</li>
<li>Amazon (May 2011). “Amazon Web Services: Overview of Security Processes.” Amazon Whitepaper.</li>
</ol>
<h3 id="Case_Study_Summary"><a href="#Case_Study_Summary" class="headerlink" title="Case Study Summary"></a>Case Study Summary</h3><ul>
<li>Many virtualization suites are available from multiple cloud vendors. They have differing architectures and features. Most suites support memory over commitment (which enhances server consolidation) as well as live migration (which allows VMs to be seamlessly moved across physical machines).</li>
<li>Amazon’s Elastic Compute Cloud (EC2) is Amazon Web Services’ primary IaaS offering.</li>
<li>Amazon Machine Images (AMIs) can be used to create a VM (or instance) in EC2. Dozens of AMIs are available for various environments and prebuilt software stacks.</li>
<li>EC2 instances can be created, launched, stopped, resumed, and terminated as needed. EC2 is believed to be using Xen as the hypervisor. Amazon’s software completely automates the process of creating instances, configuring their network and storage, and making them accessible to the user.</li>
<li>Amazon EC2 offers over 17 different instance types, each of which are configured with differing CPU, memory, local storage, and I/O specifications. EC2 instances typically boot in a minute to a few seconds, depending on the instance type, allowing for elastic scaling of compute resources on demand.</li>
<li>EC2 instances are flexible to be configured in any way the user requires because the user has full administrator privileges to the OS running in the instance. Amazon’s Elastic Block Store (EBS) service allows for raw block devices to be provisioned and attached or detached to any instance. Amazon also allows for elastic IPs (fixed IP addresses) that can be attached to any instance. All of Amazon’s services can be programmatically accessed using APIs, which allows for automated resource control and management.</li>
<li>EC2 instances can be launched in one of many regions and availability zones, allowing users to improve the fault tolerance and redundancy of their applications, if they wish to do so.</li>
<li>EC2 instances are secured through Amazon’s own configurable firewall, called security groups. The OS running in an instance is exclusively under the user’s domain and cannot be accessed by Amazon. Furthermore, Amazon employs strict security practices with public-key authentication to provide access to instances as well as API interfaces to various AWS services.</li>
</ul>
<h2 id="Storage_and_Network_Virtualization"><a href="#Storage_and_Network_Virtualization" class="headerlink" title="Storage and Network Virtualization"></a>Storage and Network Virtualization</h2><p>A typical data center architecture consists of many loosely integrated components including pools of servers (CPU and OS), storage arrays, a hierarchical physical network to interconnect these components, as well as power and cooling and a management system for each of these components. As we saw in earlier units, sharing of data center resources could happen by simply running applications or processes on the same set of servers. However, this provides minimal isolation and restricts all applications to adhere to a single software environment. Another approach to sharing can be where multiple tenants share the data center real estate by co-locating servers or racks of servers within the same physical space but sharing cooling, power and networking. Co-location offers improved isolation, however it also increased costs and lowers utilization.</p>
<p>As we read in earlier, with the advent of utility computing and better resource sharing technologies, such as server virtualization, it is possible for the applications and data of multiple tenants to co-exist within the same physical servers while maintaining a certain degree of isolation. This idea was achieved by adding a software layer that enables the sharing of physical resources. Since data centers also include large scale storage systems (SANs) and hierarchical networking fabric (switches and routers), could this same idea of utilizing a software layer by be applied to enable the sharing of storage and networking?</p>
<p>The success of server virtualization at achieving resource sharing, higher utilization, improved flexibility and elasticity, has led to the advent of the idea of Software Defined Data Center (SDDC). SDDC virtualizes all infrastructure in a manner which can be automated and easy to manage. A virtualized cluster, which includes servers, networking fabric and storage systems, can be decoupled from the physical resources and provided as software resources that can be configured and managed. Instead of building applications using dedicated servers, storage and networking resources, SDDC offers the data center infrastructure as software services since all of the needed resources can be virtualized.</p>
<p><a href="http://youtu.be/X2Ppt0MG6as" target="_blank" rel="external">Video 3.13: Software Defined Data Centers (SDDC).</a></p>
<p>The main technologies that enable an SDDC include:</p>
<ol>
<li>Server/Compute Virtualization</li>
<li>Software Defined Networking</li>
<li>Software Defined Storage</li>
<li>Management and Automation Software</li>
</ol>
<p>Earlier, we covered the mature server/compute virtualization technology in detail. In this module we will briefly introduce the main ideas behind the emerging technologies of Software Defined Networking (SDN) and Software Defined Storage (SDS). SDN and SDS can, for example, enable setting up an isolated network with dedicated bandwidth and latency requirements across multiple VMs and provisioning storage for an application with configurable bandwidth and latency requirements. As the cloud computing paradigm continues to evolve, we will see SDN and SDS play a role in enabling improved sharing, isolation, flexibility, QoS guarantees, and management of data center resources.</p>
<h3 id="Networking_within_Data_Centers"><a href="#Networking_within_Data_Centers" class="headerlink" title="Networking within Data Centers"></a>Networking within Data Centers</h3><p><strong>Data Center Networks</strong></p>
<p>Large data centers are part of the core infrastructure that supports cloud services. To be cost-effective, these data centers must have networks that are agile and easy to configure and update. In previous modules, we have already looked at some of the enabling physical technologies used in these networks, as well as a general multi-tier topology. In this module, we will look at some design considerations for the network within cloud data centers, as well as understand how and why Software-defined Networks (SDNs) have gained prominence in this domain.</p>
<p>Computer networks are a collection of nodes and links. A node can be any network equipment, such as a switch, router or server. A link is a physical or logical connection between two nodes in the network. Networks also comprise of identification resources (addresses) and labeling tags. Often, a mechanism for the management of the identification for devices (IP address and MAC address), links (flow ID), and networks (VLAN ID, VPN ID) is needed for the management and monitoring of a virtualized infrastructure. High-level organizational structures such as a network topologies can be created by assembling these resources. The topics we discuss below are extremely large and complex, this page merely introduces them to motivate the need for network virtualization within cloud data centers.</p>
<p><strong>Data Center Networks: Challenges and Design Considerations</strong></p>
<p>Designers of large data-center networks have to contend with several (sometimes contradictory) requirements [1] . They must:</p>
<ol>
<li>ensure that the topology used is scalable to cope with future demand,</li>
<li>maximize throughput while minimizing hardware cost,</li>
<li>ensure that their design guarantees availability and integrity of the system despite failures, and</li>
<li>have power-saving features to reduce operating costs (and be environment-friendly)</li>
</ol>
<p>An important consideration in designing the network for a large data center relies on selecting the right network fabric (or combination), amongst Ethernet, InfiniBand and other high-speed fabrics like Myrinet. Each one has different cost, latency, bandwidth and communication criteria and must be chosen carefully. A physical comparison of these fabrics is not within our scope. What interests us more is the topology and addressing scheme selected to interconnect these resources. For instance, if we choose to use an Ethernet-addressed network (where endpoints are identified based on a flat 6-byte MAC address), it is likely that such a network will be flat, where each interface is assigned a MAC address that does not depend upon its location. This makes routing difficult and forwarding tables large, since all addresses must be stored.</p>
<p>For this reason, we mostly rely on hierarchical routing to build scalable networks. As we discussed earlier, most data centers rely on top of rack (ToR) switches interconnected through end of row (EoR) switches. Addressing and routing is often based on IP-based routing, where each endpoint is assigned an IP address based on its location in the hierarchy. However, this introduces restrictions in a cloud data center, by limiting the mobility of a Virtual Machine (VM)- migrating a VM from one host to another must be handled using a different policy. Also, as we have seen in our discussion about security, it is often problematic to allocate IP addresses deterministically in a public cloud environment, as it reveals information about colocation. The selection of data center network topologies is an active area of research, with the focus on both fixed and flexible topologies which could be tree-based or recursive.</p>
<p>Apart from the topology and addressing scheme, it is important to decide on the routing mechanism. Routing may be centralized, where a single central controller creates lookup tables which determine the forwarding action. This is theoretically optimal, since the central controller has complete visibility of the data center network, and makes it easier to configure and to understand the impact of faults. However, the controller introduces a bottleneck and single point of failure, and results in a substantial overhead in propagating forwarding tables. Instead of centralized routing, distributed approaches may be used where decisions are based upon local information at each router and switch.</p>
<p>Traffic within large data centers may be carefully engineered to reduce congestion and latency. To do this, groups of packets are categorized as “flows” if they are sequentially or logically related. The routing protocols above perform load-balancing by distributing these as a flow between two nodes among multiple parallel paths. The design of a data center must be optimized for the specific flow patterns within it, which is why the ability to analyze traffic is important for a data center designer. Routing protocols must be state-aware, so that idle routes are favored over busy ones, and flows are dispersed over multiple parallel paths.</p>
<p>Finally, data center networks must be designed for fault-tolerance. Such networks often use gossip protocols, where neighbors talk to each other to disseminate information about failures quickly. It is important to design such mechanisms to not use a large amount of bandwidth. There must also be mechanisms to recover from failure and re-incorporate failed components within the network.</p>
<p><strong>Cloud Data Center Networks: Need for Virtualization</strong></p>
<p>By now, it should be clear that implementing a large data center network is extremely complex and needs a higher level abstraction to be easy to design and configure. However, the situation is even more complex for cloud data centers, largely due to multi-tenancy requirements. The cloud computing paradigm is only relevant if it can ensure isolation amongst multiple tenants. A cloud service provider (CSP) must ensure traffic isolation, so that one tenant’s traffic is not visible to another. The address space must also be isolated, so that each tenant has access to their own private address space.</p>
<p>Both traffic and address-space isolation are achieved by building “virtual networks” for each tenant, and traffic between these networks is restricted to a few strictly defined points. These virtual networks are generally built as an “overlay” on top of the real (physical) network. We refer to network virtualization as the process of provisioning these overlays, associating them with the tenant’s network interfaces and maintaining the lifecycle of this network as VM instances are launched, stopped or terminated. [2]</p>
<p>Overlay network : A virtual network in which the separation of tenants is hidden from the underlying physical infrastructure, such that the underlying transport network does not need to know about the different tenants to correctly forward traffic.</p>
<p>One of the important benefits of virtualizing the network is that it allows VM migration between hosts while retaining its network state (IP and MAC addresses). Changes in MAC addresses can cause many unexpected disruptions, for e.g. by invalidating software licenses. Thus, physical hosts can be assigned IP addresses hierarchically, whereas VMs can have an IP address that is within a pool of valid addresses for that subnet.</p>
<p>Another driver for virtualization is the increased complexity in maintaining forwarding tables. Instead of a single MAC address per physical server, cloud data centers will have to maintain the MAC address of upto hundreds of VM instances per server, which leads to significant demand on the forwarding node’s capacity.</p>
<p>Virtualization also helps provide individual tenants with control over the addresses they use within their view of the network.Thus, the overlay network must provide tenants access to use any address that they want, without having to check the networks of all of the neighboring tenants. These addresses should also be independent of the addresses used by the CSP’s infrastructure. Virtual networks use this address separation to limit the scope of packets that are sent on it. Packets are allowed to cross network boundaries only through controlled exit points.</p>
<p>Finally, the presence of multiple tenants and the overcommitting of the shared network bandwidth leads to a traffic and flow management challenge. CSPs must ensure a QoS in line with the guaranteed SLAs and must shape traffic from each tenant according to the peak utilization provisioned.</p>
<p><strong>Types of Network Virtualization</strong></p>
<p>As we have seen above, network virtualization is simply a sharing mechanism that allows multiple isolated virtual networks to use the same physical network infrastructure. This allows virtual networks to be dynamically allocated and deployed on-demand precisely like VMs in virtualized servers [3] .</p>
<p><img src="/images/14559995587136.jpg" alt="Figure 3.32: Types of network virtualization"></p>
<p>Network virtualization is a broad term that encompasses many different techniques. For e.g. traditional VPNs and VLANs are types of datapath virtualization, where the a physical link is extended virtually. Cloud data centers rely on a combination of all of these virtualization techniques to build a scalable, flexible and agile network. Virtual machines have virtualized Network Interface Cards, which bridge a unique virtual MAC address to the physical NIC. Router virtualization enables the creation of multiple tenant virtual networks, based on “map-and-encap”, where edge routers map the packet to the destination, then encapsulate packets within a network tunnel which are only decoded at the target node.</p>
<p>Bandwidth and physical channel virtualization are achieved by sharing slots within the network using traditional techniques like TDM/FDM and circuit switching. Datapath virtualization allows packets to travel along a programmable path, allowing flexible flows and traffic management. In the coming pages, we will also explore SDNs, which are an alternative to simple network virtualization.</p>
<p>In conclusion, cloud data centers rely on a group of techniques to decouple network services from the hardware network, making it easy to programmatically configure and deploy them.</p>
<p><strong>References</strong></p>
<ol>
<li>Liu, Yang and Muppala, Jogesh K and Veeraraghavan, Malathi and Lin, Dong and Hamdi, Mounir (2013). “Data Center Networks: Topologies, Architectures and Fault-Tolerance Characteristics.” Springer Science and Business Media.</li>
<li>Liu, Yang and Muppala, Jogesh K and Veeraraghavan, Malathi and Lin, Dong and Hamdi, Mounir (2014). “Problem statement: Overlays for network virtualization.” RFC 7364.</li>
<li>Wen, Heming and Tiwary, Prabhat Kumar and Le-Ngoc, Tho (2013). “Wireless virtualization.” Springer.</li>
</ol>
<h3 id="Software_Defined_Data_Centers_and_Networking"><a href="#Software_Defined_Data_Centers_and_Networking" class="headerlink" title="Software Defined Data Centers and Networking"></a>Software Defined Data Centers and Networking</h3><p><strong>Network Virtualization</strong></p>
<p>An application stack generally includes firewalls, load balancers, web servers, app servers, databases etc. Traditionally, these resources used to run on separate network segments as they needed to communicate with each other using a backplane network before responding back to the external user. In traditional data centers, the network was segmented using <a href="http://en.wikipedia.org/wiki/Data_link_layer" target="_blank" rel="external">Layer 2</a> Virtual LANs (VLANs), which are a mechanism to restrict the broadcast domain of each LAN segment and partition the network.</p>
<p>Unfortunately, provisioning a VLAN to create a network segment is a highly manual task, which involves configuring a hypervisor <a href="http://searchservervirtualization.techtarget.com/definition/virtual-switch" target="_blank" rel="external">vSwitch</a> as well as the physical ports for all the switches in the network segment. Due to this manual configuration, VLAN scalability is poor (Most commercial network switches offer only a maximum of 4K VLANs). Server Virtualization led to an explosion in the number of virtual machines, which in turn increased the complexity of network infrastructure and services. Manual provisioning becomes almost impossible due to the scale and complexity which arises as a result of VM features like dynamic provisioning, placement and migration.</p>
<p>The complexity of these networks led to increased complexity of switching intelligence, which made networking hardware more expensive. It also led to vendor lock-in as most routing infrastructure has poor interoperability. Traditional networks lacked well-defined open interface standards. Finally, data centers were constrained by the properties of traditional network protocols, which did not adapt well to the dynamic relocation of resources, or the colocation of isolated servers.</p>
<p>Network Virtualization technology, and Software-Defined Networking provide a solution to this problem of many VMs, which can be placed dynamically and migrated in real-time within the data center. A virtualized network includes features like dynamic creation, deletion, migration, configuration, snapshotting, and roll-back of state.</p>
<p><strong>Software Defined Networking</strong></p>
<p>Software-Defined Networking is an approach to computer networking that decouples the data plane (that forwards packet in the hardware layer) from the control plane (which decides the packet forwarding rules). SDNs use a centralized controller, which programs the data plane using well-defined APIs to modify the network flow. See Figure 3.33 to understand the various pieces of the stack.</p>
<p><img src="/images/14559996858397.jpg" alt="Figure 3.33 : SDN Component Planes"></p>
<p><a href="http://youtu.be/rVxiiJKQ73U" target="_blank" rel="external">Video 3.14: Software Defined Networking</a></p>
<p><strong>SDN Architecture</strong></p>
<p>SDNs are remarkably flexible; they can operate with different types of switches and at different protocol layers. SDN controllers and switches can be implemented for Ethernet switches (Layer 2), Internet routers (Layer 3), transport (Layer 4) switching, and even at the application layer (Layer 7). SDN relies on the common functions found on networking devices, which essentially involve forwarding packets based on some form of flow definition.</p>
<p>From the point of view of an individual switch, a flow is a sequence of packets that matches a specific entry in a flow table. A Flow Table matches incoming packets to a particular flow and specifies the functions that are to be performed on the packets. A combination of Flow Table entries on multiple switches binds a flow to a path.</p>
<p>In an SDN architecture, a virtual and/or physical switch performs the following functions:</p>
<ol>
<li>The switch encapsulates and forwards the first packet of a flow to an SDN controller, enabling the controller to decide whether the flow should be added to the switch flow table.</li>
<li>The switch forwards incoming packets out the appropriate port based on the flow table. The flow table may include priority information dictated by the controller.</li>
<li>The switch can drop packets on a particular flow, temporarily or permanently, as dictated by the controller, for security purposes, curbing Denial-of-Service (DoS) attacks or traffic management requirements (congestion control).</li>
</ol>
<p>The SDN controller manages the forwarding state of the switches using APIs that allow the controller to address a wide variety of operator requirements without changing any of the lower-level aspects of the network, including topology.</p>
<p>With the decoupling of the control and data planes, SDN enables applications to deal with a single abstracted network device without concern for the details of how the device operates. Network applications see a single API to the controller. Thus it is possible to quickly create and deploy new applications to orchestrate network traffic flow to meet specific enterprise requirements for performance or security.</p>
<p>The centralized SDN controller can be used to effectively provision, manage and tear down virtual networks over a physical <a href="http://en.wikipedia.org/wiki/Switched_fabric" target="_blank" rel="external">IP fabric</a>. The SDN controller provides a standardized API to a cloud orchestrating application to program the network elements in a data center to provide on demand virtual networks. In case a VM is migrated, the SDN controller will take care of modifying the flow tables in the network elements to re-establish the virtual network. This kind of flexibility cannot be provided if the configuration is done manually.</p>
<p>Using these techniques, SDNs create a logical overlay network that isolates the network components between different tenants, while simultaneously allowing VMs from the same tenant to exist on the same Layer 2 broadcast domain. This is allowed even if both devices physically reside in different data centers. By abstracting away the physical network, Cloud Providers can then allocate both private and public (Internet-facing) IP addresses to VMs, and even allow them to be dynamically re-located, without requiring any manual reconfiguration.</p>
<p><strong>Isolation using SDNs</strong></p>
<p>Having a central controller allows the control plane to have a global view. This reduces the need for complex protocols like Spanning Trees to compute paths. Instead a simple shortest-path algorithm (for e.g. Dijkstra) can be used to compute the topology. This removes complexity from the central plane, allows fast reconfiguration of network architectures, add and update security rules, all using a simple API.</p>
<p>Isolation between tenants is a critical SDN application on the cloud. By specifying separate network flows depending upon the tenant, multiple virtual networks can be overlaid on a single physical network. These virtual networks can even have overlapping IP address space. Consider the case with two cloud tenants– Yellow and Red, colocated on the same rack in the datacenter (Figure 3.34). They can share the same underlying infrastructure, while being isolated using a higher-level abstraction.</p>
<p><img src="/images/14559998650927.jpg" alt="Figure 3.34 : A Logical View of a Network Overlay"></p>
<p>One way to achieve isolation between the Red and Yellow tenant network is by using network tunneling. Typically, the following steps takes place for tunneling traffic,</p>
<ol>
<li>The application running on the VM considers it attached to a LAN segment and sends out an ethernet frame on the virtual network interface (VNI)</li>
<li>The vswitch (virtual switch in the hypervisor) receives the packet from the VM and encapsulates it. The encapsulation method (Figure 3.35) may be any of the following,<ul>
<li>VXLAN (Virtual Extensible LAN)</li>
<li>NVGRE (Network Virtualization using Generic Routing Encapsulation)</li>
</ul>
</li>
<li>The kernel ip-stack adds the destination mac and ip address of the target hypervisor and sends it out on the physical ip fabric.</li>
<li>At the destination, the kernel ip-stack strips the outer mac and ip address and reads the encapsulation headers to determine the destination vm and delivers the packet to the VNI of the destination VM.</li>
</ol>
<p><img src="/images/14559999035156.jpg" alt="Figure 3.35: Encapsulation Methods"></p>
<p>The overall flow of a packet from the source application to the destination application is illustrated in Figure 3.36:</p>
<p><img src="/images/14559999215584.jpg" alt="Figure 3.36: Packet Stages in Virtualized network"></p>
<p>Software defined networking comes into the picture in steps 2 and 3 of the above tunneling process. The software running on the hypervisor vswitch, inspects the packets and requests the centralized SDN controller to add a flow specification for the flow. The flow specification is used to match the {VM, Source Address, Destination Address} tuple to a flow action that adds the Encapsulation Header and the outer MAC and IP header as shown in Figure 3.35. The SDN controller knows what outer header to add because it has a global view of all the resources and the software on the controller route the flows accordingly.</p>
<h3 id="Storage_within_Data_Centers"><a href="#Storage_within_Data_Centers" class="headerlink" title="Storage within Data Centers"></a>Storage within Data Centers</h3><p><strong>Typical Data Center Storage Systems</strong></p>
<p>In previous units, we discussed the various types of on-line storage devices that are in use - primarily magnetic and solid-state drives. We also looked at how these devices can be deployed in a number of ways in a data center. They are reproduced here for your convenience:</p>
<p><img src="/images/14559999542241.jpg" alt="Figure 3.37: Individual Server Storage vs. Shared / Enterprise Storage"></p>
<ol>
<li>Individual Server Storage: This is a typical storage architecture that you are familiar with, where individual disks are present on each server. This type of configuration is usually the cheapest option as it does not require any specialized hardware or networking devices, but also requires the most amount of management; Disk failures may cause data loss and/or loss of availability unless the server applications are explicitly replicating data and are configured for fault tolerance. Due to the lower cost of hardware, this type of storage is typical in large-scale data analytics clusters that run frameworks such as Hadoop or Spark, where fault tolerance and reliability are dealt with in the software and cluster architecture.</li>
<li>Shared/ Enterprise Storage: This refers to storage architecture that is decoupled from individual servers, typically through the use of a dedicated storage servers, arrays or appliances. These systems provide shared storage (either as block devices, file systems or object storage systems) to multiple servers. These systems are connected to the servers using either a regular Ethernet network, or using dedicated storage network fabrics (such as Fiber Channel or iSCSI over Ethernet). The benefit of this approach is the loose coupling of servers and storage, allowing for the components to be individually configured and upgraded as needed. This is a popular approach used with many applications, including database-driven applications, small to medium web services and IaaS cloud providers. Specifically for IaaS services, the ability to spin up a virtual machine with attached virtual disk images is made much simpler by the use of shared storage systems.</li>
</ol>
<p>Typically, enterprise grade storage systems are specified, configured and deployed based on the needs of an application. For example, a content delivery server may require a storage system that is vastly different from one that meets the requirements of a transactional database for a bank, for example. A content delivery server may require higher bandwidth to transfer large amounts of data to clients (such as photos or video), while a transactional database for a bank requires fast, low-latency transactions (which are small amounts of data), with strong backup and persistence guarantees. Traditional infrastructure deployments would include storage specifications that are optimized to the needs of the application deployed on the infrastructure.</p>
<p>With the advent of cloud computing and multi-tenancy in the data center, different users may have very different storage needs. However, they are restricted to the service offerings of the cloud provider, which may not have enough granularity to express the exact requirements of the user’s applications. Users may specify service-level objectives (SLOs) with regards to storage such as a certain capacity and latency or bandwidth requirement of their application. In addition, these requirements can change dynamically as the application is running; users would like the flexibility to vary their storage requirements based on their demand. For example, in Amazon Web Service’s Elastic Block Storage service, users can specify a specific amount of provisioned IO Operations per Second (IOPS) that they expect to get from the virtual disk, users can, for a price, demand higher throughput and lower latency from storage services. However, designing systems that can dynamically serve these types of resource requests is challenging.</p>
<h3 id="Software_Defined_Storage"><a href="#Software_Defined_Storage" class="headerlink" title="Software Defined Storage"></a>Software Defined Storage</h3><p>As you have already seen, different types of storage systems can be deployed based on the needs of an application. The most basic form of storage is direct-attached storage, wherein storage devices such as Magnetic disks or SSDs are directly attached to a server internally (through an internal bus connection, typically SATA or SAS). On the other hand, massive, enterprise grade storage systems can be used in the form of a Storage Area Network (SAN), which use a network (using different technologies) to connect servers to a pool of disks.</p>
<p>Recall that in clouds, physical compute resources are shared among multiple tenants through virtualized servers. This approach could also apply to storage resources as well. Storage resources can be shared among multiple tenants, allowing for the lowering of cost and improving overall utilization of these resources. However, a form of isolation should be provided along with fault tolerance and other technologies to meet a specific application’s SLOs. This enables clients to have predictable behavior of their applications, while sharing storage resources with other tenants. Providing end-to-end SLOs would translate into requirements on bandwidth, Input/Output Operations Per Second (IOPS), priority or control over the entire IO path (Figure 3.38).</p>
<p><img src="/images/14559999962819.jpg" alt="Figure 3.38 : An example IO Path"></p>
<p>Software Defined Storage (SDS) allows clients to specify fine-grained capacity, latency and/or bandwidth requirements, in the form of SLOs, which are mapped to abstract storage services for the client. The cloud provider can assemble and provide a scalable storage service using various storage technologies in the back-end. Client SLOs can be met by managing the storage stack at the provider’s side.</p>
<p><a href="http://youtu.be/-JxZM7Y_8sU" target="_blank" rel="external">Video 3.15: Software Defined Storage</a></p>
<p>To better understand how this can be achieved, let’s look at one of the emerging technologies that enable SDS, IOFlow [1] .</p>
<p><strong>An Example of SDS: IOFlow</strong></p>
<p>In data centers, the operation of an application requesting file operations and the storage system fulfilling the operation is complex involving several stages such as the host OS, the hypervisor, the network fabric, the storage server and finally a disk operation. The request also appears differently at each layer. For example, a file IO request like a read, write, or create in a VM results in a block IO request in the hypervisor. This, in turn, results in Ethernet packets across the network, and finally another file IO request and block device request at the storage server.</p>
<p>The number of layers in the system means that enforcing an end-to-end flow policy (these policies are typically derived from an SLO) is hard. It requires layers along the IO path to treat requests differently based on their content and/or client SLOs. Further, policies may need to be enforced at one or more or all layers along the path. For example, prioritizing IOs from a specific VM to storage requires configuring the prioritization on all layers along the path.</p>
<p>IO­Flow is a software defined storage architecture, designed by Microsoft, to maintain end-to-end SLOs, bandwidth SLOs and also improve performance. The clients are VMs running on compute servers through a hypervisor. These VMs need access to the storage server which is regulated by IO-Flow. Data flow is achieved by the use of queues at each stage in the IO path. These queues are regulated by a centralized controller. The centralized controller in the IOFlow architecture assigns data rate metrics and the route at each stage in the overall architecture.</p>
<p>An example of a typical VM to storage server IO path is presented in Figure 3.39 below. A compute server, which consists of a number VMs running simultaneously interacts with a storage server. Thus, the application interaction with a filesystem within a VM becomes block device requests at the virtual hard disk (VHD). In this specific example, the virtual hard disk is actually a remote storage server serving files using Microsoft’s SMB protocol. Therefore, IO requests further get translated into a network packet at the hypervisor’s network driver and sent across to the remote storage server through the network. In the storage server, the network packet is unwrapped at each layer, transforming into an SMB request, which in turn translates to a file system request at the storage server. At each layer in this IO path, IOFlow adds a queuing abstraction, which is then exposed to the IOFlow controller. The controller can translate these policies into lower level queuing rules at each stage.</p>
<p><img src="/images/14560000701308.jpg" alt="Figure 3.39 : IOFlow Example (Figure from [1] )"></p>
<p>The IOFlow controller regulates IO traffic in the following manner:</p>
<ol>
<li>The IOFlow controller obtains a graph of the entire network under it, including where the compatible stages are located.</li>
<li>Policies (explained below) supported by the controller are chosen by the client.</li>
<li>Based on the client’s policy, queues are formed and configured at intermediate stages.</li>
<li>When IO traffic passes through these intermediate stages, the IO header is recognized and the respective policy is implemented.</li>
</ol>
<p>IOFlow implements flow identifiers, which are basically IO headers attached to individual IO requests, these low level headers are attached to the IO as the high level headers and cannot be recognized by the intermediate stages. Once the intermediate stages identify a header they use it as routing information, to route the IO to the next hop.</p>
<p>As an example of how SDS can use policies to regulate IO traffic, IOFlow currently presents the following five flow policies:</p>
<p><img src="/images/14560001192193.jpg" alt=""></p>
<p>For P1, every VM is promised a minimum bandwidth to access one or more storage servers. However, in P2,if there is extra bandwidth available, as a result of one of the peer VMs having gone idle, the bandwidth available to VM1 can change dynamically. P3 requires traffic to be routed to a layer for sanitization. As an example, a new untrusted VM can have its traffic routed to a malware detection system. P4 can ensure low latency operations by assigning high priority to VM2 storage traffic. P5 allows a bandwidth guarantee for a set of VMs that need to access specific storage servers. The policies P1 through P4 are specified for point-to-point flows, while P5 policies are specified for multiple flows.</p>
<p>SDS is a fast evolving approach to providing storage as a service which meets a client’s SLOs through shared storage systems. As mentioned above, sharing without end-to-end SLO guarantees will not be readily adopted. IOFlow is an early example of an SDS technology. We expect the innovation in the SDS space to continue at a rapid pace bringing with it new solutions that will attempt to meet new cloud application requirements.</p>
<p><strong>References</strong></p>
<ol>
<li>Thereska et al. (2013). “IOFlow: A Software-Defined Storage Architecture.” SOSP’13: The 24th ACM Symposium on Operating Systems Principles.</li>
</ol>
<h3 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>A Software-defined Data Center (SDCC) automates and makes infrastructure easy to manage by virtualizing all components within a cluster, including servers, networking fabric and storage systems.</li>
<li>Software-defined Networking (SDN) and Software-defined Storage (SDS) are emerging technologies that are showing promise in improving the flexibility of data center design, while also improving utilization and isolation.</li>
<li>Data center network designers have to carefully select the topology, addressing scheme, traffic shaping policies, and fault-tolerance mechanisms.</li>
<li>In multi-tenant networks, there is a strong need for virtualization, as overlay networks are constructed to allow tenants to flexibly share network resources.</li>
<li>Virtual networks can be provisioned, migrated, reconfigured and snapshotted just like virtual machines.</li>
<li>Software-defined Networking (SDN) is another alternative to basic virtualization, whereby the data plane is decoupled from the control plane of the network.</li>
<li>SDN may rely on central controllers to make routing decisions, and help in tunneling packets by adding appropriate header information based on a global view.</li>
<li>Cloud data centers rely on shared storage mechanisms to quickly create volumes for many virtual machines. The sharing is generally done over a high-speed network fabric.</li>
<li>SDS is a recent approach to provide storage as a service that meets a client SLOs. An example of an SDS system is IOFlow, which allows the application of policies such as data rate metrics or malware detection on IO traffic.</li>
</ul>
<h1 id="Cloud_Storage"><a href="#Cloud_Storage" class="headerlink" title="Cloud Storage"></a>Cloud Storage</h1><h2 id="Cloud_Storage-1"><a href="#Cloud_Storage-1" class="headerlink" title="Cloud Storage"></a>Cloud Storage</h2><p>We now move on to the next part of the course, namely Cloud Storage. Storage is an important aspect of cloud computing and has seen a lot of innovation in the last decade or so. In fact, Amazon’s first public cloud service was the Simple Storage Service (S3).</p>
<p>We will begin with a description of the types of and characteristics of data. We will then move to typical applications and thier requirements on storage systems.</p>
<p>We will then explore storage devices and recap the various types of devices that are currently used in storage systems. Next, we visit the various abstractions that are used to organize data - either as blocks on disk, files within a file system or as objects in a database. We will then explore each of those abstractions in detail.</p>
<p>The first abstraction we will dive into is file systems. We will discuss the different types of file systems as well as their design considerations. We will then explore databases, consider the various design choices made when creating database systems and discuss the three main types of databases that are popular today: Relational, NoSQL and NewSQL databases. We will finally end this unit with a discussion on a class of cloud specific storage systems, namely object storage as a cloud service.</p>
<h3 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h3><p>Data is simply a collection of raw facts and figures. Applications are responsible for generating, storing, analyzing and consuming data, or some combination thereof.</p>
<p>The nature and properties of data typically influence the design and implementation of storage systems. Some of the properties include volume, content, and the frequency of access of data. As an example, Facebook [2] recently studied the access patterns of image and video content posted by its users to find that the access rate exponentially decreases as time passes. They used these findings to design and implement a storage system specific to their needs. In video 4.1, we will look at the various properties of data that influence the design of storage systems.</p>
<p><a href="http://youtu.be/Y0C5tA4TV0g" target="_blank" rel="external">Video 4.1: Characterizing Data</a></p>
<p><strong>Structure of Data</strong></p>
<p>Data can be categorized using its dynamicity and structure. Specifically, data can be broadly segmented into one of the four quadrants in Figure 4.1. One categorization represents the structure of the data, which is considered as either structured or unstructured.</p>
<p><img src="/images/14567747998874.jpg" alt="Figure 4.1: Segmenting data into various types [3]"></p>
<p>Structured data have a predefined data model that organizes the data in a form that is relatively easy to process, store, retrieve, and manage. Structured data are usually small data that naturally fit in tabular form and hence can easily be stored in traditional databases (e.g., relational databases). An example of structured data is customers’ contact information that is stored in tables in a customer relationship management (CRM) database. These data fit in a fairly rigid model (called schema in relational databases), which can be quickly stored, accessed, and manipulated.</p>
<p>Unstructured data, on the other hand, may not necessarily have a predefined, rigid organizational model. Unstructured data may be larger and may not fit naturally in tabular form, making the data unsuitable for storage in a relational database. Thus, unstructured data may be relatively difficult to organize in a form that is simple to process, store, retrieve, and manage. Examples of unstructured data are flat binary files containing text, video, or audio information. It is important to note that unstructured data is not necessarily devoid of structure; a document, video, or audio file may have a file encoding structure or metadata with which it is associated. Hence, data with some form of structure may still be characterized as unstructured if their structure is not helpful to the processing task for which the data are needed. To illustrate, a large cache of text documents (which are unstructured) is difficult to index and search when compared to a relational database containing customer information (which is structured). For the purposes of this course, unstructured data can be defined as data that do not fit naturally in a relational database. In addition, some data may be treated as unstructured (not stored in a database) because they will be accessed using unpredictable access patterns; traditional database optimizations are pointless for such data.</p>
<p>There is a type of data that lies between structured and unstructured, referred to as semi-structured data. Semi-structured data does not conform with the formal structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data. Data that is described using markup languages, for example web pages, clickstream data, web objects are examples of semi-structured data. XML and JSON are classic examples of representations of semi-structured data as they inline data with tags that describe the data as well.</p>
<p><strong>Dynamicity of Data</strong></p>
<p>Another characterization is the dynamicity of the data, which is an indication of how often the data changes. Dynamic data, such as Microsoft Office documents and transactional entries in a financial database, change relatively frequently, whereas fixed data, once created, may not be altered. Examples of fixed data include medical imaging data from MRI and CT scans and broadcast video footage that is archived in a video library.</p>
<p>Segmenting data into one of these quadrants helps in designing and developing a storage solution for the data. Structured data are typically processed using relational databases in which the data can be accessed, managed, and manipulated using precise commands (typically issued in a query language such as SQL). Unstructured data may be stored in flat files in a file system or may be further organized using a NoSQL database (more on NoSQL later in the module).</p>
<p>The structure and dynamicity of data provide guidance on how a storage system can be architected. Large amounts of data that are relatively static can be stored on disk arrays if they are read frequently. Storage systems designed with multi-tiered caching architecture (such as multi-tiered with caching) improve the performance of read operations on such data.</p>
<p>Certain types of file systems, such as earlier versions of the Hadoop Distributed File System (HDFS), are designed for relatively static data. They allow a file to be written only once, and the file cannot be modified after it is written. Static data, such as drive images and snapshots for backups, can be archived on relatively inexpensive offline storage systems if they do not need to be accessed frequently.</p>
<p>In summary, the nature of data used by an application must be considered before its appropriate storage architecture is chosen.</p>
<p><strong>Granularity and Volume of Data</strong></p>
<p>In addition to the type of data, the volume of data that needs to be stored and processed for a particular application must be considered. The volume of data is characterized along two dimensions, the overall size of data (total volume), versus the size of a useful segment of the data (the granularity of data). As an example, consider the case of a photo sharing website that has millions of users posting tens to hundreds of photos to the website. The total size of the data may be tens or hundreds of terabytes or even petabytes, but the average photo may be a few megabytes. Contrast this to a website like youtube, where the total size of all the videos in youtube is many petabytes, and the size of an video can range from a few hundred megabytes to even gigabytes in size.</p>
<p>In this regard, we touch on an often-used term to describe large volumes of data: big data. There are many definitions of big data, but one popular description summarizes it as data is too big to be handled using conventional techniques.</p>
<p>The rapidly expanding information and communications technology (ICT) that is permeating all aspects of modern life has led to a massive explosion of data over the last few decades. Major advances in the connectivity and digitization of information has led to increasing amounts of data being created on a daily basis. These data are diverse, ranging from images and videos from mobile phones being uploaded to websites such as Facebook and YouTube, to 24/7 digital TV broadcasts, to surveillance footage from hundreds of thousands of security cameras, to large scientific experiments such as the Large Hadron Collider—all of which produce many terabytes of data every day. International Data Corporation’s (IDC) latest Digital Universe Study predicts a 300-fold increase in the amount of data created globally from 130 exabytes (1028) in 2012 to 30,000 exabytes in 2020 (Figure 4.2).</p>
<p><img src="/images/14567748621340.jpg" alt="Figure 4.2: Predicted growth of data from 2009 to 2020 [1]"></p>
<p>Organizations are trying to leverage or, in fact, cope with the massive amounts of data that are growing at ever-increasing rates. Google, Yahoo!, and Facebook have gone from processing gigabytes and terabytes to the petabyte range, which puts immense pressure on their computing infrastructures that need to be available 24/7 and must scale seamlessly as the amount of data produced rises exponentially. These are the challenges to which present and future storage technologies must respond.</p>
<p><strong>References</strong></p>
<ol>
<li>John Gantz and David Reinsel (2012). “The Digital Universe in 2020.” IDC White paper.</li>
<li>Subramanian Muralidhar, Wyatt Lloyd, Sabyasachi Roy, Cory Hill, Ernest Lin, Weiwen Liu, Satadru Pan, Shiva Shankar, Viswanath Sivakumar, Linpeng Tang and Sanjeev Kumar (2014). “f4: Facebook’s Warm BLOB Storage System.” 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14). USENIX Association. 383-398 Pages.</li>
<li>Thomas Rivera (2012). “The Evolution of File Systems.” SNIA Tutorial.</li>
</ol>
<h3 id="Applications_and_Requirements"><a href="#Applications_and_Requirements" class="headerlink" title="Applications and Requirements"></a>Applications and Requirements</h3><p>Applications can deal with data in many ways. Some applications serve to be storage and retrieval systems of data. They may archive data for a variety of reasons. Backup software archives user data in a snapshot fashion, allowing the user to restore the state their computer and files in the event of a hardware failure or accidental deletion. Large websites such as archive.org crawl popular websites and preserve a snapshot allowing users to view the previous versions. A retailer may want to store information regarding every individual transaction for accounting and tax purposes.</p>
<p>Other applications retrieve data to enable decision making. One example of this is business intelligence systems. A business intelligence system can process millions of transactions and infer sales trends. This information could be used to enable inventory and marketing decisions to be made by the retailer.</p>
<p>Applications can also extract knowledge for analysis. As an example, Google [1] recently discovered that search trends of certain keywords is highly correlated with physician visits for influenza-like visits. This enables Google to publish information about flu trends that is updated daily.</p>
<p>Data can also enable a service. Almost any type of dynamic web service which responds to user requests is an example of this. Specific instances include: mapping and navigation software. By collecting information about road networks and addresses, a system (such as Google Maps) can respond to direction-related queries.</p>
<p><strong>Application Requirements</strong></p>
<p>Different applications have different requirements of storage systems. Netflix, for example needs to serve high-bandwidth video to millions of users across the countries in which they operate in. Google search, on the other hand, must analyse a query and retrieve accurate results for that query within a very short period of time. In this section, we will look briefly at the various requirements imposed by applications on storage systems:</p>
<ol>
<li>Capacity: Storage systems must be able to handle the capacity requirements for an application. A storage system should be able to handle the entire volume of data that is required by the application, and must also be scalable in some form to meet the near and future-term requirements of the application.</li>
<li>Performance: Storage systems should be able to handle application expectations of performance. This can be broadly broken down into the following requirements:<ul>
<li>Latency: The system should respond to requests within a certain expected time frame. In web-scale systems this requirement is quite important as it directly relates to the user’s experience</li>
<li>Bandwidth: The system should be able to move data at a certain rate. For applications that rely on continuous feeds of data (such as video), this is a crucial metric.</li>
</ul>
</li>
<li>Access Patterns: The particular patterns in which an application accesses data can be used to design and implement efficient storage systems. Specifically the following patterns are of interest to storage systems designers:<ul>
<li>Granularity of Access: The granularity of access refers to the smallest amount of data retrieved in a typical operation. This can range from a few bytes to multiple megabytes, depending on the type of application.</li>
<li>Frequency of Access: Frequency of access refers to how often data is accessed from the system. If there are certain elements that are constantly accessed, they provide avenues for optimization through techniques such as caching.</li>
<li>Locality of Access: Refers to spatial and temporal locality. An example of locality is how distant data elements are when they are retrieved for the application. Locality can be considered both at the micro level (how far apart is the stride in an array index during a matrix multiplication), or even at the macro level (which data center worldwide should respond to this particular user’s request)?</li>
</ul>
</li>
<li>Durability: This refers to the applications expectations of how the data on the system needs to be persisted.</li>
<li>Fault Tolerance: Fault tolerance is a generic term that denotes several attributes of a system. They can be described in terms of the following requirements:<ul>
<li>Reliability: If a particular data item is reported as written by a system, can it be always be retrieved back from the system?</li>
<li>Availability: Is there a particular period of time where the system does not respond to requests? How often does this happen? Is my application affected by this, and what can be done to minimize the impact of this downtime?</li>
<li>Accuracy: How accurate are the results returned by the system? This may seem like a trivial question for a simple system that consists of only a single data store, but if a system is distributed with multiple copies of the same data, this may pose to be a very significant issue. Some applications can deal with data that is a bit stale, but others require an accurate answer every single time.</li>
</ul>
</li>
<li>Security: How secure does the data need to be when it is stored and accessed by the application? Is it protected against accidental or malicious access and/or modification and deletion? What kind of access restrictions can be imposed on users and applications that access this system?</li>
<li>Provenance: The process by which one can trace and record the origin of data as well as its movement between storage systems. For an application, does the functionality of being able to track all this information necessary? For certain applications dealing with sensitive and/or confidential data, it is a requirement.</li>
</ol>
<p>These requirements dictate the design of applications. Applications that require large capacity or need scalability in the near or long term need to be architected accordingly. As you will see in this Unit, storage systems that deal with large volumes of data are typically too slow or expensive to be contained within a monolithic system, these are typically distributed over multiple machines.</p>
<p>Strict performance requirements are usually translated into design choices involving caching or replication. Such systems are designed using the access patterns to determine optimal strategies for performance improvement. For applications that are serving clients over the internet, multiple data centers may be involved to provide faster performance and a better user experience by redirecting users to the closest available server that has the required data.</p>
<p>In order to use certain replication or caching techniques, the applicaiton must decide on what level or accuracy or freshness it needs of the data retreived from the system. Certain applications may be ok it they recieve stale data, but others require the most accurate and up-to-date data. This in turn affects the level of consistency that the storage system must provide to the application.</p>
<p><strong>References</strong></p>
<ol>
<li>Ginsberg, Jeremy and Mohebbi, Matthew H and Patel, Rajan S and Brammer, Lynnette and Smolinski, Mark S and Brilliant, Larry (2009). “Detecting influenza epidemics using search engine query data.” Nature.</li>
</ol>
<h3 id="Storage_Devices"><a href="#Storage_Devices" class="headerlink" title="Storage Devices"></a>Storage Devices</h3><p>We shall now dive into the actual devices used to store and persist data in Video 4.2. This video should serve to be a refresher on various types of storage technologies that are available, with their relative cost-performance tradeoffs.</p>
<p><a href="http://youtu.be/LljF0TatEGk" target="_blank" rel="external">Video 4.2: Storage Devices</a></p>
<p><strong>Memory Hierarchy</strong></p>
<p>As a quick recap, the memory hierarchy is illustrated in Figure 4.3 below. The fastest (and most expensive) storage space in a modern computer is the on-chip registers, which consist of about sixteen 8 byte registers per core. These can be accessed within a single clock cycle (&lt;1 nanosecond). Next, we have static RAM (SRAM), which the storage technology uses in cache memory, which can be accessed between 0.5 to 2.5 nanoseconds, but costs ~$10-50 per megabyte. Modern processors have a few megabytes of this type of memory located within the various levels (L1-L3) cache on the processor die. Each level differs in terms of capacity, access time, bandwidth and organization.</p>
<p><img src="/images/14567750500530.jpg" alt="Figure 4.3: Memory Hierarchy"></p>
<p>From here, there is a significant leap in terms of capacity and access time when going to main memory (DRAM). Current DRAM technology allows for access latencies of 50-70 nanoseconds, and consists of multiple gigabytes. DRAM costs about ~$10-30 per GB, allowing for a few gigabytes of storage in personal computers, and up to a terabyte in servers. Recall that all of the memories described thus far are all volatile memories. The data stored in these memories exist as long as they are powered ON. When switched OFF, they lose all information.</p>
<p>The next order of magnitude difference is observed in disks, which can take anywhere between 10s of nanoseconds to 100s of milliseconds to fetch a given item, depending on the type of storage device used. Finally, there is the network, which can connect machines that are in the same rack or can go across countries. The access latencies here can vary significantly, depending on the technologies used and the distance. Disks are non-volatile and persist data even when switched off.</p>
<p><strong>Types of Storage Devices</strong></p>
<p><strong>Magnetic Disks</strong></p>
<p>Magnetic disks have been the most cost-effective and popular storage systems for many decades now, but are slowly ceding their dominant position to solid state drives. Magnetic disks consist of one or more spinning magnetic platters and a movable read/write head that floats above the platters. Magnetic disks are extremely dense and cheap. At the time of writing, 6 Terabyte drives are available in the 3.5” desktop form factor for about $270, which translates to about 4 cents per gigabyte. This makes magnetic disks one of the cheapest on-line storage technologies available (as opposed to offline/removable technologies such as tapes and optical discs).</p>
<p>However, Magnetic disks are among the slowest storage technologies. Typical access latencies on modern disks range betweek 5-20 milliseconds. The main reason for the performance of magnetic disks is the fact that there are moving parts involved during reading and writing of data. As illustrated in Figure 4.4, magnetic disks are organized as a collection of sectors, tracks, and cylinders. In particular, a surface on a typical disk drive is divided into a number of concentric tracks. Each track is divided into a number of equal-sized sectors. The data stored at sectors are read and written using a set of read and write heads, with one head per surface. The set of tracks selected by the disk heads at any one time is called a cylinder. The data at sectors are accessed by moving disk heads to the appropriate cylinder (the time required for this task is called seek time) and then waiting for the disk to rotate until the desired disk sector is under one of the heads (called the rotational time). The time required to access a sector (i.e., seek time + rotational time) depends on the distance of the current position of the disk heads from the desired sector.</p>
<p><img src="/images/14567752865943.jpg" alt="Figure 4.4: Architecture of a magnetic hard disk drive"></p>
<p>As a result, magnetic storage disks are slow, particularly for random reads and writes, as the head must keep moving to different areas on the disk to read information, increasing the overall access time. However, they are cheap, and they are the main storage technology used in large-scale storage systems. Magnetic storage tends to be the main storage device located at the end of the spectrum, responsible for persisting large amounts of data in a cost effective manner.</p>
<p><strong>Solid State Disks (SSDs)</strong></p>
<p>The emergence of the NAND flash technology has brought increased performance and reduced prices to solid state storage in the past decade or so. Solid state drives, unlike magnetic disks, do not have any moving parts, and are nearly an order of magnitude faster than magnetic disks for random reads and writes. SSDs have access latencies that are an order of magnitude better than magnetic disks (70-150 nanoseconds for sequential operations), but cost significantly more (~$2 - $5 per GB).</p>
<p><img src="/images/14567753105073.jpg" alt="Figure 4.5: Architecture of a solid state hard drive."></p>
<p>Solid state disks, however, have their own performance and reliability issues. Due to the nature of NAND flash technology, writes to SSDs require an expensive erase cycle that erases an entire page of data (Figure 4.5), which takes time and wears out the flash medium over time. SSDs internally contain logic to level the wear of the medium by spreading out the writes over multiple pages and blocks on the disk. As discussed in Unit 2, there are multiple SSD technologies available in the market, where the primary tradeoff is cost vs. performance and disk life.</p>
<p>As a result, SSDs have different performance characteristics than rotating disks. Sequential reads and writes (where the CPU accesses logical disk blocks in sequential order) have comparable performance, with sequential reading somewhat faster than sequential writing. However, when logical blocks are accessed in random order, writing is an order of magnitude slower than reading, mainly due to the nature of the erase logic in SSDs.</p>
<p><strong>DRAM as a Storage Device</strong></p>
<p>The constant expansion of DRAM sizes, coupled with the drop in price per gigabyte has led to the emergence of in-memory storage systems. In memory storage systems offer an order of magnitude faster performance than traditional disk-based storage systems, but with one big caveat - durability. In memory storage systems typically have fairly complex schemes that stream data down to durable storage in order to persist the data for recovery and fault tolerance purposes. We will explore these types of systems in detail later in this module.</p>
<p><strong>Storage Class Memories / Non-Volatile Memories</strong></p>
<p>A number of technologies are emerging that aim to bridge the performance gap between the volatile DRAM and the non-volatile SSD/Magnetic disks. Dubbed storage class memories, these devices are aiming for access latencies that are within an order of magnitued of DRAM, allowing for the rapid movement of data, while retaining persistence properties of SSDs/magnetic disks, and having much higher storage densities than DRAM. Along with improved versions of NAND Flash memory, technologies such as memristors, phase change memory and others are competing gain a foothold in this space. SCM/NVM class memories are an ongoing developement which we expect to become part of the memory hierarchy soon.</p>
<h3 id="Storage_Abstractions"><a href="#Storage_Abstractions" class="headerlink" title="Storage Abstractions"></a>Storage Abstractions</h3><p><strong>Abstractions in Storage</strong></p>
<p>On a fundamental level, data are stored in binary encoding on some medium (such as the ones described in the previous page magnetic or solid state media). The challenge is to systematically organize the data in systems that are accessible to users and applications. These systems of organization provide abstractions to users and applications in the form of files on a file system, or as entities in a database.</p>
<p><img src="/images/14567753770162.jpg" alt="Figure 4.6: Various layers of abstraction of data"></p>
<p>It is important to note that applications can interact with any layer of the abstractions, including the block-level (which is rare for any non-system program). Block devices offer the highest performance but have the least metadata, making it very difficult to write programs using this layer. Many applications interact with a file system, and for applications that require more metadata, database systems may offer a better and more efficient abstraction to manage data.</p>
<p><strong>Block Devices</strong></p>
<p>In the previous pages you learned about different types of data and storage devices. Although there will always be data residing in various cache memories to improve performance, eventually it has to be stored on a persistent (sometimes called nonvolatile) medium such as Magnetic Disks or SSDs. Since these devices typically have a non-trivial architecture and layout, the interface presented to the operating system is a generic block device.</p>
<p>A block is simply a collection of bytes that is grouped together. The block size indicates how many bytes are in that block, such as 512 bytes, 64kiB, or 1MiB. Block sizes are usually represented as a power of 2. A single block is the smallest unit that can be addressed, so all of the bytes in a block must be read or written at the same time. This is similar to how a CPU accesses memory in words (either 32 bits or 64 bits) at a time; if you want to modify a single byte of data either in registers or main memory during a CPU instruction, you have to read 8 bytes (in case of a 64-bit CPU), modify 1, and then write the new word (1 new byte + 7 original bytes) back to memory. This process is amplified with blocks, so an application that modifies a single byte requires reading, say, 512 bytes, modifying 1 byte, and then writing back 512 bytes. This is a rare case, since applications typically read a number of contiguous blocks at a time and may be cached somewhere along the memory hierarchy.</p>
<p>A block device provides an OS access to blocks through an interface. In practice, block devices can be physical or virtual, and can be local or remote. A taxonomy of block devices along with examples are presented below:</p>
<p><img src="/images/14567754088711.jpg" alt=""></p>
<p>Physical storage devices correspond to a one-to-one mapping of physical storage blocks to the blocks exposed to the OS. Virtual devices abstract the details of a physical block device from the OS and instead present a virtualized block device to the OS.</p>
<p>Local storage refers to block devices that are directly connected to a computer. These can be either internal devices (connected via SCSI or ATA interfaces) or external devices (connected via USB, Firewire, eSATA, etc.). Remote storage is storage that is not directly attached to a computer, but is accessed through a network (using protocols such as Fibre Channel (FC) or iSCSI).</p>
<p>Physical block devices that are local to a computer include the common examples of hard disk drives, optical drives, and tape drives that are directly connected either through an internal interface or an external interface. The best example of a physical block device that is connected externally to a computer is a SAN with one-to-one mapping between the blocks allocated on the SAN to the blocks accessed by the OS.</p>
<p>Virtual block devices that are local to a computer include any block device that is represented as a file. Examples include virtual machine disk images such as Virtual Machine Disk (VMDK, VMware’s virtual disk format), Virtual Disk Image (VDI, VirtualBox’s disk format), and Virtual Hard Disk (VHD).</p>
<p>Remotely accessed virtual block devices include SANs, which export block devices that do not have a one-to-one correspondence with physically allocated blocks. For example, SANs can thinly provision a block device of 1TB to a server, but the actual disk space used on the SAN is the exact amount of storage used by the server at the moment. When an OS writes additional data to the block device, the block allocations within the SAN increase. SANs also have features such as de-duplication, which identifies identical blocks and stores only a single copy.</p>
<p><strong>File Systems</strong></p>
<p>One basic abstraction of storage is the concept of files and file systems. A file, in the context of storage, can be thought of as a block of information that is made available to computer programs. A file system is an abstraction to users and applications that includes the associated data structures to store, retrieve, and update a set of files. A file system typically manages access to its data and metadata (metadata is additional information and parameters that describe the files). File systems have a myriad of responsibilities, ranging from space management, naming, metadata management, and access control to maintaining the reliability and integrity of the files stored in the system.</p>
<p>File systems can be categorized into many types. Figure 4.7 illustrates the hierarchical taxonomy of file systems as defined by the Storage Networking Industry Association (SNIA).</p>
<p><img src="/images/14567754291984.jpg" alt="Figure 4.7: File system taxonomy [1]"></p>
<p>Based on their architecture and implementation, file systems can be broadly defined as</p>
<ul>
<li>Local file systems: These are file systems that are designed to be operated on a one or more disks, granting access to a single instance of an operating system to storage devices. Examples include FAT, ext, HFS, ReiserFS etc.</li>
<li>Shared file systems: These are special purpose file systems that allow multiple disks to be pooled into a group of available block devices to be shared among multiple machines over a network. Examples include IBM’s GPFS and SanFS.</li>
<li>Network file systems: Network file systems are higher-level storage services offered to applications (as opposed to plain block devices in Local and Shared file systems). Examples include CIFS, NFS, among others. We will explore, in detail, a special class of network file systems called Distributed File Systems in this module.</li>
</ul>
<p><strong>Databases</strong></p>
<p>Another layer of abstraction in storage systems is Databases. Databases are typically run on top of file systems, but there are instances where databases are run directly on top of block devices in order to improve performance, but these are rare. As we will see in detail in this module, databases are designed to store and retrieve information for applications. Databases have higher visibility into the data, sometimes allowing for complicated queries and operations on the data.</p>
<p><strong>References</strong></p>
<ol>
<li>Thomas Rivera (2012). “The Evolution of File Systems.” SNIA Tutorial.</li>
</ol>
<h3 id="Local_File_Systems"><a href="#Local_File_Systems" class="headerlink" title="Local File Systems"></a>Local File Systems</h3><p>In a local file system, the file system is collocated with the server that runs the application. Owing to the nature of the file system, local file systems have limited scalability and do not allow for data sharing across different clients over a network on their own (Figure 4.8).</p>
<p><img src="/images/14567754855531.jpg" alt="Figure 4.8: Local file systems [1]"></p>
<p>Data stored on a disk are typically represented as blocks, or a contiguous, unstructured collection of bytes. Local file systems provide an abstraction of files, which are simply a collection of blocks that represent a file.</p>
<p>Applications using local file systems are not concerned with how files are physically represented on storage media, the amount of data transferred to or from the application per file request (called record size), the unit by which data is transferred to or from storage mediums (called block size), and so on. All such low-level details are managed by local file systems and are effectively abstracted from user applications. In principle, local file systems are the basic building substrate of every file system type on the cloud. For instance, distributed file systems (e.g., <a href="http://hadoop.apache.org/docs/r1.0.4/hdfs_design.html" target="_blank" rel="external">Hadoop Distributed File System</a> which mimics the Google File System) [2] and parallel file systems (e.g., <a href="http://www.pvfs.org/" target="_blank" rel="external">PVFS</a>) are built and executed on multiple cooperative local file systems. Moreover, how well a virtual machine or a physical machine can survive software and hardware crashes on the cloud and on other systems depends partly on how well the local file systems are designed to handle such crashes. In short, practically every file system, whether shared or networked, relies on local file systems.</p>
<p><img src="/images/14567755234573.jpg" alt="Figure 4.9: The layout of a file system"></p>
<p>The UNIX file system is a classic local file system that was designed in the 1970s and has since been in widespread use in many forms (FFS, EXT-2, etc.). Although the data within a file is distributed as a series of blocks on a storage device, the file system maintains the abstraction of the file along with its associated data. As shown in Figure 4.9, a basic local file system includes a boot block, a superblock, an I-list and a data region. The boot block holds the boot program that reads the operating system’s (OS’s) binary image into the memory when the OS is booted. In essence, the boot block has nothing to do with any of the file system management processes and functionalities. The superblock describes the layout and the characteristics of the local file system, including its size, the block size, the blocks’ count, the I-list size and location, and so on.</p>
<p>In the I-list, the state of each file is encapsulated as a UNIX inode (index node; Figure 4.10). The inode acts as the primary data structure of a file and stores the metadata about a file, including pointers to the individual file blocks in storage, ownership and access control lists, timestamp of the last access of the file, and so on.</p>
<p><img src="/images/14567755392881.jpg" alt="Figure 4.10: Files, inodes, and blocks [1]"></p>
<p>Examples of local file systems include NTFS, FAT, and EXT. The scalability, performance, and sharing limitations of local file systems can be overcome by using shared/networked file systems.</p>
<p><strong>POSIX I/O Standards</strong></p>
<p>The Portable Operating System Interface (POSIX) is a family of standards that defines operating system (OS) interfaces for many UNIX and UNIX-like OSs. The POSIX file system standards are often used to describe the capabilities expected from a file system that can be used in UNIX and UNIX-like OSs.</p>
<p>POSIX defines the following standard operations on files: open, read, write, and close. In addition, the POSIX standards allow such file systems to be directly mounted in a UNIX or UNIX-like OS without having a special-purpose driver/client process to manage the file system.</p>
<p><strong>Kernel-Level vs. User-Level File Systems</strong></p>
<p>Kernel level file systems are file systems which contain a kernel level API, which also means that the code that interacts with the file system resides in the Kernel. In UNIX and UNIX-like operating systems, these APIs are loaded in as modules. Kernel-level file systems in UNIX-like operating systems are typically POSIX compliant and are usually limited to local file systems.</p>
<p>User-level file systems operate in user space as opposed to the kernel space. Such file system interfaces allow for a file system API to be portable and allow for installation in a much broader set of clients. Many distributed and networked file systems are designed to work in user-level, with one exception being AFS, which uses a kernel-level driver in Linux.</p>
<p><strong>Design Considerations in Local File Systems</strong></p>
<p>To understand how local file systems are designed, it is important to understand the underlying storage medium. In this discussion, we assume a spinning disk as storage medium.</p>
<p>Local file systems are designed to minimize seek and rotational times upon allocating disk capacity for files to improve system performance. Local file systems can also maximize the amount of useful data transferred after incurring the seek and rotational times. Performance is a main criterion for designing an effective local file system. Storage mediums such as disks and magnetic tapes do not provide uniform access times as primary storages (e.g., memory or caches). Consequently, a local file system should leverage the underlying storage medium to its fullest in order to achieve acceptable system performance and avoid undue waste of space. Avoiding undue waste of space is crucial, especially on the cloud where system resource utilization is of great significance.</p>
<p><strong>Performance</strong></p>
<p>To improve performance, local file systems can employ various strategies. First, local file systems can keep a number of block addresses in a file’s inode (specifically, in its diskmap). Local file systems can cache inodes in memory to reduce the number of disk accesses needed to read/write block locations. Second, to maximize the amount of useful data transferred, local file systems can make block sizes larger. Third, to minimize seek time, locality can be exploited. Specifically, blocks that are likely to be accessed in the near future can be kept in close proximity to one another on the disk, which means the blocks of each file must be stored as close together as possible. Furthermore, because inodes are accessed in conjunction with their data blocks, they must be stored close to each other. And because the inodes of a directory are often examined all at once (e.g., ls –la), the inodes of files at a single directory should be kept close to each other. Fourth, to reduce rotational latency, the blocks of a file in a cylinder (if any) should be arranged in a way that, after the seek time, they can all be read without further rotational delays. This arrangement enhances performance, especially if blocks are requested sequentially. If, however, the blocks are requested randomly, it becomes difficult to leverage such a block arrangement at a cylinder to minimize rotational latency. Finally, when a local file system is fetching a requested block, it can simultaneously prefetch blocks that are likely to be accessed in the near future. In fact, many local file systems (e.g., Ext2 and later versions) use a multiple-block-at-a-time strategy known as block clustering whereby eight contiguous blocks at a time are allocated. Blocks can also be buffered or cached for future references by the OS.</p>
<p>Although file systems were traditionally built to optimize performance on magnetic hard disks, many current file systems have operating modes for SSDs, which does away with some of the optimizations targeted at disks, and introduces new features for improved performance and wear management on SSDs.</p>
<p><strong>Dependability</strong></p>
<p>Another major criterion for designing effective local file systems is dependability. Dependability is also of a main concern for cloud storage. Local file systems should be dependable; that is, stored data must be accessible whenever it is needed. Hence, data should effectively tolerate software and hardware crashes. In addition, local file systems must ensure that the stored data are always consistent. Writing, creating, deleting, and renaming a file might require a number of disk operations that affect both data and metadata. To make sure that the underlying local file system is crash-tolerant means to ensure that any of these operations shall take the system from one consistent state to another. For instance, moving a file from one directory to another, which involves create and delete operations, might result in an inconsistent file system state. In particular, a crash might occur while the file is being moved, leading to the disappearance of the file in the two directories, the original and the projected one. To avoid this risk, the local file system can first create the file at the projected directory and subsequently remove it from the old one (after the file is committed at the new directory).</p>
<p><strong>Multi step File System Operations</strong></p>
<p>Some file operations might require multiple read or write steps, known as multi step file system operations. For instance, writing a large amount of data to a file might result in a number of separate disk operations (which is common on clouds). If a crash occurs before all of the required data is written, the local file system will end up in a state with only part of the write operation completed. A popular approach to deal with multi step operations is to use atomic transactions. With atomic transactions, if the system crashes during any step of a given multi step operation, the effect (probably after some recovery operations) is as if the whole operation either took place completely or did not occur at all. Transactions are a standard fare in database systems and are discussed in detail in the database section.</p>
<p>One basic scheme to implement atomic transactions in local file systems is called journaling. In journaling, the steps of a transaction are first written onto disk in a special journal file. Once the steps are safely recorded and the operation is committed (i.e., totally completed), they can be applied to the actual file system. If a crash occurs while the steps are being applied to the file system, the steps can be easily recovered from the journal file (assuming it is constantly protected from failures). This technique is known as the redo or new-value journaling. Even if the system crashes while the steps are being applied to the journal file, the already recorded steps in the journal file can be discarded and the actual file system will remain intact. The journaling approach guarantees that the whole operation will take place either completely or not at all.</p>
<p><strong>Expanding a Single File System over Multiple Disks</strong></p>
<p>To enhance dependability and/or performance, a local file system can be used with multiple disk drives. Video 4.3 covers the various techniques used to expand a file systems over mulitple disks</p>
<p><a href="http://youtu.be/hCygfAlgURc" target="_blank" rel="external">Video 4.3: File System Expansion</a></p>
<p>The three main reasons for expanding disk drives are</p>
<ol>
<li>To attain more disk storage</li>
<li>To store data redundantly</li>
<li>To spread blocks across multiple drives so they can be accessed in parallel, thereby improving performance</li>
</ol>
<p>Multiple disks can be transparently exposed to the local file system as a single disk using what is known as a logical volume manager (LVM). If we assume two disks, an LVM can present a large address space to the local file system and internally map half of the addresses to one disk and the other half to the second disk. To provide redundancy, an LVM can store identical copies of each block on each of the two disks. Doing so necessitates passing read and write operations through the LVM. For each write, the LVM updates the two desired identical copies on the two disks. For each read, the LVM forwards the request to the disk that is less busy. Finally, parallel accesses with multiple disks can be carried out using a technique called file striping. With file striping, a file is split into multiple units, which are subsequently spread across disks to enable parallel accesses to the file.</p>
<p>Data can be striped in different ways depending on the stripe unit (the level at which data is striped across multiple disks). In block-level striping, the stripe unit is a block of data. The size of a data block, which is known as the stripe width, varies with the implementation but is always at least as large as a disk’s sector size. When it comes time to read back this sequential data, all disks can be read in parallel. In a multitasking operating system, there is a high probability that even non-sequential disk accesses will keep all of the disks working in parallel. In byte-level striping, the stripe unit is exactly one byte. With bit-level striping, the stripe unit is exactly one bit.</p>
<p><strong>RAID</strong></p>
<p>As discussed in the data center module, multiple disks can be combined into a single logical drive using a RAID organization. RAID 0 stripes data at the block level over multiple disks with no redundancy. RAID 1 mirrors data of one disk to another and typically halves the capacity of an array. RAID 2 provides bit-level striping with hamming codes stored on parity drives. RAID 3 provides byte-level striping with parity information stored on dedicated parity drives. RAID 4 provides block-level striping with dedicated parity drives. RAID 5 provides the same block-level striping as RAID 4 and 1, but the parity information is distributed among all drives in the set. Finally RAID 6, is the same as RAID 5 but with parity blocks written twice so RAID 6 can tolerate up to two disk failures within a set. Combination RAID configurations, such as RAID 1+0 and RAID 0+1, are also possible for a mixture of performance and reliability guarantees.</p>
<p><strong>Storage Area Networks (SANs)</strong></p>
<p>In an enterprise IT environment, it is typical for storage to be consolidated so that it can be pooled and shared across multiple servers. Storage devices can be shared among multiple servers using a storage area network (SAN). A SAN is a dedicated network that provides access to consolidated, block-level data storage (Figure 4.11). The consolidated, block-level storage is typically in the form of a disk array. The disk array can be configured with some form of RAID, depending on the performance and reliability required. Servers typically access the SAN using a protocol such as iSCSI or Fibre Channel (FC). The servers that use a SAN see a logical block device, which can be formatted with a file system and mounted in the server. The application server can then use the externally stored logical blocks in the same way it would use locally stored blocks. Thus, the logical placement of the data is different from its physical placement.</p>
<p><img src="/images/14567756655339.jpg" alt="Figure 4.11: Storage area networks"></p>
<p>Although the servers share the disk array, they cannot physically share data residing on the disks. Instead, portions of the SAN (identified as logical unit numbers, or LUNs) are carved out and provided for the exclusive use of each server.</p>
<p><strong>References</strong></p>
<ol>
<li>Thomas Rivera (2012). “The Evolution of File Systems.” SNIA Tutorial.</li>
<li>Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung (2003). “The Google File Systems.” 19th ACM Symposium on Operating Systems Principles.</li>
</ol>
<h3 id="Distributed_File_Systems"><a href="#Distributed_File_Systems" class="headerlink" title="Distributed File Systems"></a>Distributed File Systems</h3><p>Switching gears now, we will discuss distributed file systems. A distributed file system is a file system that has files distributed among multiple file servers (Video 4.4).</p>
<p><a href="http://youtu.be/_1pef4h38wY" target="_blank" rel="external">Video 4.4: Distributed File Systems</a></p>
<p>It is important to note that in a distributed file system, the client views a single, global namespace that encompasses all the files across all the file system servers (Figure 4.12).</p>
<p>As with a shared file system, DFSs require metadata management so that clients can locate the required files and file blocks across the file servers. The metadata server can be asymmetric (single metadata server) or symmetric (metadata servers on each file server), similar to shared file systems.</p>
<p><img src="/images/14567757297156.jpg" alt="Figure 4.12: Distributed file system [1]"></p>
<p><strong>Origins and Evolution of Distributed File Systems</strong></p>
<p>Examples of DFSs include the Andrew File System (AFS). AFS is a distributed file system that enables cooperating hosts (clients and servers) to efficiently share file system resources across both local area and wide area networks. AFS consists of cells, an administrative grouping of servers that present a single cohesive file system. Cells can be combined to form a single global namespace. Any client that accesses data from AFS will first copy the file locally to the client. Changes to the file will be made locally as long as the file is open. When the file is closed, the AFS client will sync the changes back to the server. An evolution of AFS is CODA, which is a distributed file system that improves on AFS, particularly with respect to sharing semantics and replication.</p>
<p>In 2003, Google revealed the design of its distributed file system, called GFS [2] , which was designed from scratch to provide efficient, reliable access to data using large clusters of commodity hardware. GFS is designed to store very large files as chunks (typically of size 64MB), in a replicated fashion. Although GFS has a singular client view like AFS, the location of file chunks is exposed to the user, given opportunities to fetch files from the closest available replica. GFS is the inspiration behind the Hadoop Distributed File Systems (HDFS), which is covered in detail in the next module.</p>
<p><strong>Design Characteristics in Distributed File Systems</strong></p>
<p>DFSs are typically deployed on multiple file-sharing nodes and are intended for use by multiple users at the same time. As with any shared resource, multiple design considerations must be considered:</p>
<ul>
<li>Fault tolerance</li>
<li>Replication</li>
<li>Consistency</li>
<li>File-sharing semantics</li>
</ul>
<p>Shared and networked file systems must be designed with failures in mind. Fault tolerance can be defined as the ability of a system to respond gracefully to unexpected hardware and software failures. In the case of file systems, fault tolerance requires the file system to respond gracefully to disk, node, and network failures. With shared and networked file systems, the odds of a disk failing increase with the number of disks in the array/pool. At the hardware level, faults can be tolerated by using some form of RAID.</p>
<p>At the file system level, data in distributed systems may be replicated; the same data may be maintained on one or more nodes in the distributed file system. This replication is done in order to</p>
<ul>
<li>Improve performance (a client can potentially find a replica that is nearest to its location).</li>
<li>Enhance the scalability of the system (simultaneous requests for data can be handled by different servers).</li>
<li>Enhance reliability (replicas can provide fault tolerance and provide a checksum mechanism to ensure the integrity of the data).</li>
</ul>
<p>Replication is the primary mechanism to deliver fault tolerance in DFSs. The capacity of a DFS is usually impacted by the replication factor (the number of active replicas to be maintained). For example, a DFS configured with a raw capacity of 15TB can store only 5TB of data if all the data is triple replicated.</p>
<p>Replication poses the additional challenge of consistency. In a large distributed system, updates to files must be applied to all replicas. The level of consistency supported in a DFS also affects client interactions with the file system.</p>
<p>When a resource, such as a file, is shared between multiple users, it is necessary to define the semantics of reading and writing to the file. Following are some of the semantics that can be implemented with a DFS:</p>
<ul>
<li>UNIX semantics: In UNIX semantics, a read operation performed immediately after a write operation will always return the value that was just written. UNIX employs the strictest file-sharing semantics in file systems. With UNIX semantics, performance may be affected because reads and writes may have to be serialized to ensure that all file system operations are consistent.</li>
<li>Session semantics: In session semantics, changes to an open file are initially visible only to the process that modified the file. Once the file is closed, the changes are made visible to other processes. Session semantics relaxes the strict requirements employed by UNIX semantics, but the question of conflict handling emerges: When two clients are simultaneously editing the same file, whose session is honored? Some approaches honor only the last client to close the file, while others might even be unspecified.</li>
<li>Immutable semantics: In immutable semantics, files can be written only once to a file system and cannot be reopened for further modification. Files can be deleted, or a new file can be created to replace the old one. If two or more processes try to replace the file simultaneously, the file system should resolve the tie through first in, first out (FIFO) order or in a non-deterministic fashion. The file system must also account for the possibility that one of the processes can replace a file while another is busy reading it. In this scenario, the file system should either arrange for the reader to continue using the old file or detect that the file is now replaced and not allow the reading process to continue accessing the file.</li>
<li>Atomic transactions: In an atomic transaction model, the start and end of sequences of read and write operations are marked as being a transaction in which the changes to files happen atomically (changes are either committed as a whole or not committed at all).</li>
</ul>
<p><strong>References</strong></p>
<ol>
<li>Thomas Rivera (2012). “The Evolution of File Systems.” SNIA Tutorial.</li>
<li>Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung (2003). “The Google File System.” 19th ACM Symposium on Operating Systems Principles.</li>
</ol>
<h3 id="Databases"><a href="#Databases" class="headerlink" title="Databases"></a>Databases</h3><p>File systems are typically used to store arbitrary data as files on disks with some form of metadata (such as a filename) used to identify and locate these files. File systems offer rudimentary search and indexing capabilities, and searching the contents of files to find the information you need is often an involved and laborious process. When the data is amenable to structuring, it is typical to organize it in a database, using a well-defined model (also known as a schema). Video 4.5 provides an overview of Databases:</p>
<p><a href="http://youtu.be/9f8eIEOrcz8" target="_blank" rel="external">Video 4.5: Databases</a></p>
<p>A database consists of an organized collection of data for one or more uses, typically in digital form. Databases are one of the most popular information storage and retrieval methods for many types of applications, including financial, manufacturing, business, and personnel data.</p>
<p>Databases have evolved over the years, starting with the navigational model in the 1960s, which stored data as records with pointers that linked to the next and/or previous record in the database. Examples include the Codasyl approach, which eventually evolved into the Common Business Oriented Language (COBOL). The Codasyl approach was based on the “manual” navigation of a linked data set, which was formed into a large “network” of linked data, much like a circular linked list. When the database was first opened by a program, the program was handed back a link to the first record in the database, which also contained pointers to other pieces of data. To find any particular record, the programmer had to step through these pointers one at a time until the required record was returned. Simple queries that aggregate information across records required a complete scan of the entire database—concepts such as search and indexing did not exist then. The Codasyl approach was appropriate in an era when magnetic tapes were the primary mode of nonvolatile storage, which could be read only sequentially and had no random access properties.</p>
<p>The emergence of the relational database model and the subsequent development of IBM’s Systems R was a major milestone in database system development. For many decades Relational database systems were the only databases in wide use, and are extremely successful. With the growth of the Internet and the need for massively scalable systems that can service millions of simultaneous requests, NoSQL database systems emerged, starting with the likes of Google’s BigTable and Amazon’s dynamo. However, of late, a class of relational database systems have emerged, re-architected from scratch to try to combine the strengths of the two models.</p>
<p><strong>Types of Database Systems</strong></p>
<p>Current database systems can be classified into the following types:</p>
<p>Relational databases are the classical database systems which have been around for decades. They follow the relational model of representing data as relations between entities. Relational database systems typically have a rigid schema, use the Structured Query Language (SQL) as the interface to interact with the database, and typically have strong ACID properties.</p>
<p>NoSQL databases are a new breed of databases which are markedly different from relational database systems. NoSQL systems typically do not enforce a strong schema or relational model of the data, and instead provide a different abstraction (like a key-value store), instead of a SQL-like interface. NoSQL databases are focused on scalability and performance, and typically do not have strong transactional guarantees for database operations.</p>
<p>NewSQL is a class of modern relational database systems that seek to provide the same level of performance and scalability as NoSQL systems but while maintaining some degree of ACID properties that relational database systems offer.</p>
<h3 id="Designing_Databases_-_Schema_and_Scalability"><a href="#Designing_Databases_-_Schema_and_Scalability" class="headerlink" title="Designing Databases - Schema and Scalability"></a>Designing Databases - Schema and Scalability</h3><p>Databases, as you can imagine, encompass a broad range of applications and requirements. To this end, we will discuss a few specific design patterns that database creators have to make when creating their database systems.</p>
<p><strong>Schema Enforcement</strong></p>
<p>Database systems, in general, require some information regarding the types and structures of the data to be stored in the database. With relational database systems, the schema is formally defined in terms of :</p>
<ol>
<li>Tuples (Rows), which consist of Attributes (Columns) all of which are defined to be a certain type.</li>
<li>A Relation (table), which consists of multiple tuples of the same type.</li>
<li>Constraints, which define a set of rules on tuples, attributes and relations.</li>
</ol>
<p>Schema are not mere guidelines, but are enforced in relational systems. Schema-based databases have the following advantages:</p>
<ol>
<li>Schema express the data model in a manner which allows for complex operations and queries to be constructed. For example, a join (where data is inferred from two or more distinct tables, is relatively easy to express in a relational database).</li>
<li>Schemas can contain constraints which can be used to enforce program logic at the database level. For example, simple constraints can check if a person’s age is a positive number, while a bank ledger implemented as a set of tables in a relational database can enforce the property (using a complex constraint), that the sum of all credits and debits in the system must equal to zero. The system can automatically reject or return transactions that violates these constraints.</li>
<li>Relational systems take advantage of the information provided in the schema to construct elaborate and efficient query plans, allowing the database to efficiently answer queries regarding the data stored.</li>
</ol>
<p>However, the rigid enforcement of a schema is also a barrier to flexibility. As an application evolves, changing the schema of a table that is already populated with rows is difficult, if not impossible, depending on the table and the constraints provided.</p>
<p>On the other hand, there are systems that have either flexible schema, or no schema whatsoever. An example of a system with practically no schema is a generic key-value store, which basically acts as mapping between keys or a certain type and an associated value. Some key-value stores are very flexible; they allow for any arbitrary binary data to be stored under a particular key. Some key value stores, such as Dynamo, require that the type of the key (i.e. string, integer, etc.) should be made explicit when creating a new table. Some systems allow for nested values to be present in data attributes, typically stored as JSON or XML.</p>
<p>Another variation is systems like BigTable and HBase, which have a semi-flexible schema. These systems require the number of columns to be declared when a table is created, but a column can have any number of nested sub-columns. These systems will be covered in detail in the next module.</p>
<p><strong>Transaction vs. Analytical Processing</strong></p>
<p>A key design choice that needs to be made when optimizing a database system is to optimize for the common case of the type of queries that it will receive. There are two main patterns in database workloads that have emerged.</p>
<p>Transactional workloads, also known as Online Transactional Processing (OLTP), are workloads that are mainly composed of short on-line transactions. OLTP workloads mainly consist of insertions, updates and/or deletions. The emphasis on OLTP systems is on fast query processing and maintaining data integrity in environments where there is simultaneous access. A good example of an OLTP workload is financial transactions. Queries will typically involve only a few tables and rows and will not require vast scans of the database.</p>
<p>On the other hand, certain systems aggregate and analyze data in order to gain insights and derive information from the data. Such systems are called Online Analytic Processing (OLAP) systems. OLAP systems generally involve a lower volume of transactions but individual queries may be complex and involve aggregate computations that span multiple rows and multiple tables. Typical OLAP queries thus are significantly longer running than OLTP queries.</p>
<p>Video 4.6 provides an overview of OLTP vs. OLAP:</p>
<p><a href="http://youtu.be/BNgouADOemg" target="_blank" rel="external">Video 4.6: OLTP vs. OLAP</a></p>
<p><strong>Scalability</strong></p>
<p>Over time, when a database system’s user base and data grows, it may require some form of scaling, which would expand the capacity and/or the performance of the database system in order to support more users or data, or both. Database scaling is complex for a variety of reasons which we will cover in detail in Video 4.7:</p>
<p><a href="http://youtu.be/opYWHWG-vVg" target="_blank" rel="external">Video 4.7: Database Scaling</a></p>
<p><strong>Vertical Scaling</strong></p>
<p>Vertical scaling (or scaling up) is the process of increasing a database’s capacity or ability to handle load without adding more hosts. This can be done by hardware upgrades such as a faster CPU, increased RAM, or disks with more capacity and/or more I/O operations per second (IOPS). The scalability with vertical scaling is limited by the amount of CPU, RAM, and disk that can be configured on a single machine.</p>
<p><strong>Horizontal Scaling</strong></p>
<p>Horizontal scaling (or scaling out) means the scaling of databases by adding more nodes to handle database queries. Different RDBMS products offer different ways of scaling out, including replication and database sharding. With replication, the same data is stored across multiple nodes. This enables the RDBMS to handle additional requests in parallel, thereby increasing the number of transactions per second. This approach works when the database is read-heavy. In sharding, a large table is partitioned across multiple nodes (typically on an index or key). The amount of data shared across nodes is limited in this case, and limited sharing is preferred because it reduce issues with consistency of replicated data.</p>
<p>Replication is the process of duplicating data over multiple servers in order to increase the number of available paths to individual pieces of data. Data can be replicated for performance, availability and/or fault tolerance reasons. Replicated data allows for faster performance, particularly for reads, as replication allows for parallel access to multiple copies of the same data. Replication assists in availability and fault tolerance as data is available in a backup location should a part of the database system fail.</p>
<p>Another method, that is orthogonal to replication, it sharding. Sharding is the process by which data is partitioned into sections (known as shards) and distributed over multiple database systems. In contrast to replication, each shard acts as the single source for the subset of data contained by it. Sharding is a technique that is specifically used to improve the performance of databases, and in particular, write performance.</p>
<p>In an ideal case, a sharded database will evenly spread out the data across all partitions, but this is difficult to achieve as the data distribution across all partitions should be more or less balanced. A popular technique that is the mainstay of many current database systems is consistent hashing.</p>
<p>Consistent hashing is a special kind of hashing technique which allows for a hash table to be expanded in an efficient manner. If K is the total number of keys and n is the number of buckets in a given hash table, consistent hashing requires only K/n keys to be remapped on average every time a new bucket is added to the hash table. More details on consistent hashing can be read here [1] .</p>
<p>Some systems use a combination of replication and sharding to provide high performance while maintaining high availability and fault tolerance. However, when using replication, a major concern is consistency, which will be discussed next.</p>
<p><strong>References</strong></p>
<ol>
<li>Karger, David and Lehman, Eric and Leighton, Tom and Panigrahy, Rina and Levine, Matthew and Lewin, Danielr (1997). “Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web.” Proceedings of the Twenty-ninth Annual ACM Symposium on Theory of Computing.</li>
<li>Eric Brewer (2000). “Towards Robust Distributed Systems.” Proceedings of the Annual ACM Symposium on Principles of Distributed Computing.</li>
</ol>
<h3 id="Designing_Databases_-_Consistency_and_the_CAP_Theorem"><a href="#Designing_Databases_-_Consistency_and_the_CAP_Theorem" class="headerlink" title="Designing Databases - Consistency and the CAP Theorem"></a>Designing Databases - Consistency and the CAP Theorem</h3><p><strong>Consistency</strong></p>
<p>Without replication, consistency is not a concern. Replication entails maintaining multiple copies (or replicas) of the same data across multiple machines, whereas consistency provides a system-wide consistent view/state of replicas across multiple clients/processes. As pointed out earlier, data items are replicated for two primary reasons: performance and reliability. Regarding performance, replication is crucial when the system needs to scale in numbers (as is mainly the case with storages in private and public clouds) and in geographical areas (as is usually the case with storages in public clouds). For instance, when an increased number of processes must access data that are managed by a single storage server, performance can be improved by replicating the data across multiple servers (instead of having only one server) and subsequently dividing the work across the servers. This division allows requests to be served concurrently and thereby increases system speed. In contrast, replication across geographical areas (e.g., as applied by Amazon S3) can be ensured by replicating copies of data in the proximity of the requesting processes to minimize data access times. Concerning reliability, replication makes it possible to provide better fault tolerance against corrupted and lost data, a key requirement on the cloud. In particular, if a replica is lost or corrupted, another valid replica (if maintained) can be alternatively accessed.</p>
<p>Replication produces multiple replicas of data, and consistency ensures that those replicas remain uniform so that different processes can access (i.e., read and write) them. Specifically, a collection of replicas are considered to be consistent when they appear identical to concurrently accessing processes. The ability to handle concurrent operations on replicas while maintaining consistency is fundamental to distributed storage in general and cloud storage in particular. In this unit, consistency is discussed in the context of read and write operations performed on shared replicas in a distributed data store (Figure 4.13). The distributed data store can be a DFS, a parallel file system, or a distributed database. Consistency can be achieved by employing a consistency model. We define a consistency model as a contract between processes and the distributed data store, which implies that if processes agree to obey certain rules, the distributed data store promises to enforce those rules. Video 4.8 covers a number of consistency models.</p>
<p><a href="http://youtu.be/tNUARdbuUtQ" target="_blank" rel="external">Video 4.8: Consistency Models</a></p>
<p>We discuss three consistency models in detail in this unit: sequential consistency, causal consistency, and eventual consistency.</p>
<p><img src="/images/14567760886584.jpg" alt="Figure 4.13: A distributed data store that can be a distributed file system, a parallel file system, or a distributed database with replicas maintained across distributed storage disks"></p>
<p><strong>Sequential Consistency</strong></p>
<p>Also called strong or strict consistency, sequential consistency entails propagating updates to all replicas immediately, which typically requires applying updates on related replicas in a single atomic operation, or transaction. In practice, implementing atomicity across widely dispersed replicas in a large-scale distributed data store is inherently difficult, especially if updates are to be completed quickly. The difficulty stems from the unpredictable access latencies imposed by the underlying storage network and the lack of a global clock that can be utilized to order operations rapidly and accurately. To address this problem, the requirement of executing updates as atomic operations can be relaxed. Relaxing consistency requirements means that replicas need not always be the same in all locations. Whether or not consistency relaxation is acceptable depends on the application that is running over the distributed data store. Specifically, relaxing consistency requirements depends on the read and write access patterns of the application and the purpose for which replicas are used. For instance, browsers and Web proxies are often configured to store Web pages in local caches (this is a type of replication because multiple copies are maintained for the same Web page) to reduce access times for future references. It is acceptable in some situations that users receive outdated Web pages as long as, eventually and rapidly enough, the Web pages will be upgraded to the most up-to-date versions available at the actual Web server(s). Eventual consistency is an example of a model that suits such scenarios.</p>
<p><img src="/images/14567761066514.jpg" alt="Figure 4.14: (a) A sequentially consistent distributed data store, and (b) a nonsequentially consistent distributed data store"></p>
<p>A distributed data store is considered sequentially consistent if all processes see the same interleaving of read and write operations when accessing replicas concurrently. Figure 4.14 demonstrates two distributed data stores: a sequentially consistent data store (Figure 4.14(a)) and a nonsequentially consistent data store (Figure 4.14(b)). The symbols W(x)a and R(x)b denote a write value of a to replica x and a read value of b from replica x, respectively. The figure shows four processes operating on replicas corresponding to data item x. In Figure 4.14(a), process P1 writes a to x, and later (in absolute time), process P2 writes b to x. Subsequently, processes P3 and P4 first receive value b and later value a upon reading x. As such, the write operation carried by process P2 appears to have taken place before that of P1 and of both processes P3 and P4. Nonetheless, Figure 4.14(a) still represents a sequentially consistent distributed data store because processes P3 and P4 experience the same interleaving of operations (i.e., reading first b and then a). In contrast, processes P3 and P4 in Figure 4.14(b) see different interleaving of operations, thus violating the condition of sequential consistency.</p>
<p>Causal Consistency<br>The causal consistency model is a weaker variant of the sequential consistency model. First, causality implies that if operation b is caused or influenced by an earlier operation a, then every process accessing the distributed data store should see first a and then b. A causally consistent distributed data store enforces consistency across only the operations that are potentially causally related. The operations that are not potentially causally related can appear at processes in any interleaving and are denoted as concurrent operations. Figure 4.15 shows two causally consistent distributed data stores (Figures 4.15(a) and 4.15(c)) and one noncausally consistent distributed data store (Figure 4.15(b)). In Figure 4.15(a), W(x)b performed by process P2 is potentially dependent on W(x)a carried by process P1 because b may be a result of computation involving a read by process P2 (i.e., R(x)a) before writing b (i.e., W(x)b). Thus, the results of the write operations W(x)a and W(x)b performed by P1 and P2, respectively, should appear in the same order at each reading process. Because processes P3 and P4 read first a and then b, they are said to adhere to the causality condition, thus making the underlying distributed data store causally consistent. In contrast, process P3 in Figure 4.15(b) does not abide by the causality condition (i.e., it reads first b and then a), thus rendering the underlying distributed data store noncausally consistent. Last, Figure 4.15(c) illustrates a causality consistent distributed data store because W(x)a and W(x)b are concurrent operations; hence, their results (i.e., R(x)a and R(x)b) can appear in any order in the reading processes, which is the case for processes P3 and P4.</p>
<p><img src="/images/14567761338270.jpg" alt="Figure 4.15: (a) A causally consistent distributed data store, (b) a noncausally consistent distributed data store, and (c) A causally consistent distributed data store"></p>
<p>Eventual Consistency<br>The eventual consistency model is considered a weaker form of sequential and causal consistency models. Eventual consistency implies that a write on a replica need not be propagated to all other replicas immediately and can be delayed (or sometimes never propagated) if acceptable by applications. Specifically, if a process P accesses a certain replica, R, N times per a minute, and R is updated M times per minute, then if an application exhibits a low read-to-write ratio (i.e., N &lt;&lt; M), many updates to the replica will never be accessed by P, rendering all those updates and the required network communication pointless. In this case, it might be better to apply a weak consistency model whereby R is updated only when accessed by P. That is, it is more efficient to propagate an update in a lazy fashion, whereby a reading process will see an update only after some time has passed since the update took place (not immediately, as is the case with strict consistency). If conflicts resulting from two operations that attempt to write on the same data (i.e., write-write conflicts) seldom or never occur, it is often acceptable to delay the propagation of an update. Conflicts seldom occur with database systems, which typically employ eventual consistency. We discuss database systems in detail in the next section. Note that we presented only part of the story about consistency models. Another part covers how such models are implemented. In this unit, we are concerned only with what these models are and their applicability to cloud storage systems. Nonetheless, we briefly point out that sequential consistency is hard to implement in practice and that it scales poorly. Typically, sequential consistency requires using synchronization mechanisms such as transactions and locks. In contrast, implementing causal consistency involves building a dependency graph that captures causally related operations and enforces those operations across accessing processes. One way to implement such a model is to use vector timestamps. Eventual consistency can be implemented by grouping read and write operations into sessions and using vector timestamps.</p>
<p><strong>ACID Properties in Databases</strong></p>
<p>Databases can offer transactional properties. A transaction comprises a unit of work performed within a database management system (or similar system) against a database. Transactions are treated in a coherent and reliable way independent of other transactions. Transactions in a database environment have two main purposes:</p>
<ul>
<li>To provide reliable units of work that allow correct recovery from failures and keep a database consistent even in cases of system failures when execution stops (completely or partially) and many operations on a database remain uncompleted, with unclear status.</li>
<li>To provide isolation between programs accessing a database concurrently. If this isolation is not provided, the program’s outcomes are possibly erroneous.</li>
</ul>
<p>To better understand the need for transactions in a database, consider the following financial transaction performed on two bank accounts A and B. Suppose a user would like to transfer $100 from account A to account B. This transfer can be represented in two steps:</p>
<ol>
<li>Deduct $100 from account A.</li>
<li>Credit $100 to account B.</li>
</ol>
<p>Suppose a database failure occurs between operations 1 and 2: $100 would have been deducted from account A, but the credit to account B would have not taken place. The accounts, and the database itself, would be in an inconsistent state.</p>
<p>To solve this problem, the operations can be defined as a transaction, as follows:</p>
<ol>
<li>Begin transaction.</li>
<li>Deduct $100 from account A.</li>
<li>Credit $100 to account B.</li>
<li>End transaction.</li>
</ol>
<p>It now becomes the database’s responsibility to ensure atomicity of this transaction—that the transaction is either successful in its entirety (committed) or not performed at all (rollback). The database should be consistent after the transaction is complete; that is, the database should be in a valid state after the transaction is committed, and any rules defined for records in the database should not be violated (e.g., a savings account may not have a negative balance). Any transactions that are happening concurrently to the accounts must not interfere with each other, thus providing isolation. Finally, the transaction must be durable, which means that the actions performed in the transaction should persist after the transaction is committed to the database. The properties of atomicity, consistency, isolation, and durability are collectively known as the ACID properties of transactions and are expected to be followed by most RDBMS that are used for transaction processing. Video 4.9 provides an overview of ACID properties in databases:</p>
<p><a href="http://youtu.be/jt2eq7BgcvM" target="_blank" rel="external">Video 4.9: ACID Properties</a></p>
<ul>
<li>Atomic: The transaction is indivisible—either all the statements in the transaction are applied to the database, or none are.</li>
<li>Consistent: The database remains in a consistent state before and after transaction execution.</li>
<li>Isolated: Although multiple transactions can be executed by one or more users simultaneously, one transaction should not see the effects of other concurrent transactions.</li>
<li>Durable: Once a transaction is saved to the database (an action referred to in database programming parlance as a commit), its changes are expected to persist.</li>
</ul>
<p><strong>Why you can’t have it all: The CAP Theorem</strong></p>
<p>In 1999, Brewer [1] proposed a theorem describing the limitations of distributed data storage called the CAP theorem. The CAP theorem states that any distributed storage system with shared data can have at most two of three desirable properties:</p>
<ul>
<li>Consistency: Consistency is a state in which every node always sees the same data at any given instant (strict consistency).</li>
<li>Availability: A guarantee that every request receives a response about whether it was successful or failed is an availability guarantee.</li>
<li>Partition tolerance: A network partition is a condition where the nodes of a distributed system cannot contact each other. Partition tolerance means that the system continues to operate normally despite arbitrary message loss or failure of part of the system.</li>
</ul>
<p>Video 4.9 provides an overview of the CAP theorem:</p>
<p><a href="http://youtu.be/i90_daid8FI" target="_blank" rel="external">Video 4.9: The CAP Theorem</a></p>
<p>The easiest way to understand CAP is to think of two nodes of a distributed storage system on opposite sides of a network partition (Figure 4.16). Allowing at least one node to update state will cause the nodes to become inconsistent, thus forfeiting C. Likewise, if the choice is to preserve consistency, one side of the partition must act as if it is unavailable, thus forfeiting A. Only when nodes communicate is it possible to preserve both consistency and availability, thereby forfeiting P.</p>
<p>As an example, consider the case of a traditional single-node RDBMS. In this scenario, consistency and availability can be guaranteed, while the concept of partition tolerance does not exist because the database is on a single node.</p>
<p>When companies such as Google and Amazon were designing large-scale databases to serve millions of customers, 24/7 availability was key, as even a few minutes of downtime means lost revenue. When scaling distributed shared-data systems to hundreds or thousands of machines, the likelihood of a failure of one or more nodes (thereby creating a network partition) increases significantly. Therefore, by the CAP theorem, in order to have strong guarantees on availability and partition tolerance, one must sacrifice strict consistency in a large-scale, high-performance distributed database.</p>
<p><img src="/images/14567762499435.jpg" alt="Figure 4.16: CAP theorem illustrated"></p>
<p><strong>References</strong></p>
<ol>
<li>Eric Brewer (2000). “Towards Robust Distributed Systems.” Proceedings of the Annual ACM Symposium on Principles of Distributed Computing.</li>
</ol>
<h3 id="Relational_Databases"><a href="#Relational_Databases" class="headerlink" title="Relational Databases"></a>Relational Databases</h3><p>In the 1970s, Edgar F. Codd pioneered the relational database model, which shifted the focus away from the navigational approach to a data-centric approach. In the relational model, data is stored as fixed-length fields in normalized tables (Figure 4.17). In normalization, large tables can be divided into smaller and less redundant tables, and relationships can be defined between them. Codd went on to define various degrees of normalization for data stored in the relational model. In addition, the Structured Query Language (SQL) was developed at IBM to provide a reasonably intuitive language to create and manage databases.</p>
<p><img src="/images/14567762852498.jpg" alt="Figure 4.17: Relational database tables"></p>
<p>The concepts of relational databases were combined with SQL and used by IBM, which designed the System R and the DB2 relational database management systems (RDBMS), which were the precursors to all modern RDBMSs.</p>
<p>The typical features of modern RDBMSs include the following:</p>
<ul>
<li>Data is modeled as records in tables, and related records across tables are expressed as relations.</li>
<li>Tables are defined by schemas, which impose rules on the types and valid inputs for each column (also known as a field) in a table.</li>
<li>Tables are defined, accessed, and modified through a query language (typically SQL).</li>
</ul>
<p>Modern RDBMSs provide application developers a complete data management solution that abstracts the notion of data management away from the application developer. Once a schema is designed, applications can store, retrieve, and modify records based on the rules specified in the schema. By specifying constraints on information in a database, applications can handle data in a more robust fashion, which is crucial to avoiding data inconsistencies in application programs. For example, a user database can validate the age of each user to be a non-negative integer between, say, 1 and 130. The database can then refuse to add a user whose age does not meet this constraint and can provide an automatic validation mechanism as the data is being stored.</p>
<p><strong>Scaling in Traditional Databases</strong></p>
<p>Traditional RDBMSs have been designed to run on a single machine, but as the amount of data or the number of concurrent users increase, the database must be scaled. Recall that databases can be scaled either vertically or horizontally.</p>
<p>Guaranteeing ACID properties across a distributed database where no single node is responsible for all data affecting a transaction presents additional complications. Network connections might fail, or one node might successfully complete its part of the transaction and then be required to roll back its changes because of a failure on another node. The need for distributed concurrency control mechanisms thus emerged.</p>
<p>An early example of distributed concurrency control was the two-phase commit (2PC) protocol. 2PC provides atomicity and consistency for distributed transactions to ensure that each participant in the transaction agrees on whether or not the transaction should be committed. In the first phase, one node (the coordinator) interrogates the other nodes (the participants) and sends a VOTE_REQUEST message to all participants (Figure 4.18). The participants then submit their vote on the transaction to the completed, either a VOTE_COMMIT or a VOTE_ABORT. In the second phase, the coordinator tallies the votes from every participant and issues a GLOBAL_COMMIT message if every participant voted in favor of the transaction or a GLOBAL_ABORT message if even one of the participants voted against the transactions (Figure 4.19). Finally, the participants commit the transaction or rollback based on the message received from the coordinator. The 2PC protocol thus guarantees strict consistency, as every transaction requires the global consensus among all nodes. Variants of 2PC include the three-phase commit (3PC) and the 2-phase locking protocol. However, these concurrency control protocols are expensive and affect performance at scale.</p>
<p><img src="/images/14567763219057.jpg" alt="Figure 4.18: 2PC - Phase One"></p>
<p><img src="/images/14567763304981.jpg" alt="Figure 4.19: 2PC - Phase Two"></p>
<h3 id="NoSQL_Databases"><a href="#NoSQL_Databases" class="headerlink" title="NoSQL Databases"></a>NoSQL Databases</h3><p>Recall that Brewer’s CAP theorem proves that it is impossible to guarantee strict consistency and availability while being able to tolerate network partitions. This, in turn, led to various databases being designed with relaxed ACID guarantees, particularly in the case of consistency. An alternative to ACID is BASE (basically available, soft state, and eventual consistency).</p>
<ul>
<li>Basically available indicates that the system guarantees availability in terms of the CAP theorem. Small failures should not result in large disabilities of the system.</li>
<li>Soft state indicates that the state of the system may change over time, even without input, because of the eventual consistency model.</li>
<li>Eventual consistency means that given a sufficiently long period of time over which no changes are sent, all updates can be expected to propagate eventually through the system and the data will be consistent.</li>
</ul>
<p>For example, assume a system with two transactions, A and B, running in parallel. Let A and B change the value of a variable x at the same time. Transaction A writes value a and transaction B writes the value b to variable x. Under a strict consistency model, the database will impose a total ordering of all transactions and guarantee that every read of the variable x will return its latest value. Under eventual consistency, the value of the variable x may not propagate to each and every node before the next read. Even after transaction B executes, some nodes may still see the variable x has value a. Since the system is eventually consistent, there is a possibility of conflicts emerging with regard to operations on the data. Conflicts can be resolved through mechanisms such as vector clocks or client-specified timestamps.</p>
<p>To understand why BASE is a useful model, consider the scenario of an e-commerce vendor that deals with millions of visitors every day. A bulk of the visitors are shopping around and browsing products. One of the elements that is on the screen is the available stock of a product. Now the vendor can calculate the available stock using an expensive transactional query that guarantees the value that is provided to the user at any given instant. However, this requires getting a consistent snapshot of the inventory database, which could in-turn lead to an expensive transaction, which is even more expensive if the vendor runs a distributed database. The vendor can only scale up so much until the performance of the query is poor. Instead, the vendor can use a lightweight query that returns a recent cached estimate of the number of available units of a particular product. Note here that the application can tolerate a stale or inaccurate values. The e-commerce website only cares if the customer decides to buy a particular product, during which it must use a transactional query to confirm the available inventory and update it after a customer completes a purchase.</p>
<p>Soft state and eventual consistency are techniques that work well in the presence of partitions and thus promote availability. A new class of databases emerged that follow BASE characteristics and have been dubbed as NoSQL databases. A few examples of NoSQL databases include Amazon’s Dynamo and Google’s BigTable. Eventual consistency can be tolerated as long as the application can tolerate stale data. Applications such as instant messaging, for example, are usually tolerant of the eventual consistency limitations.</p>
<p>The typical characteristics of NoSQL databases include</p>
<ul>
<li>No strict schema requirements</li>
<li>No strict adherence to ACID properties for transactions</li>
<li>Consistency traded in favor of availability</li>
</ul>
<p>The tradeoff with NoSQL databases is between ACID properties (most notably consistency) and performance and scalability.</p>
<p><strong>Types of NoSQL Databases</strong></p>
<p>A limited taxonomy of NoSQL databases is illustrated in Figure 4.20.</p>
<p><img src="/images/14567763989736.jpg" alt="Figure 4.20: A taxonomy of NoSQL databases"></p>
<p><strong>Document Stores</strong></p>
<p>In contrast to RDBMSs, where data is stored as records with fixed-length fields, document stores store a document in some standard format or encoding. These DBs because popular because they can store a variety of data and their schema can change with time. The encoding may be in XML or JSON, as well as any arbitrary binary format such as PDFs or Office documents. These are typically called binary large objects (BLOBs). A document is addressed using a key, which can be indexed for fast retrieval. Clearly, document stores can be used to store and index documents or individual media in a manner than can be indexed using metadata. This allows document stores to outperform traditional file systems in searching and indexing capabilities. Examples of document stores include MongoDB and CouchDB.</p>
<p><strong>Graph Databases</strong></p>
<p>In graph databases, graph structures such as vertices and edges are used to represent and store data. Graph databases can be used to store data that has network-like properties between elements (e.g., a social network graph). Graph databases are a powerful tool for graph-like queries, for example, where you might want to find the shortest path between two elements. Figure 4.21 illustrates an example of a graph database with a vertex representing a person or a club and the edges representing membership or familiarity. Alice and Bob are represented using vertices with an edge that signifies the “knows” relationship connecting them. From this, we can see that Alice and Bob know each other. The relationship is further quantified using the “since” tag, which specifies that Alice has known Bob since 2001/10/03 and Bob has known Alice since 2001/10/04. Likewise, a group entity called “Chess” is defined with relationships indicated by edges to both Alice and Bob, which signifies that they are both members of a chess group.</p>
<p><img src="/images/14567764230769.jpg" alt="Figure 4.21: An example of a graph database"></p>
<p>Queries in graph databases typically consist of graph traversals. For example, graphs can be used to compute the degrees of separation between two or more people in a social graph. Graph databases thus are great at representing networked information, with the relationship between entities occupying a key aspect of the representation. Examples of graph database software include Neo4j and VertexDB.</p>
<p><strong>Key-Value Stores</strong></p>
<p>Key-value (KV) store is a database model that maps keys to (possibly) more complex values. This type of database has the most amount of flexibility because keys can be mapped to arbitrary values or structures such as lists. The key-value pair constitutes an individual record in this model, and the keys are typically stored as a hash table. Hash lookups are fast and can be distributed easily, and for this reason, key-value stores can be scaled horizontally. Key-value stores typically support no more than the regular CRUD (create, read, update and delete) operations and may not natively support more advanced operations such as aggregations (e.g., averaging the values across key-value pairs) and joins (e.g., combining multiple-key value pairs into another key-value pair). Key-value stores are great for storing semi-structured information that is difficult to encapsulate within a rigid schema. The key-value relationship thus enables fast retrieval of data through an indexed key. Examples of key-value stores include Amazon DynamoDB and Apache Cassandra.</p>
<p><strong>Columnar Databases</strong></p>
<p>Columnar databases are a hybrid of RDBMSs and KV stores. Like relational databases, they store values in groups of zero or more columns, and as in key-value stores, values are queried by matching keys. However, in columnar databases, data is physically transposed and stored in column order instead of in row-order as in traditional RDBMSs. Operations such as modification of a subset of columns or aggregation of a column across all rows become more efficient, as entire rows do not have to be read to obtain the value of a single column. Furthermore, addition of columns to an existing table is inexpensive compared to traditional RDBMSs. Columnar databases have been traditionally developed with horizontal scalability as a primary design goal. As such, they are particularly suited to “big data” problems, living in clusters of tens, hundreds, or thousands of nodes. They also tend to have built-in support for features such as compression and versioning. Examples of columnar databases include HBASE and Vertica.</p>
<p><strong>Comparing Popular Databases</strong></p>
<p>The following table illustrates the differences between various popular databases that belong to each category.</p>
<p><img src="/images/14567764629808.jpg" alt=""></p>
<p><img src="/images/14567764773347.jpg" alt=""></p>
<p><img src="/images/14567764869423.jpg" alt=""></p>
<p>Following are some of the advantages of NoSQL databases.</p>
<ul>
<li>Data flexibility: NoSQL databases are designed with non-relational models and hence typically do not enforce a rigid schema. Document stores allow arbitrary information to be stored in some form of encoding (XML/JSON, etc.) or even in binary. Graph databases do not have schemas but a set of properties that are used in different kinds of edges or nodes. In key-value stores, for example, the value associated to a key can be a single value or a larger, more complex data structure such as a hash or list. In columnar stores, it is fast and easy to alter a table to add more columns if required.</li>
<li>Scalability: Several NoSQL systems employ a distributed architecture from the ground up, unlike RDBMSs whose fundamental designs have not changed in decades. This means that NoSQL systems are built for high scalability. For example, Yahoo! has deployed a 1000+ node HBASE cluster with 1 PB of data, and such large data stores are not uncommon with companies such as Google, Amazon, and Facebook.</li>
<li>Performance: By relaxing some of the ACID guarantees, NoSQL systems can take advantage of parallel access to data and provide faster performance than their traditional SQL counterparts.</li>
</ul>
<p>Disadvantages of NoSQL databases include the following.</p>
<ul>
<li>Application developers can no longer rely on ACID guarantees and have to design for lack of consistency guarantees. They must account for the possibility of stale data from the database during reads or writes that are not fully committed to disk before the operation.</li>
<li>NoSQL has lock-in because of a lack of standards. Even if data formats may be standardized through XML or JSON, each NoSQL product may have its own query/response formats. By comparison, moving between RDBMSs is easier because the data formats and query languages are largely standardized.</li>
</ul>
<h3 id="NewSQL_Databases"><a href="#NewSQL_Databases" class="headerlink" title="NewSQL Databases"></a>NewSQL Databases</h3><p>NoSQL databases have been quite successful in very specific domains. The most popular avenue for NoSQL systems is the storage of big data for web-scale companies whose applications that can tolerate reduced consistency guarantees. However, this approach pushes some of the storage challenges to the application developer. As an example, the non-relational data model means that develops may have to implement their own joins if they ever need to combine data from two tables. They also have to handle eventually consistent data and ensure that their applications do not face any correctness issues with the lack of transactions. However, not all applications can give up these strong transactional semantics. There is a demand for database systems that can combine the best of both the relational and NoSQL worlds; these systems work using a relational model and SQL, with ACID transactions, while offering scalable performance that is similar to NoSQL systems.</p>
<p>NewSQL databases refer to the next generation of relational DBMSs that can scale like a NoSQL system, without fully giving up on SQL or some level of ACID properties for transactions. This can be done using multiple techniques, the most popular of which we will discuss here:</p>
<ol>
<li>Shared-Nothing Architectures: A common design pattern in NewSQL systems is the adoption of a shared-nothing architecture. This is a system in which each node is completely independent (sometimes down to the level of individual threads), thus ensuring that there is no single point of contention across the system. This enables systems to have a high performance at scale because there is no need for expensive locking protocols.</li>
<li>In-Memory Systems: Another avenue for improving performance in database systems is to reduce the number of trips made to disk for a given query. An extreme version of this philosophy is to operate an entire database from memory, so that the database never has to go to disk.</li>
</ol>
<p><strong>H-Store and VoltDB</strong></p>
<p>H-Store is a good example of a NewSQL system. H-Store is deployed on a cluster of nodes, using a shared-nothing architecture. At the heart of H-Store is a highly optimized single-threaded database engine which quickly processes individual queries. The database is then sharded across the cluster such that every individual core is responsible for a disjoint subset of the data. The data in h-store is stored in the memory of each of the nodes in the system.</p>
<p>Because each engine has exclusive access to all of the data at a partition, only one transaction at a time is able to access the data stored in the partition, eliminating the need for locks and latches in the system. As a result, no transaction will stall waiting for another transaction once started, at least for queries that do not span a single partition.</p>
<p>H-Store has its limitations. First, the lack of durable storage (as H-Store stores all data in-memory), coupled with the shared nothing architecture means that node failures can cause loss of data.</p>
<p>While H-Store was an academic project, the commercial realization of H-Store emerged as VoltDB. VoltDB has been improving and adding features to H-Store in order to address the shortcomings of VoltDB, including a logging mode to enchance the durability of the storage system.</p>
<h3 id="Object_Storage_as_a_Cloud_Service"><a href="#Object_Storage_as_a_Cloud_Service" class="headerlink" title="Object Storage as a Cloud Service"></a>Object Storage as a Cloud Service</h3><p>We will now look at a special class of storage systems that are designed as a cloud service that is provided over the internet. In the context of our discussion on cloud storage thus far, object storage in clouds can be considered to be a special case of key-value stores provided as a service over the Internet.</p>
<p>Providing storage as a service to clients over the Internet requires abstracting the underlying storage implementation details and providing a simple and clean interface that can be used in applications. Cloud adopters are increasingly choosing the cloud object model for storage on the cloud.</p>
<p>With the concept of objects, object-based storage systems abstract the existing file system approach at a higher level. Object-based storage systems are typically layered over existing file systems. There is no notion of hierarchies in object-based storage systems, which instead use a flat data environment.</p>
<p>An object can be considered as a generic container that can store any arbitrary type of information. Designing interfaces for such arbitrary data may be difficult, but in storage parlance, a basic set of operations can be easily defined for any arbitrary object. These operations are create, read, update, and delete (CRUD), which are typically made available through some kind of API that can be accessed through HTTP or other network protocols using REST- or SOAP-style calls.</p>
<p><strong>REST</strong></p>
<p>Representational state transfer (REST) relies on a stateless, client-server, cacheable communications protocol and is typically implemented over HTTP. A stateless protocol treats each request as an independent operation, and each communication between a client and server is treated as an independent pair of requests and responses. Video 4.10 discusses HTTP and RESTful interfaces</p>
<p><a href="http://youtu.be/34iW4tcYGxA" target="_blank" rel="external">Video 4.10: HTTP and RESTful Interfaces</a></p>
<p>REST is an architectural style for designing networked applications and does not refer to a single protocol. REST is a design strategy for communications among various entities in a networked application. The idea is to use a simple mechanism instead of CORBA, WSDL, or RPC to connect and transfer information between machines over a network. Any interface that uses REST principles is called a RESTful interface.</p>
<p>A RESTful interface uses HTTP requests to post (create and/or update), read (make queries and get information), and delete data. Thus, a RESTful interface can be used for CRUD operations.</p>
<p>A RESTful interface consists of the following components:</p>
<ul>
<li>A uniform resource identifier (URI), such as an HTTP URL through which the service can be accessed.</li>
<li>An Internet media type for the data supported by the service (typically XML or JSON).</li>
<li>A set of operations that is supported by the Web service using HTTP methods (GET, PUT, POST, and DELETE).</li>
<li>An HTTP-driven API.</li>
</ul>
<p>Thus, a program that needs to connect to a service with a RESTful interface can use standard HTTP GET, PUT, POST, and DELETE requests. An example of a REST request is covered shortly.</p>
<p>The major advantages of REST are that it is</p>
<ul>
<li>A platform-independent approach that is ideally suited for the Internet.</li>
<li>A language-independent interface because all instructions are passed over HTTP so that, for example, a C# client can talk to a Python server.</li>
<li>A standards-based communication because it runs on top of HTTP.</li>
<li>Operational in the presence of firewalls as long as HTTP or HTTPS traffic is not filtered.</li>
</ul>
<p><strong>Object Storage Systems - Amazon S3</strong></p>
<p>Video 4.11 covers the basics ideas behind Object Storage Systems:</p>
<p><a href="http://youtu.be/Drl9LvRr35Y" target="_blank" rel="external">Video 4.11: Object Storage Systems</a></p>
<p>An example of object-based storage on the cloud is Amazon’s Simple Storage Service (S3). S3 allows users to store objects in buckets. Each object can be created, read, and deleted. Note that in the S3 model, although no native update-object method exists, an entire object can be deleted and re-created, similar to a file overwrite. However, S3 supports object versioning and can maintain multiple versions of an object on S3 if it is explicitly enabled by the object owner.</p>
<p>Here is an example of a RESTful HTTP call to Amazon S3 to create a bucket named mybucket. The HTTP call includes authorization information for the client to access the bucket.</p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="request">PUT <span class="string">/mybucket</span> HTTP/1.1</span></span><br><span class="line"><span class="attribute">Content-Length</span>: <span class="string">0</span></span><br><span class="line"><span class="attribute">User/Agent</span>: <span class="string">jClientUpload</span></span><br><span class="line"><span class="attribute">Host</span>: <span class="string">s3.amazonaws.com</span></span><br><span class="line"><span class="attribute">Date</span>: <span class="string">Sun, 05 Aug 2007 15:33:59 GMT</span></span><br><span class="line"><span class="attribute">Authorization</span>: <span class="string">AWS 15B4D3461F177624206A:YFhSWKDg3qDnGbV7JCnkfdz/IHY= LE:k3nL7gH3+PadhTEVn5EXAMPLE</span></span><br></pre></td></tr></table></figure>
<p>S3 can process the request and will send back an HTTP response similar to the following:</p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="status">HTTP/1.1 <span class="number">200</span> OK</span></span><br><span class="line"><span class="attribute">x-amz-id-2</span>: <span class="string">tILPE8NBqoQ2Xn9BaddGf/YlLCSiwrKP+OQOpbi5zazMQ3pC56KQgGk</span></span><br><span class="line"><span class="attribute">x-amz-request-id</span>: <span class="string">676918167DFF7F8C</span></span><br><span class="line"><span class="attribute">Date</span>: <span class="string">Sun, 05 Aug 2007 15:30:28 GMT</span></span><br><span class="line"><span class="attribute">Location</span>: <span class="string">/mybucket</span></span><br><span class="line"><span class="attribute">Content-Length</span>: <span class="string">0</span></span><br><span class="line"><span class="attribute">Server</span>: <span class="string">AmazonS3</span></span><br></pre></td></tr></table></figure>
<p>In the response, Amazon has acknowledged the request, indicated that the request was successful (with a “200 OK” message), and returned some information regarding the request. The x-amz-id-2 and x-amz-request-id fields are unique identifiers that can be used to keep track of responses for troubleshooting and debugging purposes.</p>
<p><strong>Cloud Object Storage Standards: CDMI</strong></p>
<p>The lack of a common standard for object storage is an issue plaguing cloud object storage. The most popular cloud-based object storage system is Amazon S3, which is proprietary.</p>
<p>The Storage Network Industry Association (SNIA) is promoting an open standard for cloud objects, called cloud data management interface (CDMI).</p>
<p><img src="/images/14567766768934.jpg" alt="Figure 4.22: CDMI"></p>
<p>CDMI defines data objects and data containers with tagged metadata (as key-value pairs) and uses RESTful interfaces, with JSON as the data exchange format. CDMI can be used for accessing and managing data on a storage cloud (Figure 4.22). An example of a client interaction with a storage cloud using CDMI is shown in Figure 4.23.</p>
<p><img src="/images/14567766899496.jpg" alt="Figure 4.23: A CDMI client interacting with a CDMI storage cloud"></p>
<p>The CDMI client can issue requests over HTTPS, and the MimeType indicates the type of CDMI resource with which the client is interacting (an object, a container) and returns standard HTTP status codes, indicating the status of the request.</p>
<p>The CDMI model is illustrated in Figure 4.24. A CDMI resource exists on a root location, indicated by the root URI: https://<offering>. The example contains two containers, A and B, that contain one object each. Note that each CDMI entity can support metadata, as indicated with the key-value tags associated with every entity.</offering></p>
<p><img src="/images/14567767056712.jpg" alt="Figure 4.24: The CDMI data model"></p>
<p>In addition, CDMI supports the following resource types:</p>
<ul>
<li>cdmi-capability: A special entity that describes the capabilities of this particular cloud store. This entity is important and can be used to discover the capabilities of a cloud (e.g., backup and replication).</li>
<li>cdmi-domain: Allows for the creation of domains (e.g., groups of users with object access permissions).</li>
<li>cdmi-queues: Allow for the creation of queues of objects that operate in first in, first out (FIFO) order. Applications can use such queues to implement notification or messaging systems.</li>
</ul>
<p>The advantages of CDMI include the following:</p>
<ul>
<li>Its vendor-neutral specification of a cloud object storage system allows for simpler data migration from one cloud to another.</li>
<li>It enables cloud peering for storage, which is a concept wherein resources from different clouds can be connected to enable seamless data sharing between clouds.</li>
<li>It adheres to existing standards, such as RESTful interface for data access, and can work with multiple underlying storage abstractions, such as shared and networked file systems.</li>
<li>It is a mature standard, which has a reference implementation and ISO standardization.</li>
</ul>
<p>The disadvantage of CDMI is that its adoption is yet to be seen. The CDMI standard is backed by many storage companies, but it is not yet officially supported by vendors such as Amazon, and the standard’s success remains unclear.</p>
<h3 id="Cloud_Storage_Summary"><a href="#Cloud_Storage_Summary" class="headerlink" title="Cloud Storage Summary"></a>Cloud Storage Summary</h3><ul>
<li>Data can be characterized by its structure, dynamicity, and volume. It usually can be fixed or structured, static or dynamic.</li>
<li>Storage technologies have been evolving to keep pace with ever-growing needs to store vast amounts of data.</li>
<li>Different applications can exhibit different requirements in terms of capacity, performance, fault-tolerance, durability and others. Storage systems are designed to address these requirements in an efficient manner.</li>
<li>Storage abstractions can be in the form of blocks on a block device, files on a file system, or as entities in a database.</li>
<li>There are many types of file systems, such as local, shared, and networked file systems.</li>
<li>Local file systems manage data on block devices (physical disks or LUNs). They map files to regions of the disk called blocks. File systems are designed with performance and dependability concerns in mind.</li>
<li>A single file system can be expanded over multiple disks, typically using LVM/RAID.</li>
<li>It is typical for storage to be consolidated in enterprise environments to facilitate pooling, sharing, and improving manageability of storage resources. Consolidated storage systems are typically shared among multiple servers using a storage area network (SAN).</li>
<li>A distributed file system is a network file system with files distributed among multiple file servers. A file is stored whole on one of the file system servers that is part of the DFS.</li>
<li>Distributed file systems have many design considerations, including fault tolerance, replication, consistency, and file-sharing semantics.</li>
<li>Databases evolved from the navigational model to the modern relational database model and further to the NoSQL and NewSQL models.</li>
<li>There are multiple design considerations for database systems.</li>
<li>When the data is amenable to structuring, it is typical to organize it using a well-defined model (also known as a schema). Semi-strctured or unstructured data are typically stored in schema-less systems such as key-value stores.</li>
<li>The CAP theorem states that any distributed storage system with shared data can have at most two of three desirable properties among the following: consistency, availability, and/or partition tolerance.</li>
<li>An RDBMS models data into interconnected tables. A schema defines the rules on the types and valid inputs for each column of a table. Tables are defined, accessed, and modified through a query language (typically SQL).</li>
<li>An RDBMS supports transactions by providing atomicity, consistency, isolation, and durability (ACID) guarantees.</li>
<li>Traditional databases can be scaled either vertically or horizontally.</li>
<li>Vertical scaling simply requires the underlying hardware (CPU, memory, disk, etc.) to be upgraded.</li>
<li>With horizontal scaling, a database is distributed across multiple machines, either through replication (same data is stored across multiple machines) or sharding (data is distributed across multiple machines).</li>
<li>Achieving ACID properties in a distributed database is challenging. Typically, a two-phase commit protocol (2PC) is used to ensure ACID properties in such databases. However, this approach affects performance at very large scales.</li>
<li>NoSQL databases take a relaxed approach to strict consistency guarantees in order to provide availability and partition tolerance at large scale and high performance.</li>
<li>A few types of NoSQL databases include document stores, graph databases, key-value stores, and columnar databases.</li>
<li>NoSQL databases typically offer data flexibility, scalability, and high performance for large volumes of data when compared to traditional RDBMSs.</li>
<li>Applications must take into account the relaxed consistency model of NoSQL data stores. Lack of standardization makes it difficult to migrate data from one database to another.</li>
<li>NewSQL databases attempt to combine the relational data model and SQL interface of RDBMSes with the scalability and performance of NoSQL systems. This is done by reengineering the database engines to perform well at scale through in-memory storage and a shared-nothing architecture.</li>
<li>Object stores provide an abstraction of an object (which is a generic container to store any arbitrary type of information) and a set of very basic operations—create, read, update, and delete (CRUD)—to enable online storage. They can be considered to be a service-oriented version of key-value stores.</li>
<li>Object stores are typically accessed using an API that is accessed over the network using REST/SOAP-style calls.</li>
<li>Amazon S3 is an example of an object store, and CDMI is an upcoming open standard that defines a cloud storage environment.</li>
</ul>
<h2 id="Case_Studies_3A_Distributed_File_Systems"><a href="#Case_Studies_3A_Distributed_File_Systems" class="headerlink" title="Case Studies: Distributed File Systems"></a>Case Studies: Distributed File Systems</h2><h2 id="Case_Studies_3A_NoSQL_Databases"><a href="#Case_Studies_3A_NoSQL_Databases" class="headerlink" title="Case Studies: NoSQL Databases"></a>Case Studies: NoSQL Databases</h2><h2 id="Case_Studies_3A_Cloud_Object_Storage"><a href="#Case_Studies_3A_Cloud_Object_Storage" class="headerlink" title="Case Studies: Cloud Object Storage"></a>Case Studies: Cloud Object Storage</h2></div><div class="tags"><a href="/tags/CMU/">CMU</a><a href="/tags/云计算/">云计算</a><a href="/tags/讲义/">讲义</a></div><div class="post-nav"><a href="/2016/01/16/cc-7/" class="pre"><i class="icon-previous">云计算 第 7 课 AWS 动手玩</i></a><a href="/2016/01/15/cc-6/" class="next">云计算 第 6 课 AWS API<i class="icon-next"></i></a></div><div data-thread-key="2016/01/16/cc-0/" data-title="云计算 第 0 课 阅读材料" data-url="http://wdxtub.com/2016/01/16/cc-0/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/01/16/cc-0/" data-title="云计算 第 0 课 阅读材料" data-url="http://wdxtub.com/2016/01/16/cc-0/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search" class="search-form-input"/><input type="hidden" name="sitesearch" value="http://wdxtub.com"/></form></div><div class="widget"><div class="widget-title">分类</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Game/">Game</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Gossip/">Gossip</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Memory/">Memory</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Movie/">Movie</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Reading/">Reading</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Story/">Story</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Technique/">Technique</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Traveling/">Traveling</a></li></ul></div><div class="widget"><div class="comments-title">最近评论</div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title">友情链接</div><ul></ul><a href="http://wdxtub.com/library/" title="我的笔记" target="_blank">我的笔记</a><ul></ul><a href="http://wdxtub.com/bookclips/" title="我的书摘" target="_blank">我的书摘</a><ul></ul><a href="http://wdxtub.com/interview/" title="刷题笔记" target="_blank">刷题笔记</a></div></div></div></div><div id="footer">© <a href="/." rel="nofollow">小土刀.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><a id="rocket" href="#top" class="show"></a><script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/totop.js" type="text/javascript"></script><script src="/js/fancybox.pack.js" type="text/javascript"></script>
<script src="/js/jquery.fancybox.js" type="text/javascript"></script><link rel="stylesheet" href="/css/jquery.fancybox.css" type="text/css"><script>var duoshuoQuery = {short_name:'wdxblog'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div><!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body></html>